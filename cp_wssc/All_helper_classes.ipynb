{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models training helper functions - note these are Helper Classes with helper methods, this file should contain *no* stand alone executable code.\n",
    "\n",
    "###This file contains 4 seperate heplper classes\n",
    "    -class generic_parsing_helpers:\n",
    "    -class AIVM_helper: (including github read/write and parsing methods\n",
    "    -class roc_callback(Callback)\n",
    "    -class model_helpers:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-12T15:39:42.826638Z",
     "start_time": "2019-10-12T15:39:42.807998Z"
    }
   },
   "source": [
    "### This file contains the following helper files:\n",
    "\n",
    "#### read_glove_embeddings,  vocabulary loading\n",
    "def read_glove_embeddings():\n",
    "\n",
    "\n",
    "#### get_stop_words, returns an immutable frozenset set of stopwords from a file.\n",
    "def get_stop_words(stop_file_path):\n",
    "\n",
    "\n",
    "#### clean_line, cleaning function which calls a stemmer, cleans the text and returns a string\n",
    "def clean_line(X):\n",
    "\n",
    "\n",
    "#### def create_tokenizer, create a tokenizer given a data frame with a column \"text\"\n",
    "def create_tokenizer(df, max_words=MAX_NB_WORDS):\n",
    "\n",
    "\n",
    "#### roc_callback, calculate AUC for validation when training the model\n",
    "##As explained here: https://stackoverflow.com/questions/41032551/how-to-compute-receiving-##operating-characteristic-roc-and-auc-in-keras\n",
    "class roc_callback(Callback):\n",
    "\n",
    "\n",
    " \n",
    "AF : this file should be retained as a helper file so it should not really make any calls\n",
    "but rather just have methods dafined. \n",
    "\n",
    "it's calling: \n",
    "embeddings_index = read_glove_embeddings() \n",
    "stopwords=get_stop_words(\"Data/Iterative-models-building/Training data/resources/stopwords.txt\")\n",
    "\n",
    "\n",
    "AF:\n",
    "Questions.\n",
    "1. Where does the stopwords list come from? is there a reason not to use the NLTK onne instead of storing locally?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-28T13:34:28.129845Z",
     "start_time": "2019-10-28T13:34:27.820219Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Necessary libraries imported.\n"
     ]
    }
   ],
   "source": [
    "# Required Python utilities\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "from langdetect import detect\n",
    "from bs4 import BeautifulSoup\n",
    "from markdown import markdown\n",
    "from lxml import etree\n",
    "import random\n",
    "import tqdm\n",
    "import itertools \n",
    "import pickle\n",
    "import os\n",
    "import base64\n",
    "import io\n",
    "from io import BytesIO\n",
    "import urllib \n",
    "from collections import Counter\n",
    "from langdetect import detect\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "import time\n",
    "from PIL import Image as PIL_Img\n",
    "from datetime import datetime\n",
    "import json\n",
    "import gc\n",
    "import requests as req\n",
    "import markdown\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "## Deep Learning imports for the classifiers\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, Concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "##Â ML required imports (for clustering)\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA, LatentDirichletAllocation\n",
    "from sklearn.preprocessing import scale, StandardScaler\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "\n",
    "# Topic modeling imports\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "##Â NLP related imports\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "# visualization imports\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "\n",
    "\n",
    "from github import Github, GithubException, InputGitTreeElement\n",
    "from IPython.display import display, clear_output, HTML, Image\n",
    "\n",
    "\n",
    "\n",
    "print (\"Necessary libraries imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a default model_helpers for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-19T10:59:22.868644Z",
     "start_time": "2019-10-19T10:59:22.855391Z"
    }
   },
   "outputs": [],
   "source": [
    "def default_model_helpers_for_project ():\n",
    "    \n",
    "    #Classificaiton NETWORKs Configuration parameters\n",
    "    SEQUENCE_LENGTH = 32\n",
    "    MAX_NB_WORDS = 10000\n",
    "    EMBEDDING_DIM = 100 # 100, 200 or 300\n",
    "    VALIDATION_SPLIT = 0.2\n",
    "    TFIDF_MAX_FEATURES=10000\n",
    "    NUM_EPOCHS = 20\n",
    "    \n",
    "    #from \"Models training helpers.ipynb\". model_helpers class is defined further below. \n",
    "    def_model_helpers = model_helpers (seq_len=SEQUENCE_LENGTH, \n",
    "                                   max_num_wrds=MAX_NB_WORDS, \n",
    "                                   embedding_dim=EMBEDDING_DIM,\n",
    "                                   validation_splt =VALIDATION_SPLIT,\n",
    "                                   tfidf_max = TFIDF_MAX_FEATURES,\n",
    "                                   num_epochs = NUM_EPOCHS)\n",
    "  \n",
    "\n",
    "    HTML_list = []\n",
    "    HTML = ''      \n",
    "    HTML_list.append ('You will find instantiate_model_helpers_for_project at the very TOP of All_helper_classes.ipynb, If you do not want to change the default config, used across all files you can instantiate your own model_helpers() instead.')\n",
    "    HTML_list.append('<br><b>SEQUENCE_LENGTH:</b> '+ str(SEQUENCE_LENGTH))\n",
    "    HTML_list.append('<br><b>MAX_NB_WORDS:</b> '+ str(MAX_NB_WORDS))\n",
    "    HTML_list.append('<br><b>EMBEDDING_DIM:</b> '+ str(EMBEDDING_DIM))\n",
    "    HTML_list.append('<br><b>VALIDATION_SPLIT:</b> ' + str(VALIDATION_SPLIT))\n",
    "    HTML_list.append('<br><b>TFIDF_MAX_FEATURES:</b> ' + str(TFIDF_MAX_FEATURES))\n",
    "    HTML_list.append('<br><b>NUM_EPOCHS '  + str(NUM_EPOCHS))\n",
    "    for line in HTML_list:\n",
    "            HTML = HTML + line \n",
    "            \n",
    "    return def_model_helpers, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a default list of paths for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T22:47:20.042714Z",
     "start_time": "2019-10-29T22:47:19.937328Z"
    }
   },
   "outputs": [],
   "source": [
    "# If you add any paths to this make sure to also add to the display command \n",
    "# so the calling notebook can see all available paths\n",
    "class project_paths:\n",
    "    def __init__(self):\n",
    "        #self.something = kwargs['something']\n",
    "        \n",
    "        self.GIT_OWNER = 'aideenf'\n",
    "        self.GIT_REPO = 'AIVC'\n",
    "        self.GIT_OWNER_REPO = self.GIT_OWNER + '/' + self.GIT_REPO\n",
    "        self.BASE_DIR_LOCAL = './Data/Iterative-models-building'\n",
    "        self.GLOVE_DIR_LOCAL = 'Data/Iterative-models-building/Training data/'\n",
    "\n",
    "        self.GATHERED_DATA_CONV_DIR_LOCAL = 'Data/Iterative-models-building/Gathered_data/Conventions/'\n",
    "        self.GATHERED_DATA_CONV_DIR_GIT = 'cp_wssc/Data/Iterative-models-building/Gathered_data/Conventions/'\n",
    "        self.GITHUB_GATHERED_URL_PATH = 'https://raw.githubusercontent.com/aideenf/AIVC/master/cp_wssc/Data/Iterative-models-building/Gathered_data/Conventions/'\n",
    "\n",
    "        self.MODELS_DIR_LOCAL = \"./Data/Iterative-models-building/Models/\"\n",
    "\n",
    "        self.STOP_WORDS_DIR_FILE_LOCAL = 'Data/Iterative-models-building/Training data/resources/stopwords.txt'\n",
    "        self.STOP_WORDS_URL_GIT = 'https://raw.githubusercontent.com/aideenf/AIVC/master/cp_wssc/Data/Iterative-models-building/Training data/resources/stopwords.txt'\n",
    "\n",
    "        self.TRAINING_DATA_DIR_FILE_LOCAL = 'Data/Iterative-models-building/Training data/Conventions/training_data_original.tsv'\n",
    "        self.TRAINING_DATA_DIR_FILE_GIT = 'https://raw.githubusercontent.com/aideenf/AIVC/master/cp_wssc/Data/Iterative-models-building/Training%20data/Conventions/training_data_original.tsv'\n",
    "\n",
    "    \n",
    "        \n",
    "        self.TRAINING_DATA_ALL_DIR_FILE_GIT = 'https://raw.githubusercontent.com/aideenf/AIVC/master/cp_wssc/Data/Iterative-models-building/Training%20data/Conventions/training_ALL.tsv'\n",
    "        self.TRAINING_DATA_ALL_DIR_FILE_LOCAL = 'Data/Iterative-models-building/Training data/Conventions/training_ALL.tsv'\n",
    "    \n",
    "        \n",
    "        self.TRAINING_DATA_ALL_MULTI_DIR_FILE_GIT = 'https://raw.githubusercontent.com/aideenf/AIVC/master/cp_wssc/Data/Iterative-models-building/Training%20data/Conventions/training_ALL_Multi_format.tsv'\n",
    "        self.TRAINING_DATA_ALL_MULTI_DIR_FILE_LOCAL = 'Data/Iterative-models-building/Training data/Conventions/training_ALL_Multi_format.tsv'\n",
    "        \n",
    "        \n",
    "        self.TRAINING_DATA_DIR_LOCAL = 'Data/Iterative-models-building/Training data/Conventions/'\n",
    "        self.TRAINING_DATA_DIR_GIT = 'cp_wssc/Data/Iterative-models-building/Training data/Conventions/'\n",
    "\n",
    "        self.AUDITED_DATA_DIR_LOCAL  = './Data/Iterative-models-building/Classification results/Conventions/Audited/'\n",
    "        self.AUDITED_DATA_DIR_GIT =  'cp_wssc/Data/Iterative-models-building/Classification results/Conventions/Audited/'\n",
    "        self.GITHUB_AUDIT_URL_PATH = 'https://raw.githubusercontent.com/aideenf/AIVC/master/cp_wssc/Data/Iterative-models-building/Classification%20results/Conventions/Audited/'\n",
    "        self.GITHUB_AGGREGATED_AUDIT_URL_FILE = 'https://raw.githubusercontent.com/aideenf/AIVC/master/cp_wssc/Data/Iterative-models-building/Classification%20results/Conventions/Audited/audited_ALL.tsv'\n",
    "\n",
    "        self.CLASSIFIED_DATA_DIR_LOCAL  = './Data/Iterative-models-building/Classification results/Conventions/'\n",
    "        self.CLASSIFIED_DATA_DIR_GIT =  'cp_wssc/Data/Iterative-models-building/Classification results/Conventions/'\n",
    "        \n",
    "\n",
    "        self.CONV_MODEL_PCKL_URL = 'https://github.com/aideenf/AIVC/blob/master/cp_wssc/Data/Iterative-models-building/Models/conv_models_items.pickle?raw=true'\n",
    "        self.SIMPLER_MODEL_PCKL_URL = 'https://github.com/aideenf/AIVC/blob/master/cp_wssc/Data/Iterative-models-building/Models/Simpler_ML_models.pickle?raw=true'\n",
    "\n",
    "        if not os.path.exists(self.GLOVE_DIR_LOCAL):\n",
    "            os.makedirs(self.GLOVE_DIR_LOCAL)\n",
    "\n",
    "        if not os.path.exists(self.GATHERED_DATA_CONV_DIR_LOCAL):\n",
    "            os.makedirs(self.GATHERED_DATA_CONV_DIR_LOCAL)\n",
    "\n",
    "        if not os.path.exists(self.MODELS_DIR_LOCAL):\n",
    "            os.makedirs(self.MODELS_DIR_LOCAL)\n",
    "    \n",
    "        if not os.path.exists(self.AUDITED_DATA_DIR_LOCAL):\n",
    "            os.makedirs(self.AUDITED_DATA_DIR_LOCAL)\n",
    "            \n",
    "        if not os.path.exists(self.TRAINING_DATA_DIR_FILE_LOCAL):\n",
    "            os.makedirs(self.TRAINING_DATA_DIR_FILE_LOCAL)\n",
    "            \n",
    "    def get_paths_data (self):\n",
    "        HTML_list = []\n",
    "        HTML = ''\n",
    "        HTML_list.append(\"<b>*****PROJECT PATHS*******</b>\")    \n",
    "        HTML_list.append('<br><b>paths.BASE_DIR_LOCAL: </b>' + self.BASE_DIR_LOCAL)\n",
    "        HTML_list.append('<br><b>paths.GLOVE_DIR_LOCAL: </b>' + self.GLOVE_DIR_LOCAL)\n",
    "        HTML_list.append('<br><b>paths.GATHERED_DATA_CONV_DIR_LOCAL: </b>' + self.GATHERED_DATA_CONV_DIR_LOCAL) \n",
    "        HTML_list.append('<br><b>paths.GATHERED_DATA_CONV_DIR_GIT: </b>' + self.GATHERED_DATA_CONV_DIR_GIT)\n",
    "        HTML_list.append('<br><b>paths.GITHUB_GATHERED_URL_PATH: </b>' + self.GITHUB_GATHERED_URL_PATH) \n",
    "        HTML_list.append('<br><b>paths.MODELS_DIR_LOCAL: </b>' + self.MODELS_DIR_LOCAL)\n",
    "        HTML_list.append('<br><b>paths.STOP_WORDS_DIR_FILE_LOCAL: </b>' + self.STOP_WORDS_DIR_FILE_LOCAL) \n",
    "        HTML_list.append('<br><b>paths.STOP_WORDS_URL_GIT: </b>' + self.STOP_WORDS_URL_GIT) \n",
    "        HTML_list.append('<br><b>paths.TRAINING_DATA_DIR_FILE_LOCAL: </b>' + self.TRAINING_DATA_DIR_FILE_LOCAL) \n",
    "        HTML_list.append('<br><b>paths.TRAINING_DATA_DIR_FILE_GIT: </b>' + self.TRAINING_DATA_DIR_FILE_GIT)\n",
    "        HTML_list.append('<br><b>paths.TRAINING_DATA_ALL_DIR_FILE_GIT: </b>' +self.TRAINING_DATA_ALL_DIR_FILE_GIT)\n",
    "        HTML_list.append('<br><b>paths.TRAINING_DATA_ALL_DIR_FILE_LOCAL: </b>' +self.TRAINING_DATA_ALL_DIR_FILE_LOCAL)\n",
    "        HTML_list.append('<br><b>paths.TRAINING_DATA_ALL_MULTI_DIR_FILE_GIT: </b>' +self.TRAINING_DATA_ALL_MULTI_DIR_FILE_GIT)\n",
    "        HTML_list.append('<br><b>paths.TRAINING_DATA_ALL_MULTI_DIR_FILE_LOCAL: </b>' +self.TRAINING_DATA_ALL_MULTI_DIR_FILE_LOCAL)\n",
    "        HTML_list.append('<br><b>paths.TRAINING_DATA_DIR_LOCAL: </b>' + self.TRAINING_DATA_DIR_LOCAL) \n",
    "        HTML_list.append('<br><b>paths.TRAINING_DATA_DIR_GIT: </b>' + self.TRAINING_DATA_DIR_GIT)\n",
    "        HTML_list.append('<br><b>paths.AUDITED_DATA_DIR_LOCAL: </b>' + self.AUDITED_DATA_DIR_LOCAL)\n",
    "        HTML_list.append('<br><b>paths.AUDITED_DATA_DIR_GIT: </b>' + self.AUDITED_DATA_DIR_GIT)\n",
    "        HTML_list.append('<br><b>paths.GITHUB_AUDIT_URL_PATH: </b>' + self.GITHUB_AUDIT_URL_PATH )\n",
    "        HTML_list.append('<br><b>paths.GITHUB_AGGREGATED_AUDIT_URL_FILE: </b>' + self.GITHUB_AGGREGATED_AUDIT_URL_FILE) \n",
    "        HTML_list.append('<br><b>paths.CONV_MODEL_PCKL_URL: </b>' + self.CONV_MODEL_PCKL_URL )\n",
    "        HTML_list.append('<br><b>paths.SIMPLER_MODEL_PCKL_URL: </b>' + self.SIMPLER_MODEL_PCKL_URL)\n",
    "        HTML_list.append('<br><b>paths.CLASSIFIED_DATA_DIR_LOCAL: </b>' + self.CLASSIFIED_DATA_DIR_LOCAL )\n",
    "        HTML_list.append('<br><b>paths.CLASSIFIED_DATA_DIR_GIT: </b>' + self.CLASSIFIED_DATA_DIR_GIT)\n",
    "        for line in HTML_list:\n",
    "            HTML = HTML + line\n",
    "            \n",
    "        return HTML\n",
    "    \n",
    "    def get_repo_data (self):\n",
    "        HTML_list = []\n",
    "        HTML = ''\n",
    "        HTML_list.append ('<b>*****GIT PROJECT*******</b>') \n",
    "        HTML_list.append('<br><b>paths.GIT_OWNER:</b> ' + self.GIT_OWNER)\n",
    "        HTML_list.append('<br><b>paths.GIT_REPO:</b> ' + self.GIT_REPO)\n",
    "        HTML_list.append('<br><b>paths.GIT_OWNER_REPO:</b> ' + self.GIT_OWNER_REPO)\n",
    "        for line in HTML_list:\n",
    "            HTML = HTML + line\n",
    "            \n",
    "        return HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic parsing helper methods in class generic_parsing_helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-13T20:06:41.296336Z",
     "start_time": "2019-10-13T20:06:41.251558Z"
    }
   },
   "outputs": [],
   "source": [
    "## these are inputs to the helper functions on contextual basis and should belong to a seperate file. \n",
    "\n",
    "class generic_parsing_helpers:\n",
    "    \n",
    "    def __init__(self, **kwargs ):\n",
    "        #self.something = kwargs['something']\n",
    "        print (\"Initialised generic_parsing_helpers class and methods\")\n",
    "        \n",
    "        \n",
    "    ## Vocabulary loading\n",
    "    def read_glove_embeddings(self, glove_dir, embedding_dim):\n",
    "        ## Reading GLOVE (precalculated word embeddings)\n",
    "        embeddings_index = {}\n",
    "        f = open(os.path.join(glove_dir, 'glove.6B.{}d.txt'.format(embedding_dim)))\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        f.close()\n",
    "    \n",
    "        return embeddings_index\n",
    "   \n",
    "\n",
    "    ## Stop words list \n",
    "    def get_stop_words(self, stop_file_path):\n",
    "        \"\"\"load stop words \"\"\"\n",
    "    \n",
    "        with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
    "            stopwords = f.readlines()\n",
    "            stop_set = set(m.strip() for m in stopwords)\n",
    "            return frozenset(stop_set)\n",
    "        #load a set of stop words\n",
    "    \n",
    "    \n",
    "    ## Text cleaning functions\n",
    "    def clean_line(self, X):\n",
    "    \n",
    "        stemmer = WordNetLemmatizer()\n",
    "\n",
    "        # Remove all the special characters\n",
    "        line = re.sub(r'\\W', ' ', str(X))\n",
    "\n",
    "        # remove all single characters\n",
    "        line = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', line)\n",
    "\n",
    "        # Remove single characters from the start\n",
    "        line = re.sub(r'\\^[a-zA-Z]\\s+', ' ', line) \n",
    "\n",
    "        # Substituting multiple spaces with single space\n",
    "        line = re.sub(r'\\s+', ' ', line, flags=re.I)\n",
    "\n",
    "        # Removing prefixed 'b'\n",
    "        line = re.sub(r'^b\\s+', '', line)\n",
    "\n",
    "        # Converting to Lowercase\n",
    "        line = line.lower()\n",
    "\n",
    "        # Lemmatization\n",
    "        line = line.split()\n",
    "\n",
    "        document2 = [stemmer.lemmatize(word) for word in line]\n",
    "        line = ' '.join(document2)\n",
    "\n",
    "        return line\n",
    "\n",
    "\n",
    "    def clean_str(self, text):\n",
    "        documents = []\n",
    "        ret = documents\n",
    "        if type(text == \"list\"):\n",
    "            #print(\"list: \", text)\n",
    "            for X in text:\n",
    "                documents.append(clean_line(X))\n",
    "    \n",
    "            ret =\"\"\n",
    "            if(len(documents)>1):\n",
    "\n",
    "                for d in documents:\n",
    "                    ret+=d+\"\\n\"\n",
    "            else:\n",
    "                ret = documents[0]\n",
    "        \n",
    "        elif type(text)==\"str\":\n",
    "            p#rint(\"str\")\n",
    "            ret = clear_line(text)\n",
    "        return ret\n",
    "    \n",
    "    \n",
    "    def remove_line_breaks(self, text_str):\n",
    "        text_str = text_str.replace(\"\\\\n\", \" \")\n",
    "        return text_str\n",
    " \n",
    "\n",
    "    #this seems to remove line breaks from a column of data frame\n",
    "    # renamed generically from def read_training_data(df_path_file, df_text_column):\n",
    "    def get_clean_df_text_column(self, df, df_text_column):\n",
    "        df[df_text_column] = df[df_text_column].apply(self.remove_line_breaks)\n",
    "        return df\n",
    "\n",
    "\n",
    "    ## Helper function to create a KERAS tokenizer apn apply to column, 'column name' of a givem pandas data frame with a column \"text\" and a maximum number of words to \n",
    "    ## fit_on_texts Updates internal vocabulary based on a list of texts. \n",
    "    ## This method creates the vocabulary index based on word frequency. So if you give it something like,\n",
    "    ## \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 1; word_index[\"cat\"] = 2 \n",
    "    ## it is word -> index dictionary so every word gets a unique integer value. \n",
    "    ## 0 is reserved for padding. So lower integer means more frequent word (often the first few are stop words \n",
    "    ## because they appear a lot).\n",
    "\n",
    "    def create_tokenizer(self, df, df_column_to_tokenize, max_words):\n",
    "        \n",
    "        texts = df[df_column_to_tokenize].values\n",
    "    \n",
    "        _tokenizer = Tokenizer(num_words=max_words, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "        _tokenizer.fit_on_texts(texts)\n",
    "    \n",
    "        return _tokenizer \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing and Github helpers, should check for duplications between these and generic_parsing_helpers above and consolidate.\n",
    "\n",
    "        -markdown_to_text()\n",
    "        -remove_nums_from_str(s)\n",
    "        -pre_process_sentence(sent)\n",
    "        -pre_process(textBlob) returns tokenized to sentences\n",
    "        -remove_excluded_files(file_list)\n",
    "        -remove_excluded_files_except(file_list, except_with_text)\n",
    "        -rreplace(s, old, new, occurrence)\n",
    "        -clean_file_name(name, replacements2=[]):\n",
    "        -def save_to_github(git_user, git_password, git_repo, my_file_list, push_to_git_as):\n",
    "        -def list_files_from_github_dir (owner, repo, dir_ref):\n",
    "        -keep_pdf_urls_only(file_list):\n",
    "        -def concat_files_from_github_dir (directory_base_url, file_list):\n",
    "        -def read_single_file_from_github_dir (directory_base_url, file_name):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T14:19:50.678251Z",
     "start_time": "2019-10-23T14:19:50.576435Z"
    }
   },
   "outputs": [],
   "source": [
    "class AIVM_helper:\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"Initialised AIVM_helper class and methods\")\n",
    "\n",
    "    def image_to_byte_array(self, image: Image):\n",
    "        imgByteArr = io.BytesIO()\n",
    "        image.save(imgByteArr, format=image.format)\n",
    "        imgByteArr = imgByteArr.getvalue()\n",
    "        return imgByteArr\n",
    "\n",
    "    def time_stamp(self):\n",
    "        now = datetime.now()\n",
    "        timestamp = datetime.timestamp(now)\n",
    "        dt_object = datetime.fromtimestamp(timestamp)\n",
    "        words = str(dt_object).split(' ')\n",
    "        return words[0], words[1]\n",
    "\n",
    "    def markdown_to_text(self, markdown_string):\n",
    "        \"\"\" Converts a markdown string to plaintext \"\"\"\n",
    "\n",
    "        # md -> html -> text since BeautifulSoup can extract text cleanly\n",
    "        html = markdown(markdown_string)\n",
    "\n",
    "        # remove code snippets\n",
    "        html = re.sub(r'<pre>(.*?)</pre>', ' ', html)\n",
    "        html = re.sub(r'<code>(.*?)</code >', ' ', html)\n",
    "\n",
    "        # extract text\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        text = ''.join(soup.findAll(text=True))\n",
    "        return text\n",
    "\n",
    "    def remove_nums_from_str(self, s):\n",
    "        result = ''.join([i for i in s if not i.isdigit()])\n",
    "        return result\n",
    "\n",
    "    def pre_process_sentence(self, sent):\n",
    "        sent = re.sub('-', ' ', sent, flags=re.MULTILINE)  # Added by Aideen\n",
    "        sent = re.sub(' +', ' ', sent, flags=re.MULTILINE)  # Added by Aideen\n",
    "        sent = sent.replace(\";\", \", \")\n",
    "        sent = re.sub(' +', ' ', sent, flags=re.MULTILINE)  # Added by Aideen\n",
    "        sent = self.remove_nums_from_str(sent.replace(\",\", \" \"))\n",
    "        sent = sent.replace(\"  \", \" \")\n",
    "        sent = sent.replace(\"#\", \"\")# Added by Aideen\n",
    "        sent = sent.replace('\"', \"\")\n",
    "        sent = sent.replace(\"*\", \"\")# Added by Aideen\n",
    "        sent = sent.replace(\" .\", \".\")\n",
    "        sent = sent.replace(\"[\", \"\")# Added by Aideen\n",
    "        sent = sent.replace(\"]\", \"\")# Added by Aideen\n",
    "        sent = sent.lstrip()\n",
    "        sent = sent.rstrip()\n",
    "        sent = sent.strip()\n",
    "        return sent\n",
    "\n",
    "    def pre_process(self, text):\n",
    "\n",
    "        # Removing prefixed 'b'\n",
    "        document = re.sub(r'^b\\s+', '', str(text))\n",
    "\n",
    "        # Removing splicit line change\n",
    "        document = re.sub(r'\\\\n', '', document, flags=re.MULTILINE)\n",
    "\n",
    "        soup = BeautifulSoup(document)\n",
    "\n",
    "        # Remove HTML code from text\n",
    "        document = soup.get_text()\n",
    "\n",
    "        # Parse text from markdown code\n",
    "        document = markdown_to_text(document)\n",
    "\n",
    "        # Removing URLS\n",
    "        document = re.sub(\n",
    "            r'^https?:\\/\\/.*[\\r\\n]*', '', document, flags=re.MULTILINE)\n",
    "\n",
    "        # Removing strings such as \\\\xe5 \\\\xe6 \\\\xe7 that appear a lot in the descriptions\n",
    "        document = re.sub(r':?\\\\+x\\w{2}', ' ', document, flags=re.MULTILINE)\n",
    "\n",
    "        # Remove all the special characters except spaces, dashes, commas and dots\n",
    "        document = re.sub(r\"[^\\s.,\\-a-zA-Z0-9]\", ' ', str(document))\n",
    "\n",
    "        # Substituting multiple spaces with single space\n",
    "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "        # Substituting multiple '-' with single '-'\n",
    "        document = re.sub(r'\\-{2,50}', '', document, flags=re.I)\n",
    "\n",
    "        document = re.sub('-', ' ', document,\n",
    "                          flags=re.MULTILINE)  # Added by Aideen\n",
    "        document = re.sub(' +', ' ', document,\n",
    "                          flags=re.MULTILINE)  # Added by Aideen\n",
    "        document.replace(\";\", \", \")\n",
    "        document = re.sub(' +', ' ', document,\n",
    "                          flags=re.MULTILINE)  # Added by Aideen\n",
    "        # Converting to Lowercase\n",
    "        document = document.lower()\n",
    "\n",
    "        # Sentences Tokenization\n",
    "        return sent_tokenize(document)\n",
    "\n",
    "    def remove_excluded_files(self, file_list):\n",
    "        cleaned_file_list = []\n",
    "        for f in file_list:\n",
    "            if not f.startswith('.') and not \"random\" in f and \"gathered_\" in f and not f.startswith(\"_\"):\n",
    "                cleaned_file_list.append(f)\n",
    "        return cleaned_file_list\n",
    "\n",
    "    def keep_pdf_urls_only(self, file_list):\n",
    "        cleaned_file_list = []\n",
    "        for f in file_list:\n",
    "            if f.endswith('.pdf') and not \" \" in f:\n",
    "                cleaned_file_list.append(f)\n",
    "        return cleaned_file_list\n",
    "\n",
    "    def remove_excluded_files_except(self, file_list, except_with_text):\n",
    "        cleaned_file_list = []\n",
    "        for f in file_list:\n",
    "            if not f.startswith('.') and not \"random\" in f and except_with_text in f and not f.startswith(\"_\"):\n",
    "                cleaned_file_list.append(f)\n",
    "        return cleaned_file_list\n",
    "\n",
    "    def rreplace(self, s, old, new, occurrence):\n",
    "        li = s.rsplit(old, occurrence)\n",
    "        return new.join(li)\n",
    "\n",
    "\n",
    "    def clean_file_name(self, name, replacements2=[]):\n",
    "\n",
    "        replacements = [\".txt\", \".csv\", \".tsv\"]\n",
    "\n",
    "        for r in replacements:\n",
    "            name = name.replace(r, \"\")\n",
    "\n",
    "        for r in replacements2:\n",
    "            name = name.replace(r, \"\")\n",
    "        return name\n",
    "    \n",
    "\n",
    "    def save_img_to_github(self, git_user, git_password, git_repo, my_file_list, push_to_git_as, commit_message):\n",
    "        '''\n",
    "        in order to push a file to github it must first be stored locally, then pushed\n",
    "        this local location can also be local to a virtual machine. \n",
    "        takes: \n",
    "                git username, password, repo, \n",
    "                a list of files to push to git ie the full local location of file,\n",
    "                a matching list of paths to push each file to in Git hub \n",
    "        '''\n",
    "        user = git_user\n",
    "        password = git_password\n",
    "        url = git_repo\n",
    "        file_list = []  # push these list of files to git\n",
    "        file_names = []  # push to this location in git\n",
    "        message = 'ok'\n",
    "        element_list = []\n",
    "        try:\n",
    "            g = Github(user, password)\n",
    "            try:\n",
    "                repo = g.get_repo(url)\n",
    "            except (IOError, OSError, GithubException) as e:\n",
    "                return \"error\", e.message\n",
    "\n",
    "            \n",
    "            file_list = my_file_list\n",
    "            file_names = push_to_git_as\n",
    "\n",
    "            commit_message = commit_message\n",
    "            master_ref = repo.get_git_ref('heads/master')\n",
    "            master_sha = master_ref.object.sha\n",
    "            base_tree = repo.get_git_tree(master_sha)\n",
    "            \n",
    "\n",
    "            \n",
    "            for i, entry in enumerate(file_list):     \n",
    "                data = open(entry, \"rb\").read()\n",
    "                data = re.sub(b'/^(.+,)/', '' , data)\n",
    "                data = base64.b64encode(data)\n",
    "                blob = repo.create_git_blob(data.decode(\"utf-8\"), \"base64\")\n",
    "                element = InputGitTreeElement(\n",
    "                    path=file_names[i], mode='100644', type='blob', sha=blob.sha)\n",
    "                \n",
    "                # element_list is a list of InputGitTreeElement.\n",
    "                # Each one corresponds to a file.\n",
    "                element_list.append(element)\n",
    "            tree = repo.create_git_tree(element_list, base_tree)\n",
    "            parent = repo.get_git_commit(master_sha)\n",
    "            commit = repo.create_git_commit(commit_message, tree, [parent])\n",
    "            print (\"commit\", commit)\n",
    "            master_ref.edit(commit.sha)\n",
    "            return commit, message\n",
    "        except (IOError, OSError, GithubException) as e:\n",
    "            message = \"GitHub save FAILED:\" + '\\n' + \"Are your github login credentials correct?\" + \\\n",
    "                '\\n' + \"Are you a collaberator in the repo?\" + '\\n'\n",
    "            \n",
    "            return \"error\",  message + \"Specific Error: \" + str(e)\n",
    "        \n",
    "        \n",
    "    def save_to_github(self, git_user, git_password, git_repo, my_file_list, push_to_git_as, commit_message):\n",
    "        '''\n",
    "        in order to push a file to github it must first be stored locally, then pushed\n",
    "        this local location can also be local to a virtual machine. \n",
    "        takes: \n",
    "            git username, password, repo, \n",
    "            a list of files to push to git ie the full local location of file,\n",
    "            a matching list of paths to push each file to in Git hub \n",
    "        '''\n",
    "        user = git_user\n",
    "        password = git_password\n",
    "        url = git_repo\n",
    "        file_list = []  # push these list of files to git\n",
    "        file_names = []  # push to this location in git\n",
    "        message = 'ok'\n",
    "        element_list = []\n",
    "        try:\n",
    "            g = Github(user, password)\n",
    "            try:\n",
    "                repo = g.get_repo(url)\n",
    "            except (IOError, OSError, GithubException) as e:\n",
    "                return \"error\", e.message\n",
    "\n",
    "            \n",
    "            file_list = my_file_list\n",
    "            file_names = push_to_git_as\n",
    "\n",
    "            commit_message = commit_message\n",
    "            master_ref = repo.get_git_ref('heads/master')\n",
    "            master_sha = master_ref.object.sha\n",
    "            base_tree = repo.get_git_tree(master_sha)\n",
    "            \n",
    "            for i, entry in enumerate(file_list):    \n",
    "                data = base64.b64encode(open(entry, \"rb\").read())\n",
    "                if entry.endswith('.png'):\n",
    "                    data = base64.b64encode(data)\n",
    "                blob = repo.create_git_blob(data.decode(\"utf-8\"), \"base64\")\n",
    "\n",
    "                element = InputGitTreeElement(\n",
    "                    path=file_names[i], mode='100644', type='blob', sha=blob.sha)\n",
    "                \n",
    "                # element_list is a list of InputGitTreeElement.\n",
    "                # Each one corresponds to a file.\n",
    "                element_list.append(element)\n",
    "            tree = repo.create_git_tree(element_list, base_tree)\n",
    "            parent = repo.get_git_commit(master_sha)\n",
    "            commit = repo.create_git_commit(commit_message, tree, [parent])\n",
    "            print (\"commit\", commit)\n",
    "            master_ref.edit(commit.sha)\n",
    "            return commit, message\n",
    "        except (IOError, OSError, GithubException) as e:\n",
    "            message = \"GitHub save FAILED:\" + '\\n' + \"Are your github login credentials correct?\" + \\\n",
    "                '\\n' + \"Are you a collaberator in the repo?\" + '\\n'\n",
    "            \n",
    "            return \"error\",  message + \"Specific Error: \" + str(e)\n",
    "\n",
    "\n",
    "\n",
    "    def concat_files_from_github_dir(self, directory_base_url, file_list):\n",
    "        df_list = []\n",
    "        for file_name, i in zip(file_list, range(len(file_list))):\n",
    "            urlBase = directory_base_url\n",
    "            df_list.append(pd.read_csv(urlBase+file_name,\n",
    "                                       sep='\\t', error_bad_lines=False))\n",
    "        # NOTE: \"truncated\": false  we should check for truncated = true to do follow on call to get all files\n",
    "        df = pd.concat(df_list)\n",
    "        return df\n",
    "\n",
    "    def read_single_file_from_github_dir(self, directory_base_url, file_name):\n",
    "        df_list = []\n",
    "        for file_name, i in zip(file_list, range(len(file_list))):\n",
    "            urlBase = directory_base_url\n",
    "            df_list.append(pd.read_csv(urlBase+file_name,\n",
    "                                       sep='\\t', error_bad_lines=False))\n",
    "        # NOTE: \"truncated\": false  we should check for truncated = true to do follow on call to get all files\n",
    "        df = pd.concat(df_list)\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    \n",
    "    def list_files_from_github_dir(self, owner, repo, path):\n",
    "    \n",
    "        # read data files for source data directly from github.\n",
    "        # to obtain the id for the folder, navigate the tree using\n",
    "        # https://api.github.com/repos/{owner}/{repo}/git/trees/master\n",
    "        # e.g https://api.github.com/repos/aideenf/AIVC/git/trees/master\n",
    "        # once navigated each directory will be of format\n",
    "        # https://api.github.com/repos/aideenf/AIVC/git/trees/{dir_ref}\n",
    "        # exampe dir_ref = 048349b4dd81d95a17129e7fcd5418bdca8309b3\"\n",
    "        \n",
    "        \n",
    "        #Note A commit, or \"revision\", is an individual change to a file (or set of files). \n",
    "        #It's like when you save a file, except with Git, \n",
    "        #every time you save it creates a unique ID (a.k.a. the \"SHA\" or \"hash\") \n",
    "        #that allows you to keep record of what changes were made when and by who.\n",
    "        # in order to list the docs in the latest version of any folder the most up to date\n",
    "        #Sha should be used. \n",
    "        \n",
    "        #in order to get latest must always start at the master\n",
    "        #https://api.github.com/repos/aideenf/AIVC/git/trees/master\n",
    "        \n",
    "        # import requests as req  #we need to ensure we do not get cached response from browser.\n",
    "        \n",
    "        #get the dir for each step in the path\n",
    "        dir_list = path.split('/')\n",
    "        gathered_files = []\n",
    "        headers = {\n",
    "            'Cache-Control': 'no-cache',\n",
    "            'Pragma': 'no-cache',\n",
    "            'If-None-Match': ''\n",
    "            }\n",
    "        \n",
    "        MASTER = 'https://api.github.com/repos/' + owner + '/' + repo + '/git/trees/master'\n",
    "        NEXT = 'https://api.github.com/repos/' + owner + '/' + repo + '/git/trees/'\n",
    "        resp = req.get(MASTER,  headers=headers)\n",
    "        response = json.loads(resp.text)\n",
    "        try:\n",
    "            for value in response['tree']:\n",
    "                if value['path'] == dir_list[0]:\n",
    "                    NEXT_DIR = NEXT + value['sha']\n",
    "            \n",
    "            for d, i in zip(dir_list, range(len(dir_list)-1)):                          \n",
    "                resp = req.get(NEXT_DIR,  headers=headers)\n",
    "                response = json.loads(resp.text)\n",
    "                for value in response['tree']:\n",
    "                    if value['path'] == dir_list[i+1]:\n",
    "                        NEXT_DIR = NEXT + value['sha']\n",
    "                   \n",
    "        \n",
    "            resp = req.get(NEXT_DIR,  headers=headers)\n",
    "            response = json.loads(resp.text)\n",
    "            for value in response['tree']:\n",
    "                gathered_files.append(value['path'])\n",
    "\n",
    "            return gathered_files\n",
    "        except: \n",
    "            print (response['message'])\n",
    "            return []\n",
    "        \n",
    "        \n",
    "    def save_plot(self, plot, save_as, local, git, to_git, git_user, git_pswd, git_owner_repo):\n",
    "\n",
    "        fig = plot.get_figure()\n",
    "        fig.savefig(local + save_as)  \n",
    "    \n",
    "        if to_git == True:\n",
    "            my_file_list = [local + save_as]\n",
    "            push_to_git_as = [git + save_as]\n",
    "    \n",
    "            commit, message = self.save_img_to_github(git_user, git_pswd, git_owner_repo, my_file_list, push_to_git_as, \"auto push classified conventions\")\n",
    "\n",
    "            if (commit != \"error\"):\n",
    "                print (\"File to commit:\", my_file_list)\n",
    "                print (\"Push to git as:\", push_to_git_as)\n",
    "                print (\"Commit: \", commit)\n",
    "                display(HTML(\"<font color='green'><b>Files Merged and Saved!!</b></font>\"))\n",
    "        \n",
    "            \n",
    "            if (commit == \"error\"):\n",
    "                print (\"File to commit: \", my_file_list)\n",
    "                print (\"Push to git as: \", push_to_git_as)\n",
    "                display(HTML(\"<font color='red'><b>Warning!!</b></font>\"))\n",
    "                print(message)\n",
    "        \n",
    "        return fig\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calss to calculate AUC for validation when training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-13T20:06:42.215424Z",
     "start_time": "2019-10-13T20:06:42.199245Z"
    }
   },
   "outputs": [],
   "source": [
    "## Calculate AUC for validation when training the model\n",
    "## As explained here: https://stackoverflow.com/questions/41032551/how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras\n",
    "class roc_callback(Callback):\n",
    "    def __init__(self,validation_data):#training_data,validation_data):\n",
    "        #self.x = training_data[0]\n",
    "        #self.y = training_data[1]\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "       \n",
    "        y_pred_val = self.model.predict(self.x_val)\n",
    "        roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
    "\n",
    "        print(\"Roc-AUC on validation: {}\".format(str(round(roc_val,4))))\n",
    "        return\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Helper methods in class model_helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T22:53:48.156311Z",
     "start_time": "2019-10-29T22:53:47.626382Z"
    }
   },
   "outputs": [],
   "source": [
    "class model_helpers:\n",
    "    \n",
    "    def __init__(self, **kwargs ):\n",
    "        #self.something = kwargs['something']\n",
    "\n",
    "        \n",
    "        ## Classificaiton NETWORK Configuration parameters\n",
    "        self.MAX_SEQUENCE_LENGTH = kwargs['seq_len']\n",
    "        self.MAX_NB_WORDS = kwargs['max_num_wrds']\n",
    "        self.EMBEDDING_DIM = kwargs['embedding_dim'] ## 100, 200 or 300\n",
    "        self.VALIDATION_SPLIT = kwargs['validation_splt']\n",
    "        self.TFIDF_MAX_FEATURES = kwargs['tfidf_max']\n",
    "        self.NUM_EPOCHS = kwargs['num_epochs']\n",
    "        print (\"Initialised model_helpers class and methods\")\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## Training a deep learning model given training, validation data\n",
    "    def train_DL_model(self, x_train, y_train, x_val, y_val, tokenizer):\n",
    "    \n",
    "        num_epochs=self.NUM_EPOCHS\n",
    "        word_index = tokenizer.word_index\n",
    "    \n",
    "        print('\\nNumber of elements from each class in traing and validation set ')\n",
    "        print(y_train.sum(axis=0))\n",
    "        print(y_val.sum(axis=0))\n",
    "\n",
    "\n",
    "        embedding_matrix = np.random.random((len(word_index) + 1, self.EMBEDDING_DIM))\n",
    "        for word, i in word_index.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "\n",
    "        embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                    self.EMBEDDING_DIM,\n",
    "                                    weights=[embedding_matrix],\n",
    "                                    input_length=self.MAX_SEQUENCE_LENGTH,\n",
    "                                    trainable=False)\n",
    "    \n",
    "\n",
    "        convs = []\n",
    "        filter_sizes = [3,4,5]\n",
    "\n",
    "        sequence_input = Input(shape=(self.MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "        embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "        for fsz in filter_sizes:\n",
    "            l_conv = Conv1D(nb_filter=128,filter_length=fsz,activation='relu')(embedded_sequences)\n",
    "            l_pool = MaxPooling1D(5)(l_conv)\n",
    "            convs.append(l_pool)\n",
    "\n",
    "        l_merge = Concatenate(axis=1)(convs)#\n",
    "\n",
    "        l_cov1= Conv1D(128, 5, activation='relu')(l_merge)\n",
    "        l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "\n",
    "        l_flat = Flatten()(l_pool1)\n",
    "        l_dense = Dense(128, activation='relu')(l_flat)\n",
    "        preds = Dense(2, activation='softmax')(l_dense)\n",
    "    \n",
    "        model = Model(sequence_input, preds)\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='rmsprop',\n",
    "                      metrics=['acc'])\n",
    "\n",
    "    \n",
    "        print(\"model fitting - more complex convolutional neural network\")\n",
    "        model.summary()\n",
    "        train_history = model.fit(x_train, y_train, validation_data=(x_val, y_val),epochs=num_epochs, batch_size=50, callbacks=[roc_callback(validation_data=(x_val, y_val))])\n",
    "\n",
    "        return model,tokenizer,x_val,y_val, train_history\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## Trains a DL model without using validation dataset\n",
    "    def train_DL_model_not_validation(self, x_train, y_train, tokenizer):\n",
    "        num_epochs=self.NUM_EPOCHS\n",
    "        word_index = tokenizer.word_index\n",
    "    \n",
    "        print('\\nNumber of elements from each class in traing and validation set ')\n",
    "        print(y_train.sum(axis=0))\n",
    "        print(y_val.sum(axis=0))\n",
    "\n",
    "        embedding_matrix = np.random.random((len(word_index) + 1, self.EMBEDDING_DIM))\n",
    "        for word, i in word_index.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "\n",
    "        embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                    self.EMBEDDING_DIM,\n",
    "                                    weights=[embedding_matrix],\n",
    "                                    input_length=self.MAX_SEQUENCE_LENGTH,\n",
    "                                    trainable=False)\n",
    "\n",
    "\n",
    "        convs = []\n",
    "        filter_sizes = [3,4,5]\n",
    "\n",
    "        sequence_input = Input(shape=(self.MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "        embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "        for fsz in filter_sizes:\n",
    "            l_conv = Conv1D(nb_filter=128,filter_length=fsz,activation='relu')(embedded_sequences)\n",
    "            l_pool = MaxPooling1D(5)(l_conv)\n",
    "            convs.append(l_pool)\n",
    "\n",
    "        l_merge = Concatenate(axis=1)(convs)#\n",
    "\n",
    "        l_cov1= Conv1D(128, 5, activation='relu')(l_merge)\n",
    "        l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "\n",
    "        l_flat = Flatten()(l_pool1)\n",
    "        l_dense = Dense(128, activation='relu')(l_flat)\n",
    "        preds = Dense(2, activation='softmax')(l_dense)\n",
    "\n",
    "        model = Model(sequence_input, preds)\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='rmsprop',\n",
    "                      metrics=['acc'])\n",
    "\n",
    "        print(\"model fitting - more complex convolutional neural network\")\n",
    "        model.summary()\n",
    "        train_history = model.fit(x_train, y_train,epochs=num_epochs, batch_size=50)\n",
    "        return model,tokenizer,train_history\n",
    "\n",
    "    \n",
    "    \n",
    "    ## Trains one DL model for each group of sentences (within each convention)\n",
    "    def train_DL_models(self, df_train,\n",
    "                        data_class_column=\"convention\", \n",
    "                        data_label_column=\"label\",\n",
    "                        df_val=None,\n",
    "                        tokenizer=None,\n",
    "                        random_seed=None,\n",
    "                        use_validation=True):\n",
    "        \n",
    "        num_epochs = self.NUM_EPOCHS\n",
    "        _models = {}\n",
    "        _tokenizers = {}\n",
    "        _data_val_x = {}\n",
    "        _data_val_y = {}\n",
    "        _train_histories = {}\n",
    "        if tokenizer is None:        \n",
    "            if df_val is None:\n",
    "                tokenizer = create_tokenizer(df_train, column_to_tokenize='text', max_words=self.MAX_NB_WORDS)\n",
    "            else:\n",
    "                tokenizer = create_tokenizer(pd.concat([df_train, df_val]), column_to_tokenize='text', max_words=self.MAX_NB_WORDS)\n",
    "\n",
    "        word_index = tokenizer.word_index\n",
    "        print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    \n",
    "        for convention in df_train[data_class_column].unique():\n",
    "        \n",
    "            print(\"----------------------------------------------------------------\")\n",
    "            print(\"            {}                  \".format(convention))\n",
    "            print(\"----------------------------------------------------------------\")\n",
    "        \n",
    "            tmp_df_train = df_train[df_train[data_class_column] == convention]\n",
    "            if use_validation:\n",
    "                if df_val is None:\n",
    "\n",
    "                    texts = tmp_df_train['text'].values\n",
    "                    labels = tmp_df_train[data_label_column].values\n",
    "\n",
    "                    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "\n",
    "                    data = pad_sequences(sequences, maxlen=self.MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "                    labels = to_categorical(np.asarray(labels))\n",
    "                    print('Shape of data tensor:', data.shape)\n",
    "                    print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "                    indices = np.arange(data.shape[0])\n",
    "                    if random_seed is not None:\n",
    "                        np.random.seed(random_seed)\n",
    "                    np.random.shuffle(indices)\n",
    "                    data = data[indices]\n",
    "                    labels = labels[indices]\n",
    "                    nb_validation_samples = int(self.VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "                    x_train = data[:-nb_validation_samples]\n",
    "                    y_train = labels[:-nb_validation_samples]\n",
    "                    x_val = data[-nb_validation_samples:]\n",
    "                    y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "                    _model, _tokenizer, _x_val, _y_val, _train_h = self.train_DL_model(x_train, y_train, x_val, y_val, tokenizer)\n",
    "                else:\n",
    "\n",
    "                    tmp_df_val = df_val[df_val[data_class_column] == convention]\n",
    "\n",
    "                    train_texts = tmp_df_train['text'].values\n",
    "                    train_labels = tmp_df_train[data_label_column].values\n",
    "\n",
    "                    val_texts = tmp_df_val['text'].values\n",
    "                    val_labels = tmp_df_val[data_label_column].values\n",
    "\n",
    "                    train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "                    val_sequences = tokenizer.texts_to_sequences(val_texts)\n",
    "\n",
    "                    x_train = pad_sequences(train_sequences, maxlen=self.MAX_SEQUENCE_LENGTH)\n",
    "                    x_val = pad_sequences(val_sequences, maxlen=self.MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "                    y_train = to_categorical(np.asarray(train_labels))\n",
    "                    y_val = to_categorical(np.asarray(val_labels))\n",
    "\n",
    "                    _model, _tokenizer, _x_val, _y_val, _train_h = self.train_DL_model(x_train, y_train, x_val, y_val, tokenizer)\n",
    "        \n",
    "                _models[convention] = _model\n",
    "                _tokenizers[convention] = _tokenizer\n",
    "                _data_val_x[convention] = _x_val\n",
    "                _data_val_y[convention] = _y_val\n",
    "                _train_histories[convention] = _train_h\n",
    "            else:\n",
    "                texts = tmp_df_train['text'].values\n",
    "                labels = tmp_df_train[data_label_column].values\n",
    "\n",
    "                sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "                data = pad_sequences(sequences, maxlen=self.MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "                labels = to_categorical(np.asarray(labels))\n",
    "                print('Shape of data tensor:', data.shape)\n",
    "                print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "            \n",
    "                x_train = data\n",
    "                y_train = labels\n",
    "        \n",
    "                _model, _tokenizer, _train_h = self.train_DL_model_not_validation(x_train, y_train, tokenizer)\n",
    "\n",
    "                _models[convention] = _model\n",
    "                _tokenizers[convention] = _tokenizer\n",
    "                _train_histories[convention] = _train_h\n",
    "        \n",
    "\n",
    "            print(\"\\n\\n\\n\")\n",
    "        \n",
    "        return (_models, _tokenizers, _data_val_x, _data_val_y, _train_histories)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def store_DL_models_in_picke(self, pickle_f_name, _models, _tokenizers, _val_x, _val_y, _train_histories, path):\n",
    "        _convnet_items = {}\n",
    "    \n",
    "        _convnet_items['model'] = _conventions_models\n",
    "        _convnet_items['tokenizer'] = _conventions_tokenizers\n",
    "        _convnet_items['_x_val'] = _conventions_data_val_x\n",
    "        _convnet_items['_y_val'] = _conventions_data_val_y\n",
    "        _convnet_items['train_history'] = _conventions_train_histories\n",
    "    \n",
    "        with open(os.path.join(path, ), 'wb') as f:\n",
    "            # Pickle the 'data' dictionary using the highest protocol available.\n",
    "            pickle.dump(_convnet_items, f, pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "            \n",
    "    def read_DL_models_from_pickle(self, use_git, pickle_f_name, path):\n",
    "        \n",
    "        if use_git == False:\n",
    "            f = open( os.path.join(path, pickle_f_name), 'rb') \n",
    "            _convnet_items = pickle.load(f)\n",
    "            f.close()\n",
    "            \n",
    "        if use_git == True:\n",
    "            print (\"Reading from GIT:\", pickle_f_name)   \n",
    "            _convnet_items = pickle.load(urllib.request.urlopen(pickle_f_name))\n",
    "            \n",
    "    \n",
    "    \n",
    "        ## Load the models from the downloaded pickle file             \n",
    "        _models = _convnet_items['model'] \n",
    "        _tokenizers = _convnet_items['tokenizer'] \n",
    "        _val_x = _convnet_items['_x_val'] \n",
    "        _val_y = _convnet_items['_y_val'] \n",
    "        _train_histories = _convnet_items['train_history'] \n",
    "    \n",
    "        return _models, _tokenizers, _val_x, _val_y, _train_histories\n",
    "                                                                           \n",
    "                                        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ## Trains a TF-IDF, Naive-Bayes based classifier\n",
    "    def train_new_text_pipelineNB(self, texts, labels1):\n",
    "        text_clf = Pipeline([\n",
    "            ('vect', CountVectorizer(max_df=0.85, stop_words=stopwords, max_features=self.TFIDF_MAX_FEATURES)),\n",
    "            ('tfidf', TfidfTransformer(smooth_idf=True,use_idf=True)),\n",
    "            ('clf', MultinomialNB()),\n",
    "        ])\n",
    "    \n",
    "        text_clf.fit(texts, labels1)  \n",
    "    \n",
    "        return text_clf\n",
    "    \n",
    "    \n",
    "    def train_ML_models(self, df,\n",
    "                    data_class_column=\"convention\", \n",
    "                    data_label_column=\"label\"):\n",
    "        _models = {}  \n",
    "    \n",
    "        for c in df[data_class_column].unique():\n",
    "            sentences = df[df[data_class_column] == c]['text'].values\n",
    "            labels = df[df[data_class_column] == c][data_label_column].values\n",
    "        \n",
    "            m = train_new_text_pipelineNB(sentences, labels)\n",
    "        \n",
    "            _models[c] = m\n",
    "        return _models\n",
    "    \n",
    "    \n",
    "    ## Helpers for calculating ML model probability score of class 1 for a set of sentences\n",
    "    def calculate_matches_DL(self, sentences, _models, _tokenizers):\n",
    "        _models_matches = {}\n",
    "\n",
    "        ## Getting classification confidence per model for each repo\n",
    "        for model_key in _models.keys():\n",
    "            \n",
    "            tokenized_sentences = _tokenizers[model_key].texts_to_sequences(sentences)\n",
    "        \n",
    "            tokenized_sentences = pad_sequences(tokenized_sentences, maxlen=self.MAX_SEQUENCE_LENGTH)\n",
    "            preds = _models[model_key].predict(tokenized_sentences)\n",
    "    \n",
    "            _models_matches[model_key] = preds[:,1]\n",
    "        \n",
    "        return _models_matches\n",
    "\n",
    "    def calculate_matches_ML(self, _sentences, _models): \n",
    "    \n",
    "        _models_matches = {}\n",
    "\n",
    "        ## Getting classification confidence per model for each repo\n",
    "        for model_key in _models.keys():\n",
    "        \n",
    "            preds = _models[model_key].predict_proba(_sentences)[:,1]\n",
    "            _models_matches[model_key] = preds\n",
    "\n",
    "        return _models_matches\n",
    "\n",
    "\n",
    "    ## Calculates probability of sentence combining DL models and ML models predictions\n",
    "    def calculate_matches_mixture(self, _sentences, _modelsML, _modelsDL, _tokenizersDL):\n",
    "        _repos_matches = {}\n",
    "        preds_ML = calculate_matches_ML(_sentences, _modelsML)\n",
    "        preds_DL = calculate_matches_DL(_sentences, _modelsDL, _tokenizersDL)\n",
    "\n",
    "        ## Getting classification confidence per model for each repo\n",
    "        for model_key in _modelsML.keys():\n",
    "            \n",
    "            preds1 = preds_ML[model_key]\n",
    "            preds2 = preds_DL[model_key]\n",
    "\n",
    "            _repos_matches[model_key] = preds1+preds2\n",
    "        return _repos_matches\n",
    "    \n",
    "    \n",
    "    ################################################\n",
    "    ## The files that have been produced by the audit tool are retrieved and consolodated into \n",
    "    ## a data frame, this data frame is then converted to tsv file to produce one TSV file with all audited results.\n",
    "    ## This file will be called: 'audited_ALL.tsv'\n",
    "    ##\n",
    "    ## A copy will be created with a datestamp should historic data ever be needed: 'audited_ALL_{date}-{time}.tsv'\n",
    "    ##\n",
    "    ## Any other process that wishes to use this data should read 'audited_ALL.tsv' which is the latest version.'''\n",
    "    ##############################\n",
    "    def aggregate_audited_data_from_git (self,\n",
    "                                         audited_data_dir_git,\n",
    "                                         github_audit_url, \n",
    "                                         audited_data_dir_local,\n",
    "                                         git_user, \n",
    "                                         git_pswd, \n",
    "                                         git_owner_repo,\n",
    "                                         git_owner,\n",
    "                                         git_repo) :\n",
    "       \n",
    "        audited_data_dir_git = audited_data_dir_git\n",
    "        github_audit_url = github_audit_url\n",
    "        audited_data_dir_local = audited_data_dir_local\n",
    "        git_user = git_user\n",
    "        git_pswd = git_pswd\n",
    "        git_owner_repo = git_owner_repo\n",
    "        git_owner = git_owner\n",
    "        git_repo = git_repo\n",
    "        aivm_helper = AIVM_helper()\n",
    "\n",
    "        # list all files from 'Audited' folder at cp_wssc/Data/Iterative-models-building/Classification%20results/Conventions/Audited/\n",
    "        # Helper file call to Returns a list of files from the github owner, repo and dir_ref\n",
    "        my_file_list = aivm_helper.list_files_from_github_dir (git_owner, git_repo, audited_data_dir_git)\n",
    "        ## Helper file call to Remove our standard excluded files and all files that are not \"audited_training_data\"\n",
    "        my_file_list = aivm_helper.remove_excluded_files_except(my_file_list, \"audited_training_data\")\n",
    "        if (len(my_file_list) > 0):\n",
    "            display (my_file_list.sort())\n",
    "            #Returns a dataframe of concatinated files from github, of names from file_list from the github url specified.\n",
    "            audited_df = aivm_helper.concat_files_from_github_dir (github_audit_url, my_file_list)\n",
    "            date, time = aivm_helper.time_stamp()\n",
    "            # file_name = 'audited_ALL_' + date + '-' + time + '.gz' # If using compression\n",
    "            #df.to_csv(FILE_PATH + file_name, sep='\\t', compression='gzip') # If using compression\n",
    "\n",
    "            file_name = 'audited_ALL_' + date + '-' + time + '.tsv'\n",
    "            file_name_unique = 'audited_ALL.tsv'\n",
    "        \n",
    "            audited_df.to_csv(audited_data_dir_local + file_name, sep='\\t')\n",
    "\n",
    "            #push to github\n",
    "            my_file_list = [audited_data_dir_local + file_name, audited_data_dir_local + file_name]\n",
    "            push_to_git_as = [audited_data_dir_git + file_name_unique, audited_data_dir_git + file_name]\n",
    "\n",
    "            commit, message = aivm_helper.save_to_github(git_user, git_pswd, git_owner_repo, my_file_list, push_to_git_as, \"auto push aggregated audit\")\n",
    "\n",
    "            if (commit != \"error\"):\n",
    "                print (\"File to commit:\", my_file_list[0])\n",
    "                print (\"Push to git as:\", push_to_git_as[0])\n",
    "                print (\"Commit: \", commit)\n",
    "                display(HTML(\"<font color='green'><b>Files Merged and Saved!!</b></font>\"))\n",
    "        \n",
    "            \n",
    "            if (commit == \"error\"):\n",
    "                print (\"File to commit: \", my_file_list[0])\n",
    "                print (\"Push to git as: \", push_to_git_as[0])\n",
    "                display(HTML(\"<font color='red'><b>Warning!!</b></font>\"))\n",
    "                print(message)\n",
    "    \n",
    "            return audited_df, message\n",
    "        else:\n",
    "            print (\"*****NO FILES RETRIEVED******\")\n",
    "        gc.collect()\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Method to get the latest aggregate audited data and to return a pandas DB containing the aggregated content.\n",
    "    def get_aggregated_gathered_data (self, use_git, \n",
    "                            gathered_data_dir_git,\n",
    "                            gathered_data_dir_local,\n",
    "                            github_gathered_url, \n",
    "                            git_owner,\n",
    "                            git_repo) :\n",
    "        use_git = use_git\n",
    "        gathered_data_dir_git = gathered_data_dir_git\n",
    "        gathered_data_dir_local = gathered_data_dir_local\n",
    "        github_gathered_url_path = github_gathered_url\n",
    "        git_owner = git_owner\n",
    "        git_repo = git_repo\n",
    "        \n",
    "        aivm_helper = AIVM_helper()\n",
    "\n",
    "    \n",
    "        gathered_conventions_files = []\n",
    "        gathered_conventions_data = {}\n",
    "        gathered_all_df = pd.DataFrame(columns=['text', 'provenance'])\n",
    "    \n",
    "        if (use_git == True):\n",
    "            gathered_conventions_files = aivm_helper.list_files_from_github_dir (git_owner, git_repo, gathered_data_dir_git)\n",
    "            gathered_conventions_files = aivm_helper.remove_excluded_files(gathered_conventions_files)\n",
    "                   #aivm_helper.remove_excluded_files_except(my_file_list, \"audited_training_data\")\n",
    "            #Read conventions data file from github. \n",
    "            for f in gathered_conventions_files:\n",
    "                gathered_conventions_data[aivm_helper.clean_file_name(f)] = pd.read_csv(github_gathered_url_path+f, sep='\\t', error_bad_lines=False)\n",
    "                 # NOTE: \"truncated\": false  we should check for truncated = true to do follow on call to get all files\n",
    "                tmp_df = gathered_conventions_data[aivm_helper.clean_file_name(f)]\n",
    "                tmp_df = tmp_df.rename(columns={\"sentence\": \"text\"})\n",
    "                gathered_all_df = gathered_all_df.append(tmp_df)\n",
    "                \n",
    "        elif (use_git == False):\n",
    "            #For each file() in gathered conventions folder\n",
    "            gathered_conventions_files = [f for f in os.listdir(gathered_data_dir_local)]\n",
    "            gathered_conventions_files = aivm_helper.remove_excluded_files(gathered_conventions_files) \n",
    "            for f in gathered_conventions_files:\n",
    "                gathered_conventions_data[aivm_helper.clean_file_name(f)] = pd.read_csv(os.path.join(gathered_data_dir_local, f), sep='\\t')\n",
    "                tmp_df = gathered_conventions_data[aivm_helper.clean_file_name(f)]\n",
    "                tmp_df = tmp_df.rename(columns={\"sentence\": \"text\"})\n",
    "                display (tmp_df.head())\n",
    "                gathered_all_df = gathered_all_df.append(tmp_df)\n",
    "        \n",
    "        gc.collect()\n",
    "        return gathered_conventions_files, gathered_all_df\n",
    "    \n",
    "    \n",
    "   \n",
    "    def get_training_data (self, use_git, \n",
    "                            training_data_dir_file_git,\n",
    "                            training_data_dir_file_local,\n",
    "                            git_owner,\n",
    "                            git_repo) :\n",
    "        use_git = use_git\n",
    "        training_data_dir_file_git = training_data_dir_file_git\n",
    "        training_data_dir_file_local = training_data_dir_file_local\n",
    "        git_owner = git_owner\n",
    "        git_repo = git_repo\n",
    "        \n",
    "        parsing_helpers = generic_parsing_helpers()\n",
    "\n",
    "    \n",
    "        if (use_git == True):\n",
    "            df = pd.read_csv(training_data_dir_file_git, sep='\\t', error_bad_lines=False, index_col=0)\n",
    "            \n",
    "   \n",
    "            \n",
    "        elif (use_git == False):\n",
    "            df = pd.read_csv(training_data_dir_file_local, sep='\\t', index_col=0)\n",
    "        \n",
    "       \n",
    "        df = parsing_helpers.get_clean_df_text_column(df, 'text')\n",
    "        gc.collect()\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    # Method to retrieve a pandas df created by a direct call to the audited data in github. returns the audited data \n",
    "    # in same format as the training data, so only with the new label, named 'label'\n",
    "    def retrieve_aggregate_audited_data_from_git (self, path) :\n",
    "        df = pd.read_csv(path, sep='\\t', error_bad_lines=False)\n",
    "        df = df.drop(df.columns[0], axis=1)\n",
    "        df = df.drop(['old'], axis=1)\n",
    "        df = df.rename(columns={'new': 'label'})\n",
    "        gc.collect()\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    # Method to retrieve a pandas df created by a direct call to the audited data in github. returns the audited data \n",
    "    # in same format as the training data, so only with the new label, named 'label'\n",
    "    def retrieve_tsv_data_from_git (self, path) :\n",
    "        df = pd.read_csv(path, sep='\\t', error_bad_lines=False)\n",
    "        df = df.drop(df.columns[0], axis=1)\n",
    "        gc.collect()\n",
    "        return df\n",
    "    \n",
    "\n",
    "    def create_label_balance (self, training_data_df, \n",
    "                              training_data_dir_local, \n",
    "                              training_data_dir_git,\n",
    "                              git_user, \n",
    "                              git_pswd, \n",
    "                              git_owner_repo,\n",
    "                             save_to_git):\n",
    "    \n",
    "        '''takes a pandas df, checks for an even balance of labels 0 and 1\n",
    "        per convention, and modifies data file to ensure even distribution,\n",
    "        the modified df is saved to git as a .tsv, a timestamped version is also created\n",
    "        the latest one will have title  'training_ALL.tsv'\n",
    "        the timestamped will have title 'training_ALL.{date}-{timme}.tsv'\n",
    "        the format as currently supported by each model =  label\ttext\tprovenance\tconvention\n",
    "        '''\n",
    "\n",
    "        aivm_helper = AIVM_helper()\n",
    "        conv_list = ['civic','domestic','green','industrial','inspired','market','project','renown']\n",
    "        \n",
    "        #The data format we need to return\n",
    "        new_training_data_df = pd.DataFrame(columns=['label','text', 'provenance', 'convention']) \n",
    "        \n",
    "        #Make sure index is ok on the input data.\n",
    "        training_data_df = training_data_df.reset_index(drop=True) \n",
    "        \n",
    "        #Must ensure that for each convention there is an even balance of 0 and 1 labels\n",
    "        print (\"The number of Class 1 v's Class 0 per convention in combined data\")\n",
    "        counts = training_data_df[conv_list].apply(pd.Series.value_counts)\n",
    "        display (counts)\n",
    "        conv_label_0 = []\n",
    "        conv_label_1 = []\n",
    "        # we should have the same amount of label 0 as label 1.\n",
    "        # if we have too many 1,s we can fill with 0(not_convention)\n",
    "        for conv in conv_list:\n",
    "            balance = counts[conv][1]\n",
    "            conv_one = training_data_df[(training_data_df[conv] == 1)]\n",
    "            #As there is excess of label 1 we will get label 0's from the other conventions.\n",
    "            conv_zero = training_data_df[(training_data_df[conv] == 0)].sample(n=balance)\n",
    "            temp = pd.concat([conv_one, conv_zero], sort=False)\n",
    "            #FORMAT FROM: text\tprovenance\tcivic\tdomestic\tgreen\tindustrial\tinspired\tmarket\tproject\trenown\n",
    "            #FORMAT TO: label\ttext\tprovenance\tconvention\n",
    "            temp = temp.rename(columns={conv: 'label'})\n",
    "            temp['convention'] = conv\n",
    "            temp =  temp[['label','text', 'provenance' , 'convention']]    \n",
    "            new_training_data_df =  pd.concat([new_training_data_df, temp], sort=False) \n",
    "\n",
    "\n",
    "        date, time = aivm_helper.time_stamp()\n",
    "        file_name = 'training_ALL_' + date + '-' + time + '.tsv'\n",
    "        file_name_unique = 'training_ALL.tsv'\n",
    "        file_name_two = 'training_ALL_Multi_format' + date + '-' + time + '.tsv'\n",
    "        file_name_unique_two = 'training_ALL_Multi_format.tsv'\n",
    "        \n",
    "        new_training_data_df = new_training_data_df.reset_index(drop=True)\n",
    "        new_training_data_df.to_csv(training_data_dir_local + file_name, sep='\\t')\n",
    "        new_training_data_df.to_csv(training_data_dir_local + file_name_unique, sep='\\t')\n",
    "        \n",
    "        training_data_df.to_csv(training_data_dir_local + file_name_two, sep='\\t')\n",
    "        training_data_df.to_csv(training_data_dir_local + file_name_unique_two, sep='\\t')\n",
    "        \n",
    "        if (save_to_git == True):\n",
    "             #push to github\n",
    "            my_file_list = [training_data_dir_local + file_name, \n",
    "                            training_data_dir_local + file_name,\n",
    "                           training_data_dir_local + file_name_two, \n",
    "                            training_data_dir_local + file_name_two]\n",
    "            \n",
    "            \n",
    "            push_to_git_as = [training_data_dir_git + file_name_unique, \n",
    "                              training_data_dir_git + file_name,\n",
    "                              training_data_dir_git + file_name_unique_two, \n",
    "                              training_data_dir_git + file_name_two\n",
    "                             ]\n",
    "\n",
    "            commit, message = aivm_helper.save_to_github(git_user, git_pswd, git_owner_repo, my_file_list, push_to_git_as, \"auto push aggregated audit\")\n",
    "\n",
    "            if (commit != \"error\"):\n",
    "                print (\"File to commit:\", my_file_list[0])\n",
    "                print (\"Push to git as:\", push_to_git_as[0])\n",
    "                print (\"Commit: \", commit)\n",
    "                display(HTML(\"<font color='green'><b>Files Merged and Saved!!</b></font>\"))\n",
    "        \n",
    "            \n",
    "            if (commit == \"error\"):\n",
    "                print (\"File to commit: \", my_file_list)\n",
    "                print (\"Push to git as: \", push_to_git_as)\n",
    "                display(HTML(\"<font color='red'><b>Warning!!</b></font>\"))\n",
    "                print(message)\n",
    "        \n",
    "        countsNew = pd.crosstab(new_training_data_df['label'],new_training_data_df['convention'])\n",
    "        return new_training_data_df, countsNew\n",
    "    \n",
    "    \n",
    "#OLD VERSION - DELETE LATER\n",
    "#     def create_label_balance (self, new_training_data_df, \n",
    "#                               training_data_dir_local, \n",
    "#                               training_data_dir_git,\n",
    "#                               git_user, \n",
    "#                               git_pswd, \n",
    "#                               git_owner_repo,\n",
    "#                              save_to_git):\n",
    "    \n",
    "#         '''takes a pandas df, checks for an even balance of labels 0 and 1\n",
    "#         per convention, and modifies data file to ensure even distribution,\n",
    "#         the modified df is saved to git as a .tsv, a timestamped version is also created\n",
    "#         the latest one will have title  'training_ALL.tsv'\n",
    "#         the timestamped will have title 'training_ALL.{date}-{timme}.tsv'\n",
    "#         '''\n",
    "#         #if we don't reset the index then we remove too many records.\n",
    "#         new_training_data_df = new_training_data_df.reset_index(drop=True)\n",
    "    \n",
    "#         aivm_helper = AIVM_helper()\n",
    "    \n",
    "#         #Must ensure that for each convention there is an even balance of 0 and 1 labels\n",
    "#         counts = pd.crosstab(new_training_data_df['label'],new_training_data_df['convention'])\n",
    "#         conv_list = new_training_data_df.convention.unique()\n",
    "#         conv_label_0 = []\n",
    "#         conv_label_1 = []\n",
    "#         # we should have the same amount of label 0 as label 1.\n",
    "#         # if we have too many 1,s we can fill with 0(not_convention)\n",
    "#         for conv in conv_list:\n",
    "#             print (conv)\n",
    "#             print (\"label 0:\",counts[conv][0] )\n",
    "#             print (\"label 1:\",counts[conv][1] )\n",
    "#             conv_label_0.append(counts[conv][0])\n",
    "#             conv_label_1.append(counts[conv][1])\n",
    "#             excessOne = (counts[conv][1] - counts[conv][0])\n",
    "#             excessZero = (counts[conv][0] - counts[conv][1])\n",
    "#             if (excessZero > 0):\n",
    "#                 print (\"Training data size: \", new_training_data_df.shape[0])\n",
    "#                 print (\"Excess of label 0 by:\", excessZero)\n",
    "#                  #As there is excess of label 0 we can remove excessZero num of label 0's for this conventions.\n",
    "#                 balance = new_training_data_df[(new_training_data_df['convention'] == conv) & (new_training_data_df['label']== 0)].sample(n=excessZero)\n",
    "#                 #display(balance.head(5))\n",
    "#                 print (\"Balancing df to remove: \", balance.shape[0])\n",
    "#                 print (\"num of index to rem:\", len(balance.index))\n",
    "#                 new_training_data_df = new_training_data_df.drop(balance.index)\n",
    "#                 print (\"Training data size after removal: \", new_training_data_df.shape[0])\n",
    "         \n",
    "#             if (excessOne > 0):\n",
    "#                 print (\"Training data size: \", new_training_data_df.shape[0])\n",
    "#                 print (\"Excess of label 1 by: \", excessOne )\n",
    "#                 #As there is excess of label 1 we can add additional label 0's from the other conventions.\n",
    "#                 balance = new_training_data_df[(new_training_data_df['convention'] != conv) & (new_training_data_df['label']== 1)].sample(n=excessOne)\n",
    "#                 #now we should convert thos to convention = {convention} and label = 0.\n",
    "#                 balance['convention'] = conv\n",
    "#                 balance['label'] = 0\n",
    "#                 #display(balance.head(5))\n",
    "#                 #then concatenate this to the new_training_data_df\n",
    "#                 print (\"Balancing df to add: \", balance.shape[0])\n",
    "#                 new_training_data_df =  pd.concat([new_training_data_df, balance], sort=True) \n",
    "                \n",
    "#                 print (\"Training data size after addition: \", new_training_data_df.shape[0])\n",
    "#             print(\"\")\n",
    "        \n",
    "#         date, time = aivm_helper.time_stamp()\n",
    "#         file_name = 'training_ALL_' + date + '-' + time + '.tsv'\n",
    "#         file_name_unique = 'training_ALL.tsv'\n",
    "        \n",
    "#         new_training_data_df = new_training_data_df.reset_index(drop=True)\n",
    "\n",
    "        \n",
    "#         new_training_data_df.to_csv(training_data_dir_local + file_name, sep='\\t')\n",
    "#         new_training_data_df.to_csv(training_data_dir_local + file_name_unique, sep='\\t')\n",
    "#         if (save_to_git == True):\n",
    "#             #push to github\n",
    "#             my_file_list = [training_data_dir_local + file_name, training_data_dir_local + file_name]\n",
    "#             push_to_git_as = [training_data_dir_git + file_name_unique, training_data_dir_git + file_name]\n",
    "\n",
    "#             commit, message = aivm_helper.save_to_github(git_user, git_pswd, git_owner_repo, my_file_list, push_to_git_as, \"auto push aggregated audit\")\n",
    "\n",
    "#             if (commit != \"error\"):\n",
    "#                 print (\"File to commit:\", my_file_list[0])\n",
    "#                 print (\"Push to git as:\", push_to_git_as[0])\n",
    "#                 print (\"Commit: \", commit)\n",
    "#                 display(HTML(\"<font color='green'><b>Files Merged and Saved!!</b></font>\"))\n",
    "        \n",
    "            \n",
    "#             if (commit == \"error\"):\n",
    "#                 print (\"File to commit: \", my_file_list)\n",
    "#                 print (\"Push to git as: \", push_to_git_as)\n",
    "#                 display(HTML(\"<font color='red'><b>Warning!!</b></font>\"))\n",
    "#                 print(message)\n",
    "        \n",
    "#         countsNew = pd.crosstab(new_training_data_df['label'],new_training_data_df['convention'])\n",
    "#         return new_training_data_df, counts, countsNew\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
