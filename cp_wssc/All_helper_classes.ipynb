{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models training helper functions - note these are Helper Classes with helper methods, this file should contain *no* stand alone executable code.\n",
    "\n",
    "###This file contains 4 seperate heplper classes\n",
    "    -class generic_parsing_helpers:\n",
    "    -class AIVM_helper: (including github read/write and parsing methods\n",
    "    -class roc_callback(Callback)\n",
    "    -class model_helpers:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-12T15:39:42.826638Z",
     "start_time": "2019-10-12T15:39:42.807998Z"
    }
   },
   "source": [
    "### This file contains the following helper files:\n",
    "\n",
    "#### read_glove_embeddings,  vocabulary loading\n",
    "def read_glove_embeddings():\n",
    "\n",
    "\n",
    "#### get_stop_words, returns an immutable frozenset set of stopwords from a file.\n",
    "def get_stop_words(stop_file_path):\n",
    "\n",
    "\n",
    "#### clean_line, cleaning function which calls a stemmer, cleans the text and returns a string\n",
    "def clean_line(X):\n",
    "\n",
    "\n",
    "#### def create_tokenizer, create a tokenizer given a data frame with a column \"text\"\n",
    "def create_tokenizer(df, max_words=MAX_NB_WORDS):\n",
    "\n",
    "\n",
    "#### roc_callback, calculate AUC for validation when training the model\n",
    "##As explained here: https://stackoverflow.com/questions/41032551/how-to-compute-receiving-##operating-characteristic-roc-and-auc-in-keras\n",
    "class roc_callback(Callback):\n",
    "\n",
    "\n",
    " \n",
    "AF : this file should be retained as a helper file so it should not really make any calls\n",
    "but rather just have methods dafined. \n",
    "\n",
    "it's calling: \n",
    "embeddings_index = read_glove_embeddings() \n",
    "stopwords=get_stop_words(\"Data/Iterative-models-building/Training data/resources/stopwords.txt\")\n",
    "\n",
    "\n",
    "AF:\n",
    "Questions.\n",
    "1. Where does the stopwords list come from? is there a reason not to use the NLTK onne instead of storing locally?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T16:54:56.477509Z",
     "start_time": "2019-12-01T16:54:56.303115Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Necessary libraries imported.\n"
     ]
    }
   ],
   "source": [
    "# Required Python utilities\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "from langdetect import detect\n",
    "from bs4 import BeautifulSoup\n",
    "from markdown import markdown\n",
    "from lxml import etree\n",
    "import random\n",
    "import tqdm\n",
    "import itertools \n",
    "import pickle\n",
    "import os\n",
    "import base64\n",
    "import io\n",
    "from io import BytesIO\n",
    "import urllib \n",
    "from collections import Counter\n",
    "from langdetect import detect\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "import time\n",
    "from PIL import Image as PIL_Img\n",
    "from datetime import datetime\n",
    "import json\n",
    "import gc\n",
    "import requests as req\n",
    "import markdown\n",
    "import html\n",
    "import statistics\n",
    "import plotly_express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from plotly.subplots import make_subplots\n",
    "import gzip\n",
    "\n",
    "## Deep Learning imports for the classifiers\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, Concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "##Â ML required imports (for clustering)\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA, LatentDirichletAllocation\n",
    "from sklearn.preprocessing import scale, StandardScaler\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV \n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import log_loss, brier_score_loss, accuracy_score, hamming_loss\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "# Topic modeling imports\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "##Â NLP related imports\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "# visualization imports\n",
    "from IPython.display import display, display_html, clear_output, HTML, Image\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.transforms as mtransforms\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "\n",
    "\n",
    "from github import Github, GithubException, InputGitTreeElement\n",
    "\n",
    "from matplotlib import colors as mpcolor\n",
    "from scipy import stats\n",
    "import math\n",
    "import shelve\n",
    "print (\"Necessary libraries imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a default model_helpers for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-19T10:59:22.868644Z",
     "start_time": "2019-10-19T10:59:22.855391Z"
    }
   },
   "outputs": [],
   "source": [
    "def default_model_helpers_for_project ():\n",
    "    \n",
    "    #Classificaiton NETWORKs Configuration parameters\n",
    "    MAX_SEQUENCE_LENGTH = 32\n",
    "    MAX_NB_WORDS = 10000\n",
    "    EMBEDDING_DIM = 100 # 100, 200 or 300\n",
    "    VALIDATION_SPLIT = 0.2\n",
    "    TFIDF_MAX_FEATURES=10000\n",
    "    NUM_EPOCHS = 20\n",
    "    \n",
    "    #from \"Models training helpers.ipynb\". model_helpers class is defined further below. \n",
    "    def_model_helpers = model_helpers (seq_len=MAX_SEQUENCE_LENGTH, \n",
    "                                   max_num_wrds=MAX_NB_WORDS, \n",
    "                                   embedding_dim=EMBEDDING_DIM,\n",
    "                                   validation_splt =VALIDATION_SPLIT,\n",
    "                                   tfidf_max = TFIDF_MAX_FEATURES,\n",
    "                                   num_epochs = NUM_EPOCHS)\n",
    "  \n",
    "\n",
    "    HTML_list = []\n",
    "    HTML = ''      \n",
    "    HTML_list.append ('You will find instantiate_model_helpers_for_project at the very TOP of All_helper_classes.ipynb, If you do not want to change the default config, used across all files you can instantiate your own model_helpers() instead.')\n",
    "    HTML_list.append('<br><b>MAX_SEQUENCE_LENGTH:</b> '+ str(MAX_SEQUENCE_LENGTH))\n",
    "    HTML_list.append('<br><b>MAX_NB_WORDS:</b> '+ str(MAX_NB_WORDS))\n",
    "    HTML_list.append('<br><b>EMBEDDING_DIM:</b> '+ str(EMBEDDING_DIM))\n",
    "    HTML_list.append('<br><b>VALIDATION_SPLIT:</b> ' + str(VALIDATION_SPLIT))\n",
    "    HTML_list.append('<br><b>TFIDF_MAX_FEATURES:</b> ' + str(TFIDF_MAX_FEATURES))\n",
    "    HTML_list.append('<br><b>NUM_EPOCHS '  + str(NUM_EPOCHS))\n",
    "    for line in HTML_list:\n",
    "            HTML = HTML + line \n",
    "            \n",
    "    return def_model_helpers, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a default list of paths for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-30T16:23:30.762477Z",
     "start_time": "2019-11-30T16:23:30.727815Z"
    }
   },
   "outputs": [],
   "source": [
    "# If you add any paths to this make sure to also add to the display command \n",
    "# so the calling notebook can see all available paths\n",
    "class project_paths:\n",
    "    def __init__(self):\n",
    "        #self.something = kwargs['something']\n",
    "\n",
    "        self.GIT_OWNER = 'aideenf'\n",
    "        self.GIT_REPO = 'AIVC'\n",
    "        self.GIT_OWNER_REPO = self.GIT_OWNER + '/' + self.GIT_REPO\n",
    "        self.BASE_DIR_LOCAL = './Data/Iterative-models-building'\n",
    "        self.GLOVE_DIR_LOCAL = 'Data/Iterative-models-building/Training data/'\n",
    "\n",
    "        self.GATHERED_DATA_CONV_DIR_LOCAL = 'Data/Iterative-models-building/Gathered_data/Conventions/'\n",
    "        self.GATHERED_DATA_CONV_DIR_GIT = 'cp_wssc/Data/Iterative-models-building/Gathered_data/Conventions/'\n",
    "        self.GITHUB_GATHERED_URL_PATH = 'https://raw.githubusercontent.com/aideenf/AIVC/master/cp_wssc/Data/Iterative-models-building/Gathered_data/Conventions/'\n",
    "\n",
    "        self.MODELS_DIR_LOCAL = \"./Data/Iterative-models-building/Models/\"\n",
    "        \n",
    "        self.RESOURCES_DIR_LOCAL = 'Data/Iterative-models-building/Resources/'\n",
    "        self.RESOURCES_DIR_GIT = 'cp_wssc/Data/Iterative-models-building/Resources/'\n",
    "        self.RESOURCES_URL_GIT = 'https://github.com/aideenf/AIVC/blob/master/cp_wssc/Data/Iterative-models-building/Resources/'\n",
    "        self.STOP_WORDS_DIR_FILE_LOCAL = 'Data/Iterative-models-building/Resources/stopwords.txt'\n",
    "        self.STOP_WORDS_URL_GIT = 'https://raw.githubusercontent.com/aideenf/AIVC/master/cp_wssc/Data/Iterative-models-building/Resources/stopwords.txt'\n",
    "\n",
    "        self.LABELLED_DATA_DIR_FILE_LOCAL = 'Data/Iterative-models-building/Labelled_data/Conventions/labelled_data_original.tsv'\n",
    "        self.LABELLED_DATA_DIR_FILE_GIT = 'https://raw.githubusercontent.com/aideenf/AIVC/master/cp_wssc/Data/Iterative-models-building/Labelled_data/Conventions/labelled_data_original.tsv'\n",
    "        self.LABELLED_DATA_ALL_DIR_FILE_GIT = 'https://raw.githubusercontent.com/aideenf/AIVC/master/cp_wssc/Data/Iterative-models-building/Labelled_data/Conventions/labelled_ALL.tsv'\n",
    "        self.LABELLED_DATA_ALL_DIR_FILE_LOCAL = 'Data/Iterative-models-building/Labelled_data/Conventions/labelled_ALL.tsv'\n",
    "        self.LABELLED_DATA_DIR_LOCAL = 'Data/Iterative-models-building/Labelled_data/Conventions/'\n",
    "        self.LABELLED_DATA_DIR_GIT = 'cp_wssc/Data/Iterative-models-building/Labelled_data/Conventions/'\n",
    "      \n",
    "        \n",
    "#         self.TRAINING_DATA_DIR_FILE_LOCAL = 'Data/Iterative-models-building/Training data/Conventions/training_data_original.tsv'\n",
    "#         self.TRAINING_DATA_DIR_FILE_GIT = 'https://raw.githubusercontent.com/aideenf/AIVC/master/cp_wssc/Data/Iterative-models-building/Training%20data/Conventions/labelled_data_original.tsv'\n",
    "#         self.TRAINING_DATA_ALL_DIR_FILE_GIT = 'https://raw.githubusercontent.com/aideenf/AIVC/master/cp_wssc/Data/Iterative-models-building/Training%20data/Conventions/training_ALL.tsv'\n",
    "#         self.TRAINING_DATA_ALL_DIR_FILE_LOCAL = 'Data/Iterative-models-building/Training data/Conventions/training_ALL.tsv'\n",
    "#         self.TRAINING_DATA_ALL_MULTI_DIR_FILE_GIT = 'https://raw.githubusercontent.com/aideenf/AIVC/master/cp_wssc/Data/Iterative-models-building/Training%20data/Conventions/training_ALL_Multi_format.tsv'\n",
    "#         self.TRAINING_DATA_ALL_MULTI_DIR_FILE_LOCAL = 'Data/Iterative-models-building/Training data/Conventions/training_ALL_Multi_format.tsv'\n",
    "#         self.TRAINING_DATA_DIR_LOCAL = 'Data/Iterative-models-building/Training data/Conventions/'\n",
    "#         self.TRAINING_DATA_DIR_GIT = 'cp_wssc/Data/Iterative-models-building/Training data/Conventions/'\n",
    "\n",
    "        self.AUDITED_DATA_DIR_LOCAL  = './Data/Iterative-models-building/Classification results/Conventions/Audited/'\n",
    "        self.AUDITED_DATA_DIR_GIT =  'cp_wssc/Data/Iterative-models-building/Classification results/Conventions/Audited/'\n",
    "        self.GITHUB_AUDIT_URL_PATH = 'https://raw.githubusercontent.com/aideenf/AIVC/master/cp_wssc/Data/Iterative-models-building/Classification%20results/Conventions/Audited/'\n",
    "        self.GITHUB_AGGREGATED_AUDIT_URL_FILE = 'https://raw.githubusercontent.com/aideenf/AIVC/master/cp_wssc/Data/Iterative-models-building/Classification%20results/Conventions/Audited/audited_ALL.tsv'\n",
    "\n",
    "        self.CLASSIFIED_DATA_DIR_LOCAL  = './Data/Iterative-models-building/Classification results/Conventions/'\n",
    "        self.CLASSIFIED_DATA_DIR_GIT =  'cp_wssc/Data/Iterative-models-building/Classification results/Conventions/'\n",
    "        \n",
    "\n",
    "        self.CONV_MODEL_PCKL_URL = 'https://github.com/aideenf/AIVC/blob/master/cp_wssc/Data/Iterative-models-building/Models/conv_models_items.pickle?raw=true'\n",
    "        self.SIMPLER_MODEL_PCKL_URL = 'https://github.com/aideenf/AIVC/blob/master/cp_wssc/Data/Iterative-models-building/Models/Simpler_ML_models.pickle?raw=true'\n",
    "\n",
    "        if not os.path.exists(self.GLOVE_DIR_LOCAL):\n",
    "            os.makedirs(self.GLOVE_DIR_LOCAL)\n",
    "\n",
    "        if not os.path.exists(self.GATHERED_DATA_CONV_DIR_LOCAL):\n",
    "            os.makedirs(self.GATHERED_DATA_CONV_DIR_LOCAL)\n",
    "\n",
    "        if not os.path.exists(self.MODELS_DIR_LOCAL):\n",
    "            os.makedirs(self.MODELS_DIR_LOCAL)\n",
    "    \n",
    "        if not os.path.exists(self.AUDITED_DATA_DIR_LOCAL):\n",
    "            os.makedirs(self.AUDITED_DATA_DIR_LOCAL)\n",
    "            \n",
    "        if not os.path.exists(self.LABELLED_DATA_DIR_FILE_LOCAL):\n",
    "            os.makedirs(self.LABELLED_DATA_DIR_FILE_LOCAL)\n",
    "            \n",
    "    def get_paths_data (self):\n",
    "        HTML_list = []\n",
    "        HTML = ''\n",
    "        HTML_list.append(\"<b>*****PROJECT PATHS*******</b>\")    \n",
    "        HTML_list.append('<br><b>paths.BASE_DIR_LOCAL: </b>' + self.BASE_DIR_LOCAL)\n",
    "        HTML_list.append('<br><b>paths.GLOVE_DIR_LOCAL: </b>' + self.GLOVE_DIR_LOCAL)\n",
    "        HTML_list.append('<br><b>paths.GATHERED_DATA_CONV_DIR_LOCAL: </b>' + self.GATHERED_DATA_CONV_DIR_LOCAL) \n",
    "        HTML_list.append('<br><b>paths.GATHERED_DATA_CONV_DIR_GIT: </b>' + self.GATHERED_DATA_CONV_DIR_GIT)\n",
    "        HTML_list.append('<br><b>paths.GITHUB_GATHERED_URL_PATH: </b>' + self.GITHUB_GATHERED_URL_PATH) \n",
    "        HTML_list.append('<br><b>paths.MODELS_DIR_LOCAL: </b>' + self.MODELS_DIR_LOCAL)\n",
    "        HTML_list.append('<br><b>paths.STOP_WORDS_DIR_FILE_LOCAL: </b>' + self.STOP_WORDS_DIR_FILE_LOCAL) \n",
    "        HTML_list.append('<br><b>paths.STOP_WORDS_URL_GIT: </b>' + self.STOP_WORDS_URL_GIT) \n",
    "        \n",
    "        HTML_list.append('<br><b>RESOURCES_DIR_LOCAL: </b>' + self.RESOURCES_DIR_LOCAL)\n",
    "        HTML_list.append('<br><b>RESOURCES_DIR_GIT:  </b>' + self.RESOURCES_DIR_GIT)\n",
    "        \n",
    "        HTML_list.append('<br><b>paths.LABELLED_DATA_DIR_FILE_LOCAL: </b>' + self.LABELLED_DATA_DIR_FILE_LOCAL) \n",
    "        HTML_list.append('<br><b>paths.LABELLED_DATA_DIR_FILE_GIT: </b>' + self.LABELLED_DATA_DIR_FILE_GIT) \n",
    "        HTML_list.append('<br><b>paths.LABELLED_DATA_ALL_DIR_FILE_GIT: </b>' +self.LABELLED_DATA_ALL_DIR_FILE_GIT)\n",
    "        HTML_list.append('<br><b>paths.LABELLED_DATA_ALL_DIR_FILE_LOCAL: </b>' +self.LABELLED_DATA_ALL_DIR_FILE_LOCAL)\n",
    "        HTML_list.append('<br><b>paths.LABELLED_DATA_DIR_LOCAL: </b>' + self.LABELLED_DATA_DIR_LOCAL) \n",
    "        HTML_list.append('<br><b>paths.LABELLED_DATA_DIR_GIT: </b>' + self.LABELLED_DATA_DIR_GIT)        \n",
    "        \n",
    "        HTML_list.append('<br><b>paths.AUDITED_DATA_DIR_LOCAL: </b>' + self.AUDITED_DATA_DIR_LOCAL)\n",
    "        HTML_list.append('<br><b>paths.AUDITED_DATA_DIR_GIT: </b>' + self.AUDITED_DATA_DIR_GIT)\n",
    "        HTML_list.append('<br><b>paths.GITHUB_AUDIT_URL_PATH: </b>' + self.GITHUB_AUDIT_URL_PATH )\n",
    "        HTML_list.append('<br><b>paths.GITHUB_AGGREGATED_AUDIT_URL_FILE: </b>' + self.GITHUB_AGGREGATED_AUDIT_URL_FILE) \n",
    "        HTML_list.append('<br><b>paths.CONV_MODEL_PCKL_URL: </b>' + self.CONV_MODEL_PCKL_URL )\n",
    "        HTML_list.append('<br><b>paths.SIMPLER_MODEL_PCKL_URL: </b>' + self.SIMPLER_MODEL_PCKL_URL)\n",
    "        HTML_list.append('<br><b>paths.CLASSIFIED_DATA_DIR_LOCAL: </b>' + self.CLASSIFIED_DATA_DIR_LOCAL )\n",
    "        HTML_list.append('<br><b>paths.CLASSIFIED_DATA_DIR_GIT: </b>' + self.CLASSIFIED_DATA_DIR_GIT)\n",
    "        for line in HTML_list:\n",
    "            HTML = HTML + line\n",
    "            \n",
    "        return HTML\n",
    "    \n",
    "    def get_repo_data (self):\n",
    "        HTML_list = []\n",
    "        HTML = ''\n",
    "        HTML_list.append ('<b>*****GIT PROJECT*******</b>') \n",
    "        HTML_list.append('<br><b>paths.GIT_OWNER:</b> ' + self.GIT_OWNER)\n",
    "        HTML_list.append('<br><b>paths.GIT_REPO:</b> ' + self.GIT_REPO)\n",
    "        HTML_list.append('<br><b>paths.GIT_OWNER_REPO:</b> ' + self.GIT_OWNER_REPO)\n",
    "        for line in HTML_list:\n",
    "            HTML = HTML + line\n",
    "            \n",
    "        return HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic parsing helper methods in class generic_parsing_helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T14:57:41.624641Z",
     "start_time": "2019-11-27T14:57:41.484346Z"
    }
   },
   "outputs": [],
   "source": [
    "## these are inputs to the helper functions on contextual basis and should belong to a seperate file. \n",
    "\n",
    "class generic_parsing_helpers:\n",
    "    \n",
    "    def __init__(self, **kwargs ):\n",
    "        #self.something = kwargs['something']\n",
    "        print (\"Initialised generic_parsing_helpers class and methods\")\n",
    "        \n",
    "        \n",
    "    ## Vocabulary loading\n",
    "    def read_glove_embeddings(self, glove_dir, embedding_dim):\n",
    "        ## Reading GLOVE (precalculated word embeddings)\n",
    "        embeddings_index = {}\n",
    "        f = open(os.path.join(glove_dir, 'glove.6B.{}d.txt'.format(embedding_dim)))\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        f.close()\n",
    "    \n",
    "        return embeddings_index\n",
    "   \n",
    "\n",
    "    ## Stop words list \n",
    "    def get_stop_words(self, stop_file_path):\n",
    "        \"\"\"load stop words \"\"\"\n",
    "    \n",
    "        with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
    "            stopwords = f.readlines()\n",
    "            stop_set = set(m.strip() for m in stopwords)\n",
    "            return frozenset(stop_set)\n",
    "        #load a set of stop words\n",
    "    \n",
    "    \n",
    "    ## Text cleaning functions\n",
    "    def clean_line(self, X):\n",
    "    \n",
    "        stemmer = WordNetLemmatizer()\n",
    "\n",
    "        # Remove all the special characters\n",
    "        line = re.sub(r'\\W', ' ', str(X))\n",
    "\n",
    "        # remove all single characters\n",
    "        line = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', line)\n",
    "\n",
    "        # Remove single characters from the start\n",
    "        line = re.sub(r'\\^[a-zA-Z]\\s+', ' ', line) \n",
    "\n",
    "        # Substituting multiple spaces with single space\n",
    "        line = re.sub(r'\\s+', ' ', line, flags=re.I)\n",
    "\n",
    "        # Removing prefixed 'b'\n",
    "        line = re.sub(r'^b\\s+', '', line)\n",
    "\n",
    "        # Converting to Lowercase\n",
    "        line = line.lower()\n",
    "\n",
    "        # Lemmatization\n",
    "        line = line.split()\n",
    "\n",
    "        document2 = [stemmer.lemmatize(word) for word in line]\n",
    "        line = ' '.join(document2)\n",
    "\n",
    "        return line\n",
    "\n",
    "\n",
    "    def clean_str(self, text):\n",
    "        documents = []\n",
    "        ret = documents\n",
    "        if type(text == \"list\"):\n",
    "            #print(\"list: \", text)\n",
    "            for X in text:\n",
    "                documents.append(clean_line(X))\n",
    "    \n",
    "            ret =\"\"\n",
    "            if(len(documents)>1):\n",
    "\n",
    "                for d in documents:\n",
    "                    ret+=d+\"\\n\"\n",
    "            else:\n",
    "                ret = documents[0]\n",
    "        \n",
    "        elif type(text)==\"str\":\n",
    "            p#rint(\"str\")\n",
    "            ret = clear_line(text)\n",
    "        return ret\n",
    "    \n",
    "    \n",
    "    def remove_line_breaks(self, text_str):\n",
    "        text_str = text_str.replace(\"\\\\n\", \" \")\n",
    "        return text_str\n",
    " \n",
    "\n",
    "    #this seems to remove line breaks from a column of data frame\n",
    "    # renamed generically from def read_training_data(df_path_file, df_text_column):\n",
    "    def get_clean_df_text_column(self, df, df_text_column):\n",
    "        df[df_text_column] = df[df_text_column].apply(self.remove_line_breaks)\n",
    "        return df\n",
    "\n",
    "\n",
    "    ## Helper function to create a KERAS tokenizer apn apply to column, 'column name' of a givem pandas data frame with a column \"text\" and a maximum number of words to \n",
    "    ## fit_on_texts Updates internal vocabulary based on a list of texts. \n",
    "    ## This method creates the vocabulary index based on word frequency. So if you give it something like,\n",
    "    ## \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 1; word_index[\"cat\"] = 2 \n",
    "    ## it is word -> index dictionary so every word gets a unique integer value. \n",
    "    ## 0 is reserved for padding. So lower integer means more frequent word (often the first few are stop words \n",
    "    ## because they appear a lot).\n",
    "\n",
    "    def create_tokenizer(self, df, df_column_to_tokenize, max_words):\n",
    "        \n",
    "        texts = df[df_column_to_tokenize].values\n",
    "    \n",
    "        _tokenizer = Tokenizer(num_words=max_words, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "        _tokenizer.fit_on_texts(texts)\n",
    "    \n",
    "        return _tokenizer \n",
    "    \n",
    "    \n",
    "    def clean_description (self, text):\n",
    "        #https://regex101.com/r/cO8lqs/2\n",
    "        #remove html markup\n",
    "        text = re.sub(\"(<.*?>)\",\"\",text)\n",
    "        text = re.sub(\"[\\|\\[].*?[\\|\\]]\", \"\", text)\n",
    "        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text, flags=re.MULTILINE)\n",
    "        text = text.replace(\"-\", \" \")# Added by Aideen\n",
    "        # general\n",
    "        text = text.replace(\"can't\", \"can not\")# Added by Aideen\n",
    "        text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "        text = re.sub(r\"\\'re\", \" are\", text)\n",
    "        text = re.sub(r\"\\'s\", \" is\", text)\n",
    "        text = re.sub(r\"\\'d\", \" would\", text)\n",
    "        text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "        text = re.sub(r\"\\'t\", \" not\", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "        text = re.sub(r\"\\'m\", \" am\", text)\n",
    "                    \n",
    "        text = html.unescape(text)# Added by Aideen remove the escape chars\n",
    "        text = text.replace(\"\\\\n\", \"\\n\")# Added by Aideen\n",
    "        text = text.replace(\"\\n\", \" \")# Added by Aideen\n",
    "        # Remove all the special characters except spaces, dashes, commas and dots\n",
    "        text = re.sub(r'\\. (?!\\d)', '.\\n ', str(text))\n",
    "        text = re.sub(r\"[^\\s.,\\-a-zA-Z0-9]\", ' ', str(text))\n",
    "        text = text.replace('\\\\xe\\\\x\\\\x', \"'\")## Added by Aideen\\xe\\x\\x\n",
    "        # sent = sent.replace(\"\\\\n\", \"\\n\")# Added by Aideen\n",
    "        text = aivm_helper.pre_process_sentence(text)\n",
    "        text = re.sub(r'^b\\s+', '', str(text))\n",
    "        text = text.replace(\".\\n.\\n\", \".\\n\")\n",
    "        text = text.replace(\". \", \".\")\n",
    "        text = re.sub(r\"[ ][x][a-z][a-z]\", \"\", text)\n",
    "        text = re.sub(r\"[ ][x][a-z]\", \"\", text)\n",
    "        text = re.sub(r\"[ ][x]\", \" \", text)\n",
    "        text = re.sub(' +', ' ', text)\n",
    "        text = re.sub(r\"\\b[b-zA-Z]\\b\", \"\", text)\n",
    "        text = re.sub(r\"[.][ ][.]\", \".\", text)\n",
    "        text = re.sub(r\"[.][.]\", \".\", text)\n",
    "        return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing and Github helpers, should check for duplications between these and generic_parsing_helpers above and consolidate.\n",
    "\n",
    "        -markdown_to_text()\n",
    "        -remove_nums_from_str(s)\n",
    "        -pre_process_sentence(sent)\n",
    "        -pre_process(textBlob) returns tokenized to sentences\n",
    "        -remove_excluded_files(file_list)\n",
    "        -remove_excluded_files_except(file_list, except_with_text)\n",
    "        -rreplace(s, old, new, occurrence)\n",
    "        -clean_file_name(name, replacements2=[]):\n",
    "        -def save_to_github(git_user, git_password, git_repo, my_file_list, push_to_git_as):\n",
    "        -def list_files_from_github_dir (owner, repo, dir_ref):\n",
    "        -keep_pdf_urls_only(file_list):\n",
    "        -def concat_files_from_github_dir (directory_base_url, file_list):\n",
    "        -def read_single_file_from_github_dir (directory_base_url, file_name):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T16:44:57.038539Z",
     "start_time": "2019-12-01T16:44:56.829074Z"
    }
   },
   "outputs": [],
   "source": [
    "class AIVM_helper:\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"Initialised AIVM_helper class and methods\")\n",
    "\n",
    "    def image_to_byte_array(self, image):\n",
    "        imgByteArr = io.BytesIO()\n",
    "        image.save(imgByteArr, format=image.format)\n",
    "        imgByteArr = imgByteArr.getvalue()\n",
    "        return imgByteArr\n",
    "\n",
    "    def time_stamp(self):\n",
    "        now = datetime.now()\n",
    "        timestamp = datetime.timestamp(now)\n",
    "        dt_object = datetime.fromtimestamp(timestamp)\n",
    "        words = str(dt_object).split(' ')\n",
    "        return words[0], words[1]\n",
    "\n",
    "    def markdown_to_text(self, markdown_string):\n",
    "        \"\"\" Converts a markdown string to plaintext \"\"\"\n",
    "\n",
    "        # md -> html -> text since BeautifulSoup can extract text cleanly\n",
    "        html = markdown(markdown_string)\n",
    "\n",
    "        # remove code snippets\n",
    "        html = re.sub(r'<pre>(.*?)</pre>', ' ', html)\n",
    "        html = re.sub(r'<code>(.*?)</code >', ' ', html)\n",
    "\n",
    "        # extract text\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        text = ''.join(soup.findAll(text=True))\n",
    "        return text\n",
    "\n",
    "    def remove_nums_from_str(self, s):\n",
    "        result = ''.join([i for i in s if not i.isdigit()])\n",
    "        return result\n",
    "\n",
    "    def pre_process_sentence(self, sent):\n",
    "        sent = re.sub('-', ' ', sent, flags=re.MULTILINE)  # Added by Aideen\n",
    "        sent = re.sub(' +', ' ', sent, flags=re.MULTILINE)  # Added by Aideen\n",
    "        \n",
    "        sent = sent.replace(\";\", \", \")\n",
    "        sent = re.sub(' +', ' ', sent, flags=re.MULTILINE)  # Added by Aideen\n",
    "        sent = self.remove_nums_from_str(sent.replace(\",\", \" \"))\n",
    "        sent = sent.replace(\"  \", \" \")\n",
    "        sent = sent.replace(\"#\", \"\")# Added by Aideen\n",
    "        sent = sent.replace('\"', \"\")\n",
    "        sent = sent.replace(\"*\", \"\")# Added by Aideen\n",
    "        sent = sent.replace(\" .\", \".\")\n",
    "        sent = sent.replace(\"[\", \"\")# Added by Aideen\n",
    "        sent = sent.replace(\"]\", \"\")# Added by Aideen\n",
    "        sent = sent.replace(\"+\", \" \")# Added by Aideen\n",
    "        #remove html markup\n",
    "        sent = re.sub(\"(<.*?>)\",\"\",sent)\n",
    "        sent = html.unescape(sent)# Added by Aideen remove the escape chars\n",
    "        sent = sent.replace('\\\\xe\\\\x\\\\x', \"'\")## Added by Aideen\\xe\\x\\x\n",
    "        sent = sent.replace(\"\\\\\", \"\")# Added by Aideen\n",
    "        sent = sent.lstrip()\n",
    "        sent = sent.rstrip()\n",
    "        sent = sent.strip()\n",
    "        return sent\n",
    "\n",
    "    def pre_process(self, text):\n",
    "\n",
    "        # Removing prefixed 'b'\n",
    "        document = re.sub(r'^b\\s+', '', str(text))\n",
    "\n",
    "        # Removing splicit line change\n",
    "        document = re.sub(r'\\\\n', '', document, flags=re.MULTILINE)\n",
    "\n",
    "        soup = BeautifulSoup(document)\n",
    "\n",
    "        # Remove HTML code from text\n",
    "        document = soup.get_text()\n",
    "\n",
    "        # Parse text from markdown code\n",
    "        document = markdown_to_text(document)\n",
    "\n",
    "        # Removing URLS\n",
    "        document = re.sub(\n",
    "            r'^https?:\\/\\/.*[\\r\\n]*', '', document, flags=re.MULTILINE)\n",
    "\n",
    "        # Removing strings such as \\\\xe5 \\\\xe6 \\\\xe7 that appear a lot in the descriptions\n",
    "        document = re.sub(r':?\\\\+x\\w{2}', ' ', document, flags=re.MULTILINE)\n",
    "\n",
    "        # Remove all the special characters except spaces, dashes, commas and dots\n",
    "        document = re.sub(r\"[^\\s.,\\-a-zA-Z0-9]\", ' ', str(document))\n",
    "\n",
    "        # Substituting multiple spaces with single space\n",
    "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "        # Substituting multiple '-' with single '-'\n",
    "        document = re.sub(r'\\-{2,50}', '', document, flags=re.I)\n",
    "\n",
    "        document = re.sub('-', ' ', document,\n",
    "                          flags=re.MULTILINE)  # Added by Aideen\n",
    "        document = re.sub(' +', ' ', document,\n",
    "                          flags=re.MULTILINE)  # Added by Aideen\n",
    "        document.replace(\";\", \", \")\n",
    "        document = re.sub(' +', ' ', document,\n",
    "                          flags=re.MULTILINE)  # Added by Aideen\n",
    "        # Converting to Lowercase\n",
    "        document = document.lower()\n",
    "\n",
    "        # Sentences Tokenization\n",
    "        return sent_tokenize(document)\n",
    "\n",
    "    def remove_excluded_files(self, file_list):\n",
    "        cleaned_file_list = []\n",
    "        for f in file_list:\n",
    "            if not f.startswith('.') and not \"random\" in f and \"gathered_\" in f and not f.startswith(\"_\"):\n",
    "                cleaned_file_list.append(f)\n",
    "        return cleaned_file_list\n",
    "\n",
    "    def keep_pdf_urls_only(self, file_list):\n",
    "        cleaned_file_list = []\n",
    "        for f in file_list:\n",
    "            if f.endswith('.pdf') and not \" \" in f:\n",
    "                cleaned_file_list.append(f)\n",
    "        return cleaned_file_list\n",
    "\n",
    "    def remove_excluded_files_except(self, file_list, except_with_text):\n",
    "        cleaned_file_list = []\n",
    "        for f in file_list:\n",
    "            if not f.startswith('.') and not \"random\" in f and except_with_text in f and not f.startswith(\"_\"):\n",
    "                cleaned_file_list.append(f)\n",
    "        return cleaned_file_list\n",
    "\n",
    "    def rreplace(self, s, old, new, occurrence):\n",
    "        li = s.rsplit(old, occurrence)\n",
    "        return new.join(li)\n",
    "\n",
    "\n",
    "    def clean_file_name(self, name, replacements2=[]):\n",
    "\n",
    "        replacements = [\".txt\", \".csv\", \".tsv\"]\n",
    "\n",
    "        for r in replacements:\n",
    "            name = name.replace(r, \"\")\n",
    "\n",
    "        for r in replacements2:\n",
    "            name = name.replace(r, \"\")\n",
    "        return name\n",
    "    \n",
    "\n",
    "    def save_img_to_github(self, git_user, git_password, git_repo, my_file_list, push_to_git_as, commit_message):\n",
    "        '''\n",
    "        in order to push a file to github it must first be stored locally, then pushed\n",
    "        this local location can also be local to a virtual machine. \n",
    "        takes: \n",
    "                git username, password, repo, \n",
    "                a list of files to push to git ie the full local location of file,\n",
    "                a matching list of paths to push each file to in Git hub \n",
    "        '''\n",
    "        user = git_user\n",
    "        password = git_password\n",
    "        url = git_repo\n",
    "        file_list = []  # push these list of files to git\n",
    "        file_names = []  # push to this location in git\n",
    "        message = 'ok'\n",
    "        element_list = []\n",
    "        try:\n",
    "            g = Github(user, password)\n",
    "            try:\n",
    "                repo = g.get_repo(url)\n",
    "            except (IOError, OSError, GithubException) as e:\n",
    "                return \"error\", e.message\n",
    "\n",
    "            \n",
    "            file_list = my_file_list\n",
    "            file_names = push_to_git_as\n",
    "\n",
    "            commit_message = commit_message\n",
    "            master_ref = repo.get_git_ref('heads/master')\n",
    "            master_sha = master_ref.object.sha\n",
    "            base_tree = repo.get_git_tree(master_sha)\n",
    "            \n",
    "\n",
    "            \n",
    "            for i, entry in enumerate(file_list):     \n",
    "                data = open(entry, \"rb\").read()\n",
    "                data = re.sub(b'/^(.+,)/', '' , data)\n",
    "                data = base64.b64encode(data)\n",
    "                blob = repo.create_git_blob(data.decode(\"utf-8\"), \"base64\")\n",
    "                element = InputGitTreeElement(\n",
    "                    path=file_names[i], mode='100644', type='blob', sha=blob.sha)\n",
    "                \n",
    "                # element_list is a list of InputGitTreeElement.\n",
    "                # Each one corresponds to a file.\n",
    "                element_list.append(element)\n",
    "            tree = repo.create_git_tree(element_list, base_tree)\n",
    "            parent = repo.get_git_commit(master_sha)\n",
    "            commit = repo.create_git_commit(commit_message, tree, [parent])\n",
    "            print (\"commit\", commit)\n",
    "            master_ref.edit(commit.sha)\n",
    "            return commit, message\n",
    "        except (IOError, OSError, GithubException) as e:\n",
    "            message = \"GitHub save FAILED:\" + '\\n' + \"Are your github login credentials correct?\" + \\\n",
    "                '\\n' + \"Are you a collaberator in the repo?\" + '\\n'\n",
    "            \n",
    "            return \"error\",  message + \"Specific Error: \" + str(e)\n",
    "        \n",
    "        \n",
    "    def save_to_github(self, git_user, git_password, git_repo, my_file_list, push_to_git_as, commit_message):\n",
    "        '''\n",
    "        in order to push a file to github it must first be stored locally, then pushed\n",
    "        this local location can also be local to a virtual machine. \n",
    "        takes: \n",
    "            git username, password, repo, \n",
    "            a list of files to push to git ie the full local location of file,\n",
    "            a matching list of paths to push each file to in Git hub \n",
    "        '''\n",
    "        user = git_user\n",
    "        password = git_password\n",
    "        url = git_repo\n",
    "        file_list = []  # push these list of files to git\n",
    "        file_names = []  # push to this location in git\n",
    "        message = 'ok'\n",
    "        element_list = []\n",
    "        try:\n",
    "            g = Github(user, password)\n",
    "            try:\n",
    "                repo = g.get_repo(url)\n",
    "            except (IOError, OSError, GithubException) as e:\n",
    "                return \"error\", e.message\n",
    "\n",
    "            \n",
    "            file_list = my_file_list\n",
    "            file_names = push_to_git_as\n",
    "\n",
    "            commit_message = commit_message\n",
    "            master_ref = repo.get_git_ref('heads/master')\n",
    "            master_sha = master_ref.object.sha\n",
    "            base_tree = repo.get_git_tree(master_sha)\n",
    "            \n",
    "            for i, entry in enumerate(file_list):    \n",
    "                data = base64.b64encode(open(entry, \"rb\").read())\n",
    "                if entry.endswith('.png'):\n",
    "                    data = base64.b64encode(data)\n",
    "                blob = repo.create_git_blob(data.decode(\"utf-8\"), \"base64\")\n",
    "\n",
    "                element = InputGitTreeElement(\n",
    "                    path=file_names[i], mode='100644', type='blob', sha=blob.sha)\n",
    "                \n",
    "                # element_list is a list of InputGitTreeElement.\n",
    "                # Each one corresponds to a file.\n",
    "                element_list.append(element)\n",
    "            tree = repo.create_git_tree(element_list, base_tree)\n",
    "            parent = repo.get_git_commit(master_sha)\n",
    "            commit = repo.create_git_commit(commit_message, tree, [parent])\n",
    "            print (\"commit\", commit)\n",
    "            master_ref.edit(commit.sha)\n",
    "            return commit, message\n",
    "        except (IOError, OSError, GithubException) as e:\n",
    "            message = \"GitHub save FAILED:\" + '\\n' + \"Are your github login credentials correct?\" + \\\n",
    "                '\\n' + \"Are you a collaberator in the repo?\" + '\\n'\n",
    "            \n",
    "            return \"error\",  message + \"Specific Error: \" + str(e)\n",
    "\n",
    "\n",
    "\n",
    "    def concat_files_from_github_dir(self, directory_base_url, file_list):\n",
    "        df_list = []\n",
    "        for file_name, i in zip(file_list, range(len(file_list))):\n",
    "            urlBase = directory_base_url\n",
    "            df_list.append(pd.read_csv(urlBase+file_name,\n",
    "                                       sep='\\t', error_bad_lines=False))\n",
    "        # NOTE: \"truncated\": false  we should check for truncated = true to do follow on call to get all files\n",
    "        df = pd.concat(df_list)\n",
    "        return df\n",
    "\n",
    "    def read_single_file_from_github_dir(self, directory_base_url, file_name):\n",
    "        df_list = []\n",
    "        for file_name, i in zip(file_list, range(len(file_list))):\n",
    "            urlBase = directory_base_url\n",
    "            df_list.append(pd.read_csv(urlBase+file_name,\n",
    "                                       sep='\\t', error_bad_lines=False))\n",
    "        # NOTE: \"truncated\": false  we should check for truncated = true to do follow on call to get all files\n",
    "        df = pd.concat(df_list)\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    \n",
    "    def list_files_from_github_dir(self, owner, repo, path):\n",
    "    \n",
    "        # read data files for source data directly from github.\n",
    "        # to obtain the id for the folder, navigate the tree using\n",
    "        # https://api.github.com/repos/{owner}/{repo}/git/trees/master\n",
    "        # e.g https://api.github.com/repos/aideenf/AIVC/git/trees/master\n",
    "        # once navigated each directory will be of format\n",
    "        # https://api.github.com/repos/aideenf/AIVC/git/trees/{dir_ref}\n",
    "        # exampe dir_ref = 048349b4dd81d95a17129e7fcd5418bdca8309b3\"\n",
    "        \n",
    "        \n",
    "        #Note A commit, or \"revision\", is an individual change to a file (or set of files). \n",
    "        #It's like when you save a file, except with Git, \n",
    "        #every time you save it creates a unique ID (a.k.a. the \"SHA\" or \"hash\") \n",
    "        #that allows you to keep record of what changes were made when and by who.\n",
    "        # in order to list the docs in the latest version of any folder the most up to date\n",
    "        #Sha should be used. \n",
    "        \n",
    "        #in order to get latest must always start at the master\n",
    "        #https://api.github.com/repos/aideenf/AIVC/git/trees/master\n",
    "        \n",
    "        # import requests as req  #we need to ensure we do not get cached response from browser.\n",
    "        \n",
    "        #get the dir for each step in the path\n",
    "        dir_list = path.split('/')\n",
    "        gathered_files = []\n",
    "        headers = {\n",
    "            'Cache-Control': 'no-cache',\n",
    "            'Pragma': 'no-cache',\n",
    "            'If-None-Match': ''\n",
    "            }\n",
    "        \n",
    "        MASTER = 'https://api.github.com/repos/' + owner + '/' + repo + '/git/trees/master'\n",
    "        NEXT = 'https://api.github.com/repos/' + owner + '/' + repo + '/git/trees/'\n",
    "        resp = req.get(MASTER,  headers=headers)\n",
    "        response = json.loads(resp.text)\n",
    "        try:\n",
    "            for value in response['tree']:\n",
    "                if value['path'] == dir_list[0]:\n",
    "                    NEXT_DIR = NEXT + value['sha']\n",
    "            \n",
    "            for d, i in zip(dir_list, range(len(dir_list)-1)):                          \n",
    "                resp = req.get(NEXT_DIR,  headers=headers)\n",
    "                response = json.loads(resp.text)\n",
    "                for value in response['tree']:\n",
    "                    if value['path'] == dir_list[i+1]:\n",
    "                        NEXT_DIR = NEXT + value['sha']\n",
    "                   \n",
    "        \n",
    "            resp = req.get(NEXT_DIR,  headers=headers)\n",
    "            response = json.loads(resp.text)\n",
    "            for value in response['tree']:\n",
    "                gathered_files.append(value['path'])\n",
    "\n",
    "            return gathered_files\n",
    "        except: \n",
    "            print (response['message'])\n",
    "            return []\n",
    "        \n",
    "        \n",
    "    def save_plot(self, plot, save_as, local, git, to_git, git_user, git_pswd, git_owner_repo):\n",
    "        try:\n",
    "            fig = plot.get_figure()\n",
    "        except:\n",
    "            fig = plot\n",
    "        fig.savefig(local + save_as)  \n",
    "    \n",
    "        if to_git == True:\n",
    "            my_file_list = [local + save_as]\n",
    "            push_to_git_as = [git + save_as]\n",
    "    \n",
    "            commit, message = self.save_img_to_github(git_user, git_pswd, git_owner_repo, my_file_list, push_to_git_as, \"auto push classified conventions\")\n",
    "\n",
    "            if (commit != \"error\"):\n",
    "                print (\"File to commit:\", my_file_list)\n",
    "                print (\"Push to git as:\", push_to_git_as)\n",
    "                print (\"Commit: \", commit)\n",
    "                display(HTML(\"<font color='green'><b>Files Merged and Saved!!</b></font>\"))\n",
    "        \n",
    "            \n",
    "            if (commit == \"error\"):\n",
    "                print (\"File to commit: \", my_file_list)\n",
    "                print (\"Push to git as: \", push_to_git_as)\n",
    "                display(HTML(\"<font color='red'><b>Warning!!</b></font>\"))\n",
    "                print(message)\n",
    "        \n",
    "        return fig\n",
    "        gc.collect()\n",
    "        \n",
    "        \n",
    "    def save_to_shelf(self, file_name, key, my_object, use_git, usr, pswd ):  \n",
    "\n",
    "        filename = './'+file_name    \n",
    "        f = shelve.open(filename, flag = 'n')\n",
    "        f[key] = my_object\n",
    "        paths = project_paths()\n",
    "        f.close()\n",
    "        if use_git == True:\n",
    "            my_file_list = ['./'+file_name]\n",
    "            push_to_git_as = [paths.RESOURCES_DIR_GIT + file_name ]\n",
    "    \n",
    "            commit, message = self.save_to_github(usr, pswd, paths.GIT_OWNER_REPO, my_file_list, push_to_git_as, \"auto save keywords\")\n",
    "\n",
    "            if (commit != \"error\"):\n",
    "                print (\"File to commit:\", my_file_list)\n",
    "                print (\"Push to git as:\", push_to_git_as)\n",
    "                print (\"Commit: \", commit)\n",
    "                display(HTML(\"<font color='green'><b>Shelf saved to GIT!!</b></font>\"))\n",
    "        \n",
    "            \n",
    "            if (commit == \"error\"):\n",
    "                print (\"File to commit: \", my_file_list)\n",
    "                print (\"Push to git as: \", push_to_git_as)\n",
    "                display(HTML(\"<font color='red'><b>Warning!!</b></font>\"))\n",
    "                print(message)\n",
    "\n",
    "        \n",
    "    \n",
    "    def retrieve_shelved_object(self, file_name, key, use_git):\n",
    "        filename = './'+file_name\n",
    "        url = 'https://github.com/aideenf/AIVC/blob/master/cp_wssc/Data/Iterative-models-building/Resources/my_shelf?raw=true'\n",
    "        if use_git == True:\n",
    "            urllib.request.urlretrieve(url, filename)\n",
    "        f = shelve.open(filename,flag = 'r' )\n",
    "        obj = f.get(key)\n",
    "        f.close()\n",
    "        return obj\n",
    "\n",
    "    def get_all_shelfed(self, file_name, use_git):\n",
    "        filename = './'+file_name\n",
    "        url = 'https://github.com/aideenf/AIVC/blob/master/cp_wssc/Data/Iterative-models-building/Resources/my_shelf?raw=true'\n",
    "        if use_git == True:\n",
    "            urllib.request.urlretrieve(url, filename)\n",
    "        f = shelve.open(filename,flag = 'r' )\n",
    "        f = shelve.open(filename, flag = 'r')\n",
    "        my_list = f.keys()\n",
    "        print (list(my_list))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calss to calculate AUC for validation when training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-13T20:06:42.215424Z",
     "start_time": "2019-10-13T20:06:42.199245Z"
    }
   },
   "outputs": [],
   "source": [
    "## Calculate AUC for validation when training the model\n",
    "## As explained here: https://stackoverflow.com/questions/41032551/how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras\n",
    "class roc_callback(Callback):\n",
    "    def __init__(self,validation_data):#training_data,validation_data):\n",
    "        #self.x = training_data[0]\n",
    "        #self.y = training_data[1]\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "       \n",
    "        y_pred_val = self.model.predict(self.x_val)\n",
    "        roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
    "\n",
    "        print(\"Roc-AUC on validation: {}\".format(str(round(roc_val,4))))\n",
    "        return\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Helper methods in class model_helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T21:10:29.062362Z",
     "start_time": "2019-12-01T21:10:28.706897Z"
    }
   },
   "outputs": [],
   "source": [
    "class model_helpers:\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        #self.something = kwargs['something']\n",
    "\n",
    "        self.keyword_from_social_scientists_dict = {\n",
    "            'industrial':\n",
    "            ['efficiency', 'efficiant', 'performance', 'performances', 'productivity', 'productive', 'output', 'outputs', 'control', 'power', 'functionality', 'functional', 'organisation', 'organisations', 'professional', 'professionals', 'profession', 'professions', 'reliability', 'reliable', 'foreseeability', 'qualification', 'qualifications', 'expert', 'experts',\n",
    "             'expertise', 'progress', 'standard', 'standards', 'standardized', 'norms', 'norm', 'growth', 'quantification', 'quantifiy', 'measurement', 'measure', 'measures', 'engineering', 'engineer', 'forecast', 'linearity', 'extrapolation', 'extrapolate', 'scientific', 'plans', 'plan', 'audit', 'audits', 'benchmark', 'benchmarks', 'benchmarking', 'test', 'tests'],\n",
    "            'project':\n",
    "            ['flexibility', 'project', 'projects', 'networking', 'networker', 'networkers', 'activity', 'intermediary', 'mediation', 'switch', 'switching', 'shift', 'shifting', 'mobility', 'dynamic',\n",
    "             'transition', 'transitions', 'temporality', 'fluidity', 'communication', 'communications', 'communicate', 'initiative', 'initiatives', 'bridging', 'adapt', 'adaption', 'adaptions'],\n",
    "            'market':\n",
    "            ['wealth', 'money', 'goods', 'rival', 'selfishness', 'winner', 'winners', 'price', 'customer', 'customers', 'customized', 'seller', 'sellers', 'sell', 'vendor', 'vendors', 'buyer', 'buyers', 'buy', 'purchaser', 'purchasers', 'purchase', 'opportunity', 'opportunities', 'competition', 'compete',\n",
    "             'sale', 'sales', 'business', 'interest', 'transaction', 'transactions', 'availability', 'owner', 'owners', 'bargain', 'contract', 'contracts', 'pay', 'deal', 'trade', 'scarcity', 'trading', 'entrepreneur', 'entrepreneurs', 'entrepreneurial', 'market', 'markets', 'marketing', 'marketplace'],\n",
    "            'inspired':\n",
    "            ['inspiration', 'illuminated', 'unexpected', 'feeling', 'fascination', 'fascinating', 'thrilling', 'genuineness', 'intuition', 'genius', 'brilliant', 'uniqueness', 'non-conformity',\n",
    "             'non-conform', 'passion', 'passionate', 'creative', 'creativity', 'imagination', 'visionary', 'extraordinary', 'emotional', 'conviction', 'holy', 'gifted', 'blessed'],\n",
    "            'civic':\n",
    "            ['collective', 'collectives', 'common good', 'community', 'communities', 'representative', 'representatives', 'general interest', 'unite', 'unity', 'union', 'unions', 'majority', 'civil right',\n",
    "             'civil rights', 'assembly', 'democracy', 'vote', 'votes', 'voting', 'election', 'elections', 'elect', 'equality', 'fair', 'fairness', 'law', 'laws', 'justice', 'unbiased', 'impartial'],\n",
    "            'domestic':\n",
    "            ['superior', 'superiority', 'continuity', 'continuation', 'dependence', 'dependency', 'familiarity', 'tradition', 'traditions', 'origins', 'origin',\n",
    "             'routine', 'routines', 'habit', 'habits', 'etiquette', 'common sense', 'respect', 'duty', 'duties', 'trust', 'craft', 'crafts', 'craftsman', 'crafted'],\n",
    "            'green':\n",
    "            ['nature', 'ecological', 'ecology', 'renewable', 'recyclable', 'recycle', 'sustainable', 'sustainability',\n",
    "             'preserve', 'preservation', 'holism', 'holistic', 'green', 'emission', 'organic', 'organical'],\n",
    "            # 'Vitality'\n",
    "            'renown':\n",
    "            ['acknowledgement', 'praise', 'public relation', 'public relations', 'public opinion', 'brands', 'brand', 'audience',\n",
    "             'follower', 'followers', 'supporter', 'supporters', 'fame', 'influence', 'influences', 'glory', 'attractive', 'appealing']\n",
    "        }\n",
    "        # Classificaiton NETWORK Configuration parameters\n",
    "        self.MAX_SEQUENCE_LENGTH = kwargs['seq_len']\n",
    "        self.MAX_NB_WORDS = kwargs['max_num_wrds']\n",
    "        self.EMBEDDING_DIM = kwargs['embedding_dim']  # 100, 200 or 300\n",
    "        self.VALIDATION_SPLIT = kwargs['validation_splt']\n",
    "        self.TFIDF_MAX_FEATURES = kwargs['tfidf_max']\n",
    "        self.NUM_EPOCHS = kwargs['num_epochs']\n",
    "        print(\"Initialised model_helpers class and methods\")\n",
    "\n",
    "        # Training a deep learning model given training, validation data\n",
    "\n",
    "    def train_DL_model(self, x_train, y_train, x_val, y_val, tokenizer, embeddings_index):\n",
    "\n",
    "        num_epochs = self.NUM_EPOCHS\n",
    "        word_index = tokenizer.word_index\n",
    "\n",
    "        print('\\nNumber of elements from each class in traing and validation set ')\n",
    "        print(y_train.sum(axis=0))\n",
    "        print(y_val.sum(axis=0))\n",
    "\n",
    "        embedding_matrix = np.random.random(\n",
    "            (len(word_index) + 1, self.EMBEDDING_DIM))\n",
    "        for word, i in word_index.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "\n",
    "        embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                    self.EMBEDDING_DIM,\n",
    "                                    weights=[embedding_matrix],\n",
    "                                    input_length=self.MAX_SEQUENCE_LENGTH,\n",
    "                                    trainable=False)\n",
    "\n",
    "        convs = []\n",
    "        filter_sizes = [3, 4, 5]\n",
    "\n",
    "        sequence_input = Input(\n",
    "            shape=(self.MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "        embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "        for fsz in filter_sizes:\n",
    "            l_conv = Conv1D(nb_filter=128, filter_length=fsz,\n",
    "                            activation='relu')(embedded_sequences)\n",
    "            l_pool = MaxPooling1D(5)(l_conv)\n",
    "            convs.append(l_pool)\n",
    "\n",
    "        l_merge = Concatenate(axis=1)(convs)\n",
    "\n",
    "        l_cov1 = Conv1D(128, 5, activation='relu')(l_merge)\n",
    "        l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "\n",
    "        l_flat = Flatten()(l_pool1)\n",
    "        l_dense = Dense(128, activation='relu')(l_flat)\n",
    "        preds = Dense(2, activation='softmax')(l_dense)\n",
    "\n",
    "        model = Model(sequence_input, preds)\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='rmsprop',\n",
    "                      metrics=['acc'])\n",
    "\n",
    "        print(\"model fitting - more complex convolutional neural network\")\n",
    "        model.summary()\n",
    "        train_history = model.fit(x_train, y_train, validation_data=(\n",
    "            x_val, y_val), epochs=num_epochs, batch_size=50, callbacks=[roc_callback(validation_data=(x_val, y_val))])\n",
    "\n",
    "        return model, tokenizer, x_val, y_val, train_history\n",
    "\n",
    "    # Trains a DL model without using validation dataset\n",
    "\n",
    "    def train_DL_model_not_validation(self, x_train, y_train, tokenizer):\n",
    "        num_epochs = self.NUM_EPOCHS\n",
    "        word_index = tokenizer.word_index\n",
    "\n",
    "        print('\\nNumber of elements from each class in traing and validation set ')\n",
    "        print(y_train.sum(axis=0))\n",
    "        print(y_val.sum(axis=0))\n",
    "\n",
    "        embedding_matrix = np.random.random(\n",
    "            (len(word_index) + 1, self.EMBEDDING_DIM))\n",
    "        for word, i in word_index.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "\n",
    "        embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                    self.EMBEDDING_DIM,\n",
    "                                    weights=[embedding_matrix],\n",
    "                                    input_length=self.MAX_SEQUENCE_LENGTH,\n",
    "                                    trainable=False)\n",
    "\n",
    "        convs = []\n",
    "        filter_sizes = [3, 4, 5]\n",
    "\n",
    "        sequence_input = Input(\n",
    "            shape=(self.MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "        embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "        for fsz in filter_sizes:\n",
    "            l_conv = Conv1D(nb_filter=128, filter_length=fsz,\n",
    "                            activation='relu')(embedded_sequences)\n",
    "            l_pool = MaxPooling1D(5)(l_conv)\n",
    "            convs.append(l_pool)\n",
    "\n",
    "        l_merge = Concatenate(axis=1)(convs)\n",
    "\n",
    "        l_cov1 = Conv1D(128, 5, activation='relu')(l_merge)\n",
    "        l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "\n",
    "        l_flat = Flatten()(l_pool1)\n",
    "        l_dense = Dense(128, activation='relu')(l_flat)\n",
    "        preds = Dense(2, activation='softmax')(l_dense)\n",
    "\n",
    "        model = Model(sequence_input, preds)\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='rmsprop',\n",
    "                      metrics=['acc'])\n",
    "\n",
    "        print(\"model fitting - more complex convolutional neural network\")\n",
    "        model.summary()\n",
    "        train_history = model.fit(\n",
    "            x_train, y_train, epochs=num_epochs, batch_size=50)\n",
    "        return model, tokenizer, train_history\n",
    "\n",
    "    # Trains one DL model for each group of sentences (within each convention)\n",
    "\n",
    "    def train_DL_models(self, df_train, embeddings_index,\n",
    "                        data_class_column=\"convention\",\n",
    "                        data_label_column=\"label\",\n",
    "                        df_val=None,\n",
    "                        tokenizer=None,\n",
    "                        random_seed=None,\n",
    "                        use_validation=True):\n",
    "\n",
    "        num_epochs = self.NUM_EPOCHS\n",
    "        _models = {}\n",
    "        _tokenizers = {}\n",
    "        _data_val_x = {}\n",
    "        _data_val_y = {}\n",
    "        _train_histories = {}\n",
    "        _data_val_x_txt = {}\n",
    "        _data_val_x_in = {}\n",
    "\n",
    "        if tokenizer is None:\n",
    "            if df_val is None:\n",
    "                tokenizer = create_tokenizer(\n",
    "                    df_train, column_to_tokenize='text', max_words=self.MAX_NB_WORDS)\n",
    "            else:\n",
    "                tokenizer = create_tokenizer(pd.concat(\n",
    "                    [df_train, df_val]), column_to_tokenize='text', max_words=self.MAX_NB_WORDS)\n",
    "\n",
    "        word_index = tokenizer.word_index\n",
    "        print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "        for convention in df_train[data_class_column].unique():\n",
    "\n",
    "            print(\"----------------------------------------------------------------\")\n",
    "            print(\"            {}                  \".format(convention))\n",
    "            print(\"----------------------------------------------------------------\")\n",
    "\n",
    "            tmp_df_train = df_train[df_train[data_class_column] == convention]\n",
    "            if use_validation:\n",
    "                if df_val is None:\n",
    "\n",
    "                    texts = tmp_df_train['text'].values\n",
    "                    labels = tmp_df_train[data_label_column].values\n",
    "\n",
    "                    sequences = tokenizer.texts_to_sequences(texts)\n",
    "                    data = pad_sequences(\n",
    "                        sequences, maxlen=self.MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "                    labels = to_categorical(np.asarray(labels))\n",
    "                    print('Shape of data tensor:', data.shape)\n",
    "                    print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "                    indices = np.arange(data.shape[0])\n",
    "                    if random_seed is not None:\n",
    "                        np.random.seed(random_seed)\n",
    "                    np.random.shuffle(indices)\n",
    "                    data = data[indices]\n",
    "                    labels = labels[indices]\n",
    "                    texts = texts[indices]\n",
    "                    nb_validation_samples = int(\n",
    "                        self.VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "                    x_train = data[:-nb_validation_samples]\n",
    "                    y_train = labels[:-nb_validation_samples]\n",
    "\n",
    "                    x_val_txts = texts[-nb_validation_samples:]\n",
    "                    x_val_in = data[-nb_validation_samples:]\n",
    "                    y_val_in = labels[-nb_validation_samples:]\n",
    "\n",
    "                    _model, _tokenizer, _x_val, _y_val, _train_h = self.train_DL_model(\n",
    "                        x_train, y_train, x_val_in, y_val_in, tokenizer, embeddings_index)\n",
    "                else:\n",
    "\n",
    "                    tmp_df_val = df_val[df_val[data_class_column]\n",
    "                                        == convention]\n",
    "\n",
    "                    train_texts = tmp_df_train['text'].values\n",
    "                    train_labels = tmp_df_train[data_label_column].values\n",
    "\n",
    "                    val_texts = tmp_df_val['text'].values\n",
    "                    val_labels = tmp_df_val[data_label_column].values\n",
    "                    x_val_txts = val_texts\n",
    "\n",
    "                    train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "                    val_sequences = tokenizer.texts_to_sequences(val_texts)\n",
    "\n",
    "                    x_train = pad_sequences(\n",
    "                        train_sequences, maxlen=self.MAX_SEQUENCE_LENGTH)\n",
    "                    x_val_in = pad_sequences(\n",
    "                        val_sequences, maxlen=self.MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "                    y_train = to_categorical(np.asarray(train_labels))\n",
    "                    y_val_in = to_categorical(np.asarray(val_labels))\n",
    "\n",
    "                    _model, _tokenizer, _x_val, _y_val, _train_h = self.train_DL_model(\n",
    "                        x_train, y_train, x_val_in, y_val_in, tokenizer)\n",
    "\n",
    "                _models[convention] = _model\n",
    "                _tokenizers[convention] = _tokenizer\n",
    "                _data_val_x[convention] = _x_val\n",
    "                _data_val_y[convention] = _y_val\n",
    "                _train_histories[convention] = _train_h\n",
    "                _data_val_x_txt[convention] = x_val_txts\n",
    "            else:\n",
    "                texts = tmp_df_train['text'].values\n",
    "                labels = tmp_df_train[data_label_column].values\n",
    "\n",
    "                sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "                data = pad_sequences(\n",
    "                    sequences, maxlen=self.MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "                labels = to_categorical(np.asarray(labels))\n",
    "                print('Shape of data tensor:', data.shape)\n",
    "                print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "                x_train = data\n",
    "                y_train = labels\n",
    "\n",
    "                _model, _tokenizer, _train_h = self.train_DL_model_not_validation(\n",
    "                    x_train, y_train, tokenizer)\n",
    "\n",
    "                _models[convention] = _model\n",
    "                _tokenizers[convention] = _tokenizer\n",
    "                _train_histories[convention] = _train_h\n",
    "\n",
    "            print(\"\\n\\n\\n\")\n",
    "\n",
    "        return (_models, _tokenizers, _data_val_x, _data_val_x_txt, _data_val_y, _train_histories)\n",
    "\n",
    "    \n",
    "    \n",
    "    def store_DL_models_in_picke(self, pickle_f_name, _model, _tokenizers, _val_x, _val_y, _train_histories, path, use_git, usr, pswd):\n",
    "        _convnet_items = {}\n",
    "\n",
    "        if _model != None:\n",
    "            _convnet_items['model'] = _model\n",
    "        \n",
    "        if _tokenizers != None:\n",
    "            _convnet_items['tokenizer'] = _tokenizers\n",
    "        if _val_x != None:\n",
    "            _convnet_items['_x_val'] = _val_x\n",
    "        if _val_y != None:\n",
    "            _convnet_items['_y_val'] = _val_y\n",
    "        if _train_histories != None:    \n",
    "            _convnet_items['train_history'] = _train_histories\n",
    "            \n",
    "        \n",
    "       \n",
    "        with open(os.path.join(path, pickle_f_name), 'wb') as f:\n",
    "            # Pickle the 'data' dictionary using the highest protocol available.\n",
    "            pickle.dump(_convnet_items, f, pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "        if use_git == True:\n",
    "            print(\"Save to Git\")\n",
    "            paths = project_paths()\n",
    "            aivm_helper = AIVM_helper()\n",
    "            my_file_list = [path + pickle_f_name]\n",
    "            push_to_git_as = [paths.RESOURCES_DIR_GIT + pickle_f_name ]\n",
    "    \n",
    "            commit, message = aivm_helper.save_to_github(usr, pswd, paths.GIT_OWNER_REPO, my_file_list, push_to_git_as, \"auto save keywords\")\n",
    "\n",
    "            if (commit != \"error\"):\n",
    "                print (\"File to commit:\", my_file_list)\n",
    "                print (\"Push to git as:\", push_to_git_as)\n",
    "                print (\"Commit: \", commit)\n",
    "                display(HTML(\"<font color='green'><b>Pickle saved to GIT in Resources dir!!</b></font>\"))\n",
    "        \n",
    "            \n",
    "            if (commit == \"error\"):\n",
    "                print (\"File to commit: \", my_file_list)\n",
    "                print (\"Push to git as: \", push_to_git_as)\n",
    "                display(HTML(\"<font color='red'><b>Warning!!</b></font>\"))\n",
    "                print(message)\n",
    "\n",
    "\n",
    "    \n",
    "    def read_thresholds_from_pickle(self, use_git ):\n",
    "        _convnet_items = {}\n",
    "        paths = project_paths()\n",
    "        pickle_f_name = 'global_final_thresholds.pickle'\n",
    "\n",
    "    \n",
    "        if use_git == False:\n",
    "            f = open(paths.RESOURCES_DIR_LOCAL+pickle_f_name, 'rb')\n",
    "            _convnet_items['model'] = pickle.load(f)\n",
    "            f.close()\n",
    "\n",
    "        if use_git == True:\n",
    "            print(\"Reading from GIT:\", pickle_f_name+'?raw=true')\n",
    "            _convnet_items = pickle.load(urllib.request.urlopen(paths.RESOURCES_URL_GIT+pickle_f_name+'?raw=true'))\n",
    "        \n",
    "        return _convnet_items['model']\n",
    "    \n",
    "    \n",
    "    def read_tokenizer_from_pickle(self, use_git ):\n",
    "        tokenizer = ''\n",
    "        paths = project_paths()\n",
    "        pickle_f_name = 'extended_tokenizer.pickle'\n",
    "\n",
    "    \n",
    "        if use_git == False:\n",
    "            f = open(paths.RESOURCES_DIR_LOCAL+pickle_f_name, 'rb')\n",
    "            tokenizer = pickle.load(f)\n",
    "            f.close()\n",
    "\n",
    "        if use_git == True:\n",
    "            print(\"Reading from GIT:\", pickle_f_name+'?raw=true')\n",
    "            tokenizer = pickle.load(urllib.request.urlopen(paths.RESOURCES_URL_GIT+pickle_f_name+'?raw=true'))\n",
    "        \n",
    "        return tokenizer\n",
    "    \n",
    "    \n",
    "    \n",
    "    def read_models_from_pickle(self, use_git, models ):\n",
    "        _convnet_items = {}\n",
    "        paths = project_paths()\n",
    "        for conv in models:\n",
    "            \n",
    "            pickle_f_name = conv +'_models.pickle'\n",
    "            if use_git == False:\n",
    "                print(\"Reading \",conv)\n",
    "                f = open(paths.RESOURCES_DIR_LOCAL+pickle_f_name, 'rb')\n",
    "                _convnet_items[conv] = pickle.load(f)\n",
    "                f.close()\n",
    "\n",
    "            if use_git == True:\n",
    "                print(\"Reading from GIT:\", pickle_f_name+'?raw=true')\n",
    "                _convnet_items[conv] = pickle.load(urllib.request.urlopen(paths.RESOURCES_URL_GIT+pickle_f_name+'?raw=true'))\n",
    "        \n",
    "        return _convnet_items\n",
    "    \n",
    "    \n",
    "    def read_calibration_models_from_pickle(self, use_git, models ):\n",
    "        _convnet_items = {}\n",
    "        paths = project_paths()\n",
    "        for conv in models:\n",
    "            \n",
    "            pickle_f_name = conv +'calibration_model.pickle'\n",
    "            if use_git == False:\n",
    "                print(\"Reading \",conv)\n",
    "                f = open(paths.RESOURCES_DIR_LOCAL+pickle_f_name, 'rb')\n",
    "                _convnet_items[conv] = pickle.load(f)\n",
    "                f.close()\n",
    "\n",
    "            if use_git == True:\n",
    "                print(\"Reading from GIT:\", pickle_f_name+'?raw=true')\n",
    "                _convnet_items[conv] = pickle.load(urllib.request.urlopen(paths.RESOURCES_URL_GIT+pickle_f_name+'?raw=true'))\n",
    "        \n",
    "        return _convnet_items\n",
    "    \n",
    "    \n",
    "    \n",
    "    def read_DL_models_from_pickle(self, use_git, pickle_f_name, path):\n",
    "\n",
    "        if use_git == False:\n",
    "            f = open(os.path.join(path, pickle_f_name), 'rb')\n",
    "            _convnet_items = pickle.load(f)\n",
    "            f.close()\n",
    "\n",
    "        if use_git == True:\n",
    "            print(\"Reading from GIT:\", pickle_f_name)\n",
    "            _convnet_items = pickle.load(urllib.request.urlopen(pickle_f_name))\n",
    "\n",
    "        # Load the models from the downloaded pickle file\n",
    "        _models = _convnet_items['model']\n",
    "        _tokenizers = _convnet_items['tokenizer']\n",
    "        \n",
    "        try:\n",
    "            _val_x = _convnet_items['_x_val']\n",
    "        except:\n",
    "            print (\"_val_x never pickled\")\n",
    "            _val_x = None\n",
    "            \n",
    "        try:\n",
    "            _val_y = _convnet_items['_y_val']\n",
    "        except:\n",
    "            print (\"_val_y never pickled\")\n",
    "            _val_y = None\n",
    "            \n",
    "        try:   \n",
    "            _train_histories = _convnet_items['train_history']\n",
    "        except:\n",
    "            print (\"_train_histories never pickled\")\n",
    "            _train_histories = None    \n",
    "        \n",
    "\n",
    "        return _models, _tokenizers, _val_x, _val_y, _train_histories\n",
    "\n",
    "        # Trains a TF-IDF, Naive-Bayes based classifier\n",
    "\n",
    "    def train_new_text_pipelineNB(self, texts, labels1):\n",
    "        text_clf = Pipeline([\n",
    "            ('vect', CountVectorizer(max_df=0.85, stop_words=stopwords,\n",
    "                                     max_features=self.TFIDF_MAX_FEATURES)),\n",
    "            ('tfidf', TfidfTransformer(smooth_idf=True, use_idf=True)),\n",
    "            ('clf', MultinomialNB()),\n",
    "        ])\n",
    "\n",
    "        text_clf.fit(texts, labels1)\n",
    "\n",
    "        return text_clf\n",
    "\n",
    "    def train_ML_models(self, df,\n",
    "                        data_class_column=\"convention\",\n",
    "                        data_label_column=\"label\"):\n",
    "        _models = {}\n",
    "\n",
    "        for c in df[data_class_column].unique():\n",
    "            sentences = df[df[data_class_column] == c]['text'].values\n",
    "            labels = df[df[data_class_column] == c][data_label_column].values\n",
    "\n",
    "            m = train_new_text_pipelineNB(sentences, labels)\n",
    "\n",
    "            _models[c] = m\n",
    "        return _models\n",
    "\n",
    "    # Helpers for calculating ML model probability score of class 1 for a set of sentences\n",
    "\n",
    "    def calculate_matches_DL(self, sentences, _models, _tokenizers):\n",
    "        _models_matches = {}\n",
    "\n",
    "        # Getting classification confidence per model for each repo\n",
    "        for model_key in _models.keys():\n",
    "\n",
    "            tokenized_sentences = _tokenizers[model_key].texts_to_sequences(\n",
    "                sentences)\n",
    "\n",
    "            tokenized_sentences = pad_sequences(\n",
    "                tokenized_sentences, maxlen=self.MAX_SEQUENCE_LENGTH)\n",
    "            preds = _models[model_key].predict(tokenized_sentences)\n",
    "\n",
    "            _models_matches[model_key] = preds[:, 1]\n",
    "\n",
    "        return _models_matches\n",
    "\n",
    "    def calculate_matches_ML(self, _sentences, _models):\n",
    "\n",
    "        _models_matches = {}\n",
    "\n",
    "        # Getting classification confidence per model for each repo\n",
    "        for model_key in _models.keys():\n",
    "\n",
    "            preds = _models[model_key].predict_proba(_sentences)[:, 1]\n",
    "            _models_matches[model_key] = preds\n",
    "\n",
    "        return _models_matches\n",
    "\n",
    "    # Calculates probability of sentence combining DL models and ML models predictions\n",
    "\n",
    "    def calculate_matches_mixture(self, _sentences, _modelsML, _modelsDL, _tokenizersDL):\n",
    "        _repos_matches = {}\n",
    "        preds_ML = calculate_matches_ML(_sentences, _modelsML)\n",
    "        preds_DL = calculate_matches_DL(_sentences, _modelsDL, _tokenizersDL)\n",
    "\n",
    "        # Getting classification confidence per model for each repo\n",
    "        for model_key in _modelsML.keys():\n",
    "\n",
    "            preds1 = preds_ML[model_key]\n",
    "            preds2 = preds_DL[model_key]\n",
    "\n",
    "            _repos_matches[model_key] = preds1+preds2\n",
    "        return _repos_matches\n",
    "\n",
    "    ################################################\n",
    "    # The files that have been produced by the audit tool are retrieved and consolodated into\n",
    "    # a data frame, this data frame is then converted to tsv file to produce one TSV file with all audited results.\n",
    "    # This file will be called: 'audited_ALL.tsv'\n",
    "    ##\n",
    "    # A copy will be created with a datestamp should historic data ever be needed: 'audited_ALL_{date}-{time}.tsv'\n",
    "    ##\n",
    "    # Any other process that wishes to use this data should read 'audited_ALL.tsv' which is the latest version.'''\n",
    "    ##############################\n",
    "\n",
    "    def aggregate_audited_data_from_git(self,\n",
    "                                        audited_data_dir_git,\n",
    "                                        github_audit_url,\n",
    "                                        audited_data_dir_local,\n",
    "                                        git_user,\n",
    "                                        git_pswd,\n",
    "                                        git_owner_repo,\n",
    "                                        git_owner,\n",
    "                                        git_repo):\n",
    "\n",
    "        audited_data_dir_git = audited_data_dir_git\n",
    "        github_audit_url = github_audit_url\n",
    "        audited_data_dir_local = audited_data_dir_local\n",
    "        git_user = git_user\n",
    "        git_pswd = git_pswd\n",
    "        git_owner_repo = git_owner_repo\n",
    "        git_owner = git_owner\n",
    "        git_repo = git_repo\n",
    "        aivm_helper = AIVM_helper()\n",
    "\n",
    "        # list all files from 'Audited' folder at cp_wssc/Data/Iterative-models-building/Classification%20results/Conventions/Audited/\n",
    "        # Helper file call to Returns a list of files from the github owner, repo and dir_ref\n",
    "        my_file_list = aivm_helper.list_files_from_github_dir(\n",
    "            git_owner, git_repo, audited_data_dir_git)\n",
    "        # Helper file call to Remove our standard excluded files and all files that are not \"audited_training_data\"\n",
    "        my_file_list = aivm_helper.remove_excluded_files_except(\n",
    "            my_file_list, \"audited_training_data\")\n",
    "        if (len(my_file_list) > 0):\n",
    "            display(my_file_list.sort())\n",
    "            # Returns a dataframe of concatinated files from github, of names from file_list from the github url specified.\n",
    "            audited_df = aivm_helper.concat_files_from_github_dir(\n",
    "                github_audit_url, my_file_list)\n",
    "            date, time = aivm_helper.time_stamp()\n",
    "            # file_name = 'audited_ALL_' + date + '-' + time + '.gz' # If using compression\n",
    "            # df.to_csv(FILE_PATH + file_name, sep='\\t', compression='gzip') # If using compression\n",
    "\n",
    "            file_name = 'audited_ALL_' + date + '-' + time + '.tsv'\n",
    "            file_name_unique = 'audited_ALL.tsv'\n",
    "\n",
    "            audited_df.to_csv(audited_data_dir_local + file_name, sep='\\t')\n",
    "\n",
    "            # push to github\n",
    "            my_file_list = [audited_data_dir_local +\n",
    "                            file_name, audited_data_dir_local + file_name]\n",
    "            push_to_git_as = [audited_data_dir_git +\n",
    "                              file_name_unique, audited_data_dir_git + file_name]\n",
    "\n",
    "            commit, message = aivm_helper.save_to_github(\n",
    "                git_user, git_pswd, git_owner_repo, my_file_list, push_to_git_as, \"auto push aggregated audit\")\n",
    "\n",
    "            if (commit != \"error\"):\n",
    "                print(\"File to commit:\", my_file_list[0])\n",
    "                print(\"Push to git as:\", push_to_git_as[0])\n",
    "                print(\"Commit: \", commit)\n",
    "                display(\n",
    "                    HTML(\"<font color='green'><b>Files Merged and Saved!!</b></font>\"))\n",
    "\n",
    "            if (commit == \"error\"):\n",
    "                print(\"File to commit: \", my_file_list[0])\n",
    "                print(\"Push to git as: \", push_to_git_as[0])\n",
    "                display(HTML(\"<font color='red'><b>Warning!!</b></font>\"))\n",
    "                print(message)\n",
    "\n",
    "            return audited_df, message\n",
    "        else:\n",
    "            print(\"*****NO FILES RETRIEVED******\")\n",
    "        gc.collect()\n",
    "\n",
    "    # Method to get the latest aggregate audited data and to return a pandas DB containing the aggregated content.\n",
    "\n",
    "    def get_aggregated_gathered_data(self, use_git,\n",
    "                                     gathered_data_dir_git,\n",
    "                                     gathered_data_dir_local,\n",
    "                                     github_gathered_url,\n",
    "                                     git_owner,\n",
    "                                     git_repo):\n",
    "        use_git = use_git\n",
    "        gathered_data_dir_git = gathered_data_dir_git\n",
    "        gathered_data_dir_local = gathered_data_dir_local\n",
    "        github_gathered_url_path = github_gathered_url\n",
    "        git_owner = git_owner\n",
    "        git_repo = git_repo\n",
    "\n",
    "        aivm_helper = AIVM_helper()\n",
    "\n",
    "        gathered_conventions_files = []\n",
    "        gathered_conventions_data = {}\n",
    "        gathered_all_df = pd.DataFrame(columns=['text', 'provenance'])\n",
    "\n",
    "        if (use_git == True):\n",
    "            gathered_conventions_files = aivm_helper.list_files_from_github_dir(\n",
    "                git_owner, git_repo, gathered_data_dir_git)\n",
    "            gathered_conventions_files = aivm_helper.remove_excluded_files(\n",
    "                gathered_conventions_files)\n",
    "            #aivm_helper.remove_excluded_files_except(my_file_list, \"audited_training_data\")\n",
    "            # Read conventions data file from github.\n",
    "            for f in gathered_conventions_files:\n",
    "                gathered_conventions_data[aivm_helper.clean_file_name(f)] = pd.read_csv(\n",
    "                    github_gathered_url_path+f, sep='\\t', error_bad_lines=False)\n",
    "                # NOTE: \"truncated\": false  we should check for truncated = true to do follow on call to get all files\n",
    "                tmp_df = gathered_conventions_data[aivm_helper.clean_file_name(\n",
    "                    f)]\n",
    "                tmp_df = tmp_df.rename(columns={\"sentence\": \"text\"})\n",
    "                gathered_all_df = gathered_all_df.append(tmp_df)\n",
    "\n",
    "        elif (use_git == False):\n",
    "            # For each file() in gathered conventions folder\n",
    "            gathered_conventions_files = [\n",
    "                f for f in os.listdir(gathered_data_dir_local)]\n",
    "            gathered_conventions_files = aivm_helper.remove_excluded_files(\n",
    "                gathered_conventions_files)\n",
    "            for f in gathered_conventions_files:\n",
    "                gathered_conventions_data[aivm_helper.clean_file_name(f)] = pd.read_csv(\n",
    "                    os.path.join(gathered_data_dir_local, f), sep='\\t')\n",
    "                tmp_df = gathered_conventions_data[aivm_helper.clean_file_name(\n",
    "                    f)]\n",
    "                tmp_df = tmp_df.rename(columns={\"sentence\": \"text\"})\n",
    "                gathered_all_df = gathered_all_df.append(tmp_df)\n",
    "\n",
    "        gc.collect()\n",
    "        return gathered_conventions_files, gathered_all_df\n",
    "    \n",
    "    def get_training_data(self, use_git,\n",
    "                          dir_file_git,\n",
    "                          dir_file_local,\n",
    "                          git_owner,\n",
    "                          git_repo):\n",
    "        print (\"****\")\n",
    "        print (\"****\")\n",
    "        print (\"****\")\n",
    "        print (\"ERROR: 'get_training_data' is depricated, please rename to 'get_labelled_data' instead, with same inputs\" )\n",
    "        print (\"****\")\n",
    "        print (\"****\")\n",
    "        print (\"****\")\n",
    "        print (\"****\")\n",
    "    \n",
    "    def get_labelled_data(self, use_git,\n",
    "                          dir_file_git,\n",
    "                          dir_file_local,\n",
    "                          git_owner,\n",
    "                          git_repo):\n",
    "\n",
    "\n",
    "        parsing_helpers = generic_parsing_helpers()\n",
    "\n",
    "        if (use_git == True):\n",
    "            df = pd.read_csv(dir_file_git, sep='\\t', error_bad_lines=False, index_col=0)\n",
    "\n",
    "        elif (use_git == False):\n",
    "            df = pd.read_csv(dir_file_local, sep='\\t', index_col=0)\n",
    "        \n",
    "        df = df.reset_index()\n",
    "\n",
    "        df = parsing_helpers.get_clean_df_text_column(df, 'text')\n",
    "        df['text'] = df['text'].str.strip()\n",
    "        df['text'] = df['text'].apply(aivm_helper.pre_process_sentence)\n",
    "        gc.collect()\n",
    "        return df.drop(\"index\", axis = 1)\n",
    "\n",
    "    # Method to retrieve a pandas df created by a direct call to the audited data in github. returns the audited data\n",
    "    # in same format as the training data, so only with the new label, named 'label'\n",
    "\n",
    "    def retrieve_aggregate_audited_data_from_git(self, path):\n",
    "        df = pd.read_csv(path, sep='\\t', error_bad_lines=False)\n",
    "        df = df.drop(df.columns[0], axis=1)\n",
    "        df = df.drop(['old'], axis=1)\n",
    "        df = df.rename(columns={'new': 'label'})\n",
    "        gc.collect()\n",
    "        return df\n",
    "\n",
    "    # Method to retrieve a pandas df created by a direct call to the audited data in github. returns the audited data\n",
    "    # in same format as the training data, so only with the new label, named 'label'\n",
    "\n",
    "    def retrieve_tsv_data_from_git(self, path):\n",
    "        df = pd.read_csv(path, sep='\\t', error_bad_lines=False)\n",
    "        df = df.drop(df.columns[0], axis=1)\n",
    "        gc.collect()\n",
    "        return df\n",
    "\n",
    "    def create_label_balance(self, df):\n",
    "        '''takes a pandas df, checks for an even balance of labels 0 and 1\n",
    "        per convention, and modifies data file to ensure even distribution,\n",
    "        the modified df is saved to git as a .tsv, a timestamped version is also created\n",
    "        the latest one will have title  'training_ALL.tsv'\n",
    "        the timestamped will have title 'training_ALL.{date}-{timme}.tsv'\n",
    "        the format as currently supported by each model =  label\ttext\tprovenance\tconvention\n",
    "        '''\n",
    "\n",
    "        aivm_helper = AIVM_helper()\n",
    "        conv_list = ['civic', 'domestic', 'green', 'industrial',\n",
    "                     'inspired', 'market', 'project', 'renown']\n",
    "\n",
    "        # The data format we need to return\n",
    "        new_df = pd.DataFrame(\n",
    "            columns=['label', 'text', 'provenance', 'convention'])\n",
    "\n",
    "        # Make sure index is ok on the input data.\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "        # Must ensure that for each convention there is an even balance of 0 and 1 labels\n",
    "        counts = df[conv_list].apply(pd.Series.value_counts)\n",
    "        display(counts)\n",
    "        conv_label_0 = []\n",
    "        conv_label_1 = []\n",
    "        # we should have the same amount of label 0 as label 1.\n",
    "        # if we have too many 1,s we can fill with 0(not_convention)\n",
    "        for conv in conv_list:\n",
    "            balance = counts[conv][1]\n",
    "            conv_one = df[(df[conv] == 1)]\n",
    "            # As there is excess of label 1 we will get label 0's from the other conventions.\n",
    "            conv_zero = df[(df[conv] == 0)].sample(n=balance)\n",
    "            temp = pd.concat([conv_one, conv_zero], sort=False)\n",
    "            # FORMAT FROM: text\tprovenance\tcivic\tdomestic\tgreen\tindustrial\tinspired\tmarket\tproject\trenown\n",
    "            # FORMAT TO: label\ttext\tprovenance\tconvention\n",
    "            temp = temp.rename(columns={conv: 'label'})\n",
    "            temp['convention'] = conv\n",
    "            temp = temp[['label', 'text', 'provenance', 'convention']]\n",
    "            new_df = pd.concat([new_df, temp], sort=False)\n",
    "\n",
    "\n",
    "        new_df = new_df.reset_index(drop=True)#labelled_ALL\n",
    "        countsNew = pd.crosstab(new_df['label'], new_df['convention'])\n",
    "        return new_df, countsNew\n",
    "    \n",
    "\n",
    "    def co_occurance_matrix(self, df_class_only):\n",
    "        convention_list = []\n",
    "        for col in df_class_only.columns:\n",
    "            convention_list.append(col)\n",
    "        df_class_only = df_class_only.astype(float)\n",
    "\n",
    "        # The dot product of df_class_only and the transpose of df_class_only\n",
    "        co_occ_a = df_class_only.T.dot(df_class_only)\n",
    "        co_occ = co_occ_a.copy()\n",
    "        gc.collect()\n",
    "        jaccardHTML = HTML('The Jaccard similarity index:(Sometimes called the Jaccard similarity coefficient) compares members for two sets to see which members are shared and which are distinct. It’s a measure of similarity for the two sets of data, with a range from 0% to 100%. The higher the percentage, the more similar the two populations. Although it’s easy to interpret, it is extremely sensitive to small samples sizes and may give erroneous results, especially with very small samples or data sets with missing observations.<br/>J(X,Y) = |X∩Y| / |X∪Y|')\n",
    "\n",
    "        # normalise using Jaccard similarity, ie  The Jaccard similarity index\n",
    "        # (sometimes called the Jaccard similarity coefficient) compares members for two sets\n",
    "        # to see which members are shared and which are distinct. It’s a measure of similarity\n",
    "        # for the two sets of data, with a range from 0% to 100%. The higher the percentage, the\n",
    "        # more similar the two populations. Although it’s easy to interpret, it is extremely\n",
    "        # sensitive to small samples sizes and may give erroneous results, especially with very\n",
    "        # small samples or data sets with missing observations.\n",
    "        # for co-occurance of 'green and industrial' divide by sum of 'green or industrial'\n",
    "        # and divide each entry in the matrix by it).\n",
    "        # this should result in symmetric matrix.\n",
    "        ##############################################\n",
    "        for row in convention_list:\n",
    "            for column in convention_list:\n",
    "                if row != column:\n",
    "                    #print(\"new value\", co_occ.at[row,column]/(co_occ.at[row,row] + co_occ.at[column,column]))\n",
    "                    co_occ.at[row, column] = round(\n",
    "                        co_occ.at[row, column]/(co_occ.at[row, row] + co_occ.at[column, column]), 2)\n",
    "        for row in convention_list:\n",
    "            for column in convention_list:\n",
    "                if row == column:\n",
    "                    co_occ.at[row, column] = 1\n",
    "        ###############################################\n",
    "\n",
    "        display(HTML(\n",
    "            \"<b>***Co-Occurance matrix (calculated with df_class_only.T.dot(df_class_only)***</b>\"))\n",
    "        display(co_occ_a)\n",
    "        print(\"\")\n",
    "        display(\n",
    "            HTML(\"<b>***Co-Occurance matrix Normalised with Jaccard similarity***</b>\"))\n",
    "        display(jaccardHTML)\n",
    "        display(co_occ)\n",
    "\n",
    "        print(\"\")\n",
    "        display(\n",
    "            HTML(\"<b>***Co-occurance heat map normalised using Jaccard similarity***</b>\"))\n",
    "        ax = sns.heatmap(co_occ, annot=True, vmin=0, vmax=1,\n",
    "                         center=0.5, annot_kws={\"fontsize\": 10})\n",
    "        ax.set_ylabel('convention')\n",
    "        ax.set_xlabel('convention')\n",
    "        ax.set_title('co-occurance matrix')\n",
    "        gc.collect()\n",
    "        return ax\n",
    "\n",
    "    def label_cardinality_bar(self, df_class_only, title=\"Label Cardinality count\"):\n",
    "\n",
    "        #rowSums = df_class_only.iloc[:,2:].sum(axis=1)\n",
    "        rowSums = df_class_only.sum(axis=1)\n",
    "        multiLabel_counts = rowSums.value_counts()\n",
    "\n",
    "        multiLabel_counts = multiLabel_counts.sort_index(ascending=True)\n",
    "\n",
    "        ax = sns.barplot(multiLabel_counts.index, multiLabel_counts.values)\n",
    "        ax.set_title(title, fontsize=18)\n",
    "        ax.set_ylabel('Number of sentences', fontsize=16)\n",
    "        ax.set_xlabel('Number of conventions(labels)', fontsize=16)\n",
    "\n",
    "        # adding the text labels\n",
    "        rects = ax.patches\n",
    "        labels = multiLabel_counts.values\n",
    "        for rect, label in zip(rects, labels):\n",
    "            height = rect.get_height()\n",
    "            ax.text(rect.get_x() + rect.get_width()/3,\n",
    "                    height, label, ha='center', va='bottom')\n",
    "        return ax\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    def get_keyword_list(self, df, min_score, top_x, show=True):\n",
    "        MIN_TFIDF_SCORE = min_score\n",
    "        \n",
    "        convention_list =  sorted (list(self.keyword_from_social_scientists_dict.keys()))\n",
    "        keywords_df = pd.DataFrame(columns=['source', 'convention', 'word'])\n",
    "\n",
    "        for conv in self.keyword_from_social_scientists_dict:\n",
    "            temp_df = pd.DataFrame(columns=['source', 'convention', 'word'])\n",
    "            temp_df['word'] = model_helpers.keyword_from_social_scientists_dict[conv]\n",
    "            temp_df['convention'] = conv\n",
    "            temp_df['source'] = \"domain_expert\"\n",
    "            keywords_df = pd.concat([keywords_df, temp_df])\n",
    "\n",
    "        def sort_coo(coo_matrix):\n",
    "            tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "            return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "        def extract_topn_from_vector(feature_names, sorted_items, topn=10, min_score=MIN_TFIDF_SCORE):\n",
    "            \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "            # use only items with > min score\n",
    "            sorted_items = [[idx, score]\n",
    "                            for idx, score in sorted_items if score >= min_score]\n",
    "\n",
    "            # return only top n items\n",
    "            if len(sorted_items) > topn:\n",
    "                sorted_items = sorted_items[:topn]\n",
    "\n",
    "            score_vals = []\n",
    "            feature_vals = []\n",
    "\n",
    "            for idx, score in sorted_items:\n",
    "                fname = feature_names[idx]\n",
    "\n",
    "                # keep track of feature name and its corresponding score\n",
    "                score_vals.append(round(score, 3))\n",
    "                feature_vals.append(feature_names[idx])\n",
    "\n",
    "            # create a tuples of feature,score\n",
    "            #results = zip(feature_vals,score_vals)\n",
    "            results = {}\n",
    "            for idx in range(len(feature_vals)):\n",
    "                results[feature_vals[idx]] = score_vals[idx]\n",
    "\n",
    "            return results\n",
    "\n",
    "        stopwords = parsing_helpers.get_stop_words(paths.STOP_WORDS_DIR_FILE_LOCAL)\n",
    "\n",
    "        docs = []\n",
    "\n",
    "        for conv in convention_list:\n",
    "            docs.append(\"\\n\".join(list(df[(df[conv] == 1)]['text'].values)))\n",
    "        # create a vocabulary of words,\n",
    "        # ignore words that appear in 85% of documents,\n",
    "        # eliminate stop words\n",
    "        cv = CountVectorizer(max_df=0.85, stop_words=stopwords)\n",
    "        word_count_vector = cv.fit_transform(docs)\n",
    "\n",
    "        tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "        tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "        # you only needs to do this once\n",
    "        feature_names = cv.get_feature_names()\n",
    "\n",
    "        # get the document that we want to extract keywords from\n",
    "        for doc_idx, doc in enumerate(docs):\n",
    "            if show == True:\n",
    "                print(\"=====================================\")\n",
    "                print(\"         {}                   \".format(\n",
    "                    convention_list[doc_idx]))\n",
    "                print(\"=====================================\")\n",
    "\n",
    "            # generate tf-idf for the given document\n",
    "            tf_idf_vector = tfidf_transformer.transform(cv.transform([doc]))\n",
    "\n",
    "            # sort the tf-idf vectors by descending order of scores\n",
    "            sorted_items = sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "            # extract only the top n; n here is 30\n",
    "            keywords = extract_topn_from_vector(\n",
    "                feature_names, sorted_items, top_x, MIN_TFIDF_SCORE)\n",
    "            key_list = list(keywords.keys())\n",
    "            if show == True:\n",
    "                print(key_list)\n",
    "            temp_df = pd.DataFrame(columns=['source', 'convention', 'word'])\n",
    "            temp_df['word'] = key_list\n",
    "            temp_df['convention'] = convention_list[doc_idx]\n",
    "            temp_df['source'] = \"training_data\"\n",
    "            keywords_df = pd.concat([keywords_df, temp_df])\n",
    "\n",
    "        display(HTML(\"<font color = green><h3><left>Top 5 Keywords from labelled data for results section</left></h3></font>\"))\n",
    "        top_x_df = {}\n",
    "        for doc_idx, doc in enumerate(docs):\n",
    "            # generate tf-idf for the given document\n",
    "            tf_idf_vector = tfidf_transformer.transform(cv.transform([doc]))\n",
    "            # sort the tf-idf vectors by descending order of scores\n",
    "            sorted_items = sort_coo(tf_idf_vector.tocoo())\n",
    "            # Extract the top 5 for the paper Results\n",
    "            keywords = extract_topn_from_vector(\n",
    "                feature_names, sorted_items, 5, MIN_TFIDF_SCORE)\n",
    "            key_list = list(keywords.keys())\n",
    "            key_list = [x.capitalize() for x in key_list]\n",
    "\n",
    "            top_x_df[convention_list[doc_idx]] = str(', '.join(key_list))\n",
    "        \n",
    "        top = pd.DataFrame.from_dict(\n",
    "            top_x_df, orient='index', columns=['Keywords'])\n",
    "    \n",
    "        top ['Convention'] = [x.capitalize() for x in convention_list]\n",
    "        top = top[['Convention', 'Keywords']]\n",
    "        top = top.reset_index()\n",
    "        top = top.drop(top.columns[[0]], axis=1)\n",
    "        return keywords_df, top\n",
    "\n",
    "    \n",
    "    def get_positive_preds_with_threshold(self, preds, threshold=0.5):\n",
    "        #Keras .predict returns a list of [x, y] values where x = probability of 0 and y is probability of 1\n",
    "        #ie \n",
    "        preds_results = []\n",
    "        for j in range(len(preds)):\n",
    "            #int_pred = int(preds[j][1]>=threshold)\n",
    "            int_pred = int(preds[j]>=threshold)\n",
    "            preds_results.append(int_pred)\n",
    "        return preds_results\n",
    "\n",
    "    def get_positive_preds_probabilities(self, preds):\n",
    "        #Keras .predict returns a list of [x, y] values where x = probability of 0 and y is probability of 1\n",
    "        #i.e preds[j][1] is the probability of a positive prediction.\n",
    "        preds_results = []\n",
    "        for j in range(len(preds)):\n",
    "            int_pred = preds[j][1]\n",
    "            preds_results.append(int_pred)    \n",
    "        return preds_results\n",
    "\n",
    "    def apply_pred_threshold(self, preds, t):\n",
    "        res = np.zeros(len(preds))  \n",
    "        res[preds>=t] = 1\n",
    "        return res\n",
    "    \n",
    "    def get_count_per_model(self, model, df, num_bins, thresholds):\n",
    "    #just to remember: we are not using \"conf_bin\" here it is for the set confidence\n",
    "\n",
    "        model_count = df[['text', model+'_prob_1',model+'_y_pred', model]].copy()\n",
    "        model_count['prob_bin'] = pd.qcut(model_count[ model+'_prob_1'], q=num_bins, duplicates='drop')\n",
    "        model_grouped_count = model_count.groupby('prob_bin')[model+'_prob_1'].count().copy()\n",
    " \n",
    "        mean_prob_per_bucket = []\n",
    "        occurance_per_bucket = []\n",
    "        total = 0\n",
    "        for x in range (len(model_grouped_count)):\n",
    "            a = model_grouped_count.index.categories[x].left\n",
    "            b = model_grouped_count.index.categories[x].right\n",
    "            mean_prob_per_bucket.append (np.mean([a,b]))\n",
    "            occurance_per_bucket.append (model_grouped_count[x])\n",
    "\n",
    "        def percentage(percent, whole):\n",
    "            return (percent * whole) / 100.0\n",
    "        total = 0\n",
    "        for prob, occurances in zip(mean_prob_per_bucket,occurance_per_bucket ):\n",
    "            total = total + percentage(prob*100,occurances)\n",
    "    \n",
    "        actual = \"not applicable\"\n",
    "        try:\n",
    "            counts_y_actual = model_count[model]==1\n",
    "            actual = counts_y_actual.value_counts()[1]\n",
    "        except:\n",
    "            actual = \"not applicable\"\n",
    "        \n",
    "        counts_y_predict = model_count[model+'_y_pred']==1\n",
    "        try:\n",
    "            predict = counts_y_predict.value_counts()[1]\n",
    "        except:\n",
    "            predict = 0\n",
    "    \n",
    "\n",
    "        return actual, int(round(total)), predict, thresholds[model]\n",
    "\n",
    "    def get_count(self, df_in, _DLModels, thresholds, num_prob_buckets = 8, actual=False ):\n",
    "        plt.clf()\n",
    "        predicted_prob_count = {}\n",
    "        predicted_classification_count = {}\n",
    "        actual_count = {}\n",
    "        predicted_prob_count_list = []\n",
    "        predicted_classification_count_list = []\n",
    "        actual_count_list = []\n",
    "        model_list = []\n",
    "        thr_list = []\n",
    "        thresh = thresholds\n",
    "        df = pd.DataFrame (columns = ['Convention','Calibrated Prob count','Classifier count','Threshold','True count'])\n",
    "        for k in _DLModels.keys():\n",
    "            actual_cnt, prob_cnt, classif_cnt, thr = self.get_count_per_model(k, df_in, num_prob_buckets, thresh)\n",
    "            predicted_prob_count[k] = prob_cnt\n",
    "            predicted_classification_count[k] = classif_cnt\n",
    "            actual_count[k] = actual_cnt\n",
    "        \n",
    "            predicted_prob_count_list.append(prob_cnt)\n",
    "            predicted_classification_count_list.append(classif_cnt)\n",
    "            actual_count_list.append(actual_cnt)\n",
    "            model_list.append(k)\n",
    "            thr_list.append(thr)\n",
    "        my_colors = ['orange', 'green',  'red', 'pink', 'purple', 'blue', 'brown', 'black']    \n",
    "        df['Calibrated Prob count'] = predicted_prob_count_list\n",
    "        df['Classifier count'] = predicted_classification_count_list\n",
    "        df['True count'] = actual_count_list\n",
    "        df['Convention'] = model_list\n",
    "        df['Threshold'] = thr_list\n",
    "        choose = []\n",
    "        choose.append(predicted_prob_count [max(predicted_prob_count, key=predicted_prob_count.get)])\n",
    "        choose.append(predicted_classification_count [max(predicted_classification_count, key=predicted_classification_count.get)])\n",
    "        choose.append(actual_count [max(actual_count, key=actual_count.get)])\n",
    "  \n",
    "        fig = go.Figure(data=[\n",
    "        go.Bar(name='Calibrated confidence', x=model_list, y=predicted_prob_count_list, textposition='auto'),\n",
    "        go.Bar(name='Actual count', x=model_list, y=actual_count_list, textposition='auto'),\n",
    "        go.Bar(name='Predicted by classifier', x=model_list, y=predicted_classification_count_list, textposition='auto'),\n",
    "    \n",
    "        ] )\n",
    "        # Change the bar mode\n",
    "        fig.update_layout(barmode='group', xaxis_tickangle=-45, title_text='Probabilistic count vs classifier predicted' )#stack\n",
    "        fig.show()\n",
    "        table = ff.create_table(df)\n",
    "        iplot(table, filename='Count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
