{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T21:17:47.785017Z",
     "start_time": "2019-12-03T21:17:37.390327Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Necessary libraries imported.\n",
      "Initialised generic_parsing_helpers class and methods\n",
      "Initialised AIVM_helper class and methods\n",
      "Initialised model_helpers class and methods\n"
     ]
    }
   ],
   "source": [
    "%run \"All_helper_classes.ipynb\" sort=False\n",
    "\n",
    "#from \"Models training helpers.ipynb\" we will import 3 helper classes and the associated helper methods.\n",
    "parsing_helpers = generic_parsing_helpers()\n",
    "aivm_helper = AIVM_helper()\n",
    "model_helpers, info = default_model_helpers_for_project()\n",
    "#if you want to see the paths now can call with...\n",
    "# # display (HTML(info))\n",
    "\n",
    "paths = project_paths()\n",
    "\n",
    "#same for paths and repo data, display using...\n",
    "# display (HTML(paths.get_paths_data()))\n",
    "# display (HTML(paths.get_repo_data()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T21:17:47.971825Z",
     "start_time": "2019-12-03T21:17:47.944469Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "//To remove scroll from any output area and automatically extend the jupyter cell\n",
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "//To remove scroll from any output area and automatically extend the jupyter cell\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First we will retrieve the most recently trained DL Models, Calibration Models and the Threshold.\n",
    "these have been stored in the \"Resource\" directory of Git as a pickle file.\n",
    "        * global_final_thresholds.pickle\n",
    "        * {conv}_models.pickle\n",
    "        * {conv}_calibration_model.pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T21:19:53.121891Z",
     "start_time": "2019-12-03T21:17:48.191997Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from GIT: global_final_thresholds.pickle?raw=true\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'civic': 0.5,\n",
       " 'domestic': 0.5,\n",
       " 'green': 0.5,\n",
       " 'industrial': 0.5,\n",
       " 'inspired': 0.5,\n",
       " 'market': 0.5,\n",
       " 'project': 0.5,\n",
       " 'renown': 0.5}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from GIT: civic_models.pickle?raw=true\n",
      "Reading from GIT: domestic_models.pickle?raw=true\n",
      "Reading from GIT: green_models.pickle?raw=true\n",
      "Reading from GIT: industrial_models.pickle?raw=true\n",
      "Reading from GIT: inspired_models.pickle?raw=true\n",
      "Reading from GIT: market_models.pickle?raw=true\n",
      "Reading from GIT: project_models.pickle?raw=true\n",
      "Reading from GIT: renown_models.pickle?raw=true\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'civic': {'model': <keras.engine.training.Model at 0x1369a3f98>},\n",
       " 'domestic': {'model': <keras.engine.training.Model at 0x136850d30>},\n",
       " 'green': {'model': <keras.engine.training.Model at 0x136850c18>},\n",
       " 'industrial': {'model': <keras.engine.training.Model at 0x1372e4cc0>},\n",
       " 'inspired': {'model': <keras.engine.training.Model at 0x136ad7278>},\n",
       " 'market': {'model': <keras.engine.training.Model at 0x1381a0fd0>},\n",
       " 'project': {'model': <keras.engine.training.Model at 0x13b1ebf60>},\n",
       " 'renown': {'model': <keras.engine.training.Model at 0x13b1f2f98>}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from GIT: civiccalibration_model.pickle?raw=true\n",
      "Reading from GIT: domesticcalibration_model.pickle?raw=true\n",
      "Reading from GIT: greencalibration_model.pickle?raw=true\n",
      "Reading from GIT: industrialcalibration_model.pickle?raw=true\n",
      "Reading from GIT: inspiredcalibration_model.pickle?raw=true\n",
      "Reading from GIT: marketcalibration_model.pickle?raw=true\n",
      "Reading from GIT: projectcalibration_model.pickle?raw=true\n",
      "Reading from GIT: renowncalibration_model.pickle?raw=true\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'civic': {'model': IsotonicRegression(increasing=True, out_of_bounds='clip', y_max=None,\n",
       "                     y_min=None)},\n",
       " 'domestic': {'model': IsotonicRegression(increasing=True, out_of_bounds='clip', y_max=None,\n",
       "                     y_min=None)},\n",
       " 'green': {'model': IsotonicRegression(increasing=True, out_of_bounds='clip', y_max=None,\n",
       "                     y_min=None)},\n",
       " 'industrial': {'model': IsotonicRegression(increasing=True, out_of_bounds='clip', y_max=None,\n",
       "                     y_min=None)},\n",
       " 'inspired': {'model': IsotonicRegression(increasing=True, out_of_bounds='clip', y_max=None,\n",
       "                     y_min=None)},\n",
       " 'market': {'model': IsotonicRegression(increasing=True, out_of_bounds='clip', y_max=None,\n",
       "                     y_min=None)},\n",
       " 'project': {'model': IsotonicRegression(increasing=True, out_of_bounds='clip', y_max=None,\n",
       "                     y_min=None)},\n",
       " 'renown': {'model': IsotonicRegression(increasing=True, out_of_bounds='clip', y_max=None,\n",
       "                     y_min=None)}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from GIT: extended_tokenizer.pickle?raw=true\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tokenizer': <keras_preprocessing.text.Tokenizer at 0x13b1d8438>}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Read the latest models/Tresholds etc from GIT \"Resource\" directory.\n",
    "USE_GIT_HUB = True\n",
    "\n",
    "\n",
    "AI_GITHUB = \"Data/ToBeAnalyzed/github_AI_repos_dump.csv\"\n",
    "REDDIT_AI = \"Data/ToBeAnalyzed/reddit_data_all.csv\"\n",
    "\n",
    "# NON_AI_GITHUB = \"Data/ToBeAnalyzed/github_non_AI_repos_dump.csv\"\n",
    "# SEM_SCHOLAR = \"\"\n",
    "\n",
    "\n",
    "AI_GITHUB_GIT = \"https://github.com/aideenf/AIVC/blob/master/cp_wssc/Data/ToBeAnalyzed/github_AI_repos_dump.csv?raw=true\"\n",
    "REDDIT_AI_GIT = \"https://github.com/aideenf/AIVC/blob/master/cp_wssc/Data/ToBeAnalyzed/reddit_data_all.csv?raw=true\"\n",
    "\n",
    "# NON_AI_GITHUB_GIT = \"Data/ToBeAnalyzed/github_non_AI_repos_dump.csv\"\n",
    "# SEM_SCHOLAR_GIT = \"\"\n",
    "\n",
    "\n",
    "_DLModelsIsotonicRegression = {}\n",
    "_DLModels = {}\n",
    "_thresholds = {}\n",
    "_tokenizer = \"\"\n",
    "\n",
    "_thresholds = model_helpers.read_thresholds_from_pickle(USE_GIT_HUB)\n",
    "display(_thresholds)\n",
    "\n",
    "_DLModels = model_helpers.read_models_from_pickle(USE_GIT_HUB, _thresholds )\n",
    "display(_DLModels)\n",
    "\n",
    "_DLModelsIsotonicRegression = model_helpers.read_calibration_models_from_pickle(USE_GIT_HUB, _thresholds)\n",
    "display(_DLModelsIsotonicRegression)\n",
    "\n",
    "_tokenizer = model_helpers.read_tokenizer_from_pickle(USE_GIT_HUB)\n",
    "display (_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T01:56:31.155223Z",
     "start_time": "2019-12-04T01:56:31.062586Z"
    }
   },
   "outputs": [],
   "source": [
    "# This method will use the trained classification and calibration models to predict and quantify \n",
    "# the presence of conventions in new data(the data to be analyzed)\n",
    " \n",
    "def predict_scores_and_classify(thresholds_in, to_predict_df, _tokenizer, _DLModels, _DLModelsIsotonicRegression, seq_len):\n",
    "    \n",
    "    predict = True\n",
    "     \n",
    "    extended_tokenizer = _tokenizer\n",
    "        \n",
    "    sequences = extended_tokenizer.texts_to_sequences(to_predict_df['text'])\n",
    "    data = pad_sequences(sequences, maxlen= seq_len) \n",
    "    extended_tokenizer = 0\n",
    "    sequences = 0\n",
    "    gc.collect()\n",
    "    \n",
    "    thresholds = {'civic': 0.5,\n",
    "                  'domestic': 0.5,\n",
    "                  'green': 0.5,\n",
    "                  'industrial': 0.5,\n",
    "                  'inspired': 0.5,\n",
    "                  'market': 0.5,\n",
    "                  'project': 0.5,\n",
    "                  'renown': 0.5}\n",
    "    # otherwise use default.\n",
    "    \n",
    "    if thresholds_in != None:\n",
    "        thresholds = thresholds_in\n",
    "       \n",
    "    column_list = []\n",
    "    column_list.append(\"repo_id\")\n",
    "    column_list.append(\"text\")\n",
    "    column_list.append(\"repo\")\n",
    "    \n",
    "    for conv in _DLModels.keys():\n",
    "        column_list.append(conv)\n",
    "        column_list.append(conv+\"_prob\")\n",
    "        column_list.append(conv+\"_y_pred\")\n",
    "        column_list.append(conv+\"_prob_1\")\n",
    "        \n",
    "    df = pd.DataFrame(columns=column_list)\n",
    "    all_df = pd.DataFrame(columns=column_list)\n",
    "    \n",
    "    ###################\n",
    "    #a sub function within the function, just used in this scope \n",
    "    def sub_funct_get_prob(prob, y):\n",
    "        res = []\n",
    "        for (probability, predicted) in zip(prob, y):\n",
    "            if predicted == 1:\n",
    "                res.append(probability)\n",
    "            if predicted == 0:\n",
    "                res.append(1 - probability)\n",
    "        return res\n",
    "    ####################\n",
    "\n",
    "    df = pd.DataFrame(columns=column_list)\n",
    "    \n",
    "    #get the the txt of the sentence to be Analyzed\n",
    "    df['text'] = to_predict_df[\"text\"]\n",
    "    df['repo_id'] = to_predict_df[\"repo_id\"]\n",
    "    df['repo'] = to_predict_df[\"repo\"]\n",
    "\n",
    "        \n",
    "\n",
    "    for model in _DLModels.keys():\n",
    "        my_model = _DLModels[model]           \n",
    "        #First through the model\n",
    "        y_predict = my_model['model'].predict(data)\n",
    "                \n",
    "        #get the score for value = 1 as the response is [x,y] we need only the y values\n",
    "        y_pos_predict_prob = model_helpers.get_positive_preds_probabilities(y_predict)\n",
    "                \n",
    "        #get the 1 or 0 value depending on the threshold\n",
    "        y_pred = model_helpers.get_positive_preds_with_threshold(y_pos_predict_prob, thresholds[model])\n",
    "                \n",
    "        #if the call has been made with an isotonic reg calibration model too then call its predict.\n",
    "        if _DLModelsIsotonicRegression != None:\n",
    "            other_ir = _DLModelsIsotonicRegression[model]\n",
    "            y_pos_predict_isotonic_prob = other_ir['model'].predict(y_pos_predict_prob)\n",
    "            y_pred = model_helpers.get_positive_preds_with_threshold(y_pos_predict_isotonic_prob, thresholds[model])\n",
    "            y_pos_predict_prob = y_pos_predict_isotonic_prob\n",
    "                \n",
    "        #The probability of the y_predict, whether it i 1 or 0\n",
    "        df[model+\"_prob\"] =  sub_funct_get_prob(y_pos_predict_prob, y_pred)\n",
    "        df[model+\"_y_pred\"] = y_pred\n",
    "        df[model+\"_prob_1\"] = y_pos_predict_prob         \n",
    "        \n",
    "   \n",
    "    y_pred_list = []\n",
    "    for x in df.columns.values:\n",
    "        if x.endswith(('_y_pred')):\n",
    "            y_pred_list.append(x)\n",
    "            \n",
    "    prob_list = []\n",
    "    conv_list = []\n",
    "    for x in df.columns.values:\n",
    "        if x.endswith(('_prob')):\n",
    "            prob_list.append(x)\n",
    "            conv_list.append(x.replace('_prob', ''))\n",
    "    \n",
    "    if predict == True:\n",
    "        #add the label cardinality as the sum of all the predicted = 1 for sentence for conventions\n",
    "        pred_sums = df[y_pred_list].sum(axis=1)\n",
    "        df['lbl_cnt'] = pred_sums\n",
    "        #add the overall score as the product of all the individual predicted convention scores\n",
    "        prob_product= all_df[prob_list].prod(axis=1)\n",
    "        df['set_conf'] = prob_product\n",
    "    \n",
    "    \n",
    "    #this will be > 1 if the same sentence was used as True for 2 or more conventions.\n",
    "    positive_sample = df[conv_list].sum(axis=1)\n",
    "    df['pos_sample'] = positive_sample\n",
    "     \n",
    "    print (\"Num Dupes:\", len(df.index) - len(df['text'].unique()) )\n",
    "\n",
    "    #now we are droppping the dupes but we should not. \n",
    "    df = df.drop_duplicates(keep = \"first\")\n",
    "    \n",
    "    co_occur_list = []\n",
    "    rename = []\n",
    "    for x in df.columns.values:\n",
    "            if x.endswith(('_y_pred')):\n",
    "                co_occur_list.append(x)\n",
    "                rename.append(x.replace('_y_pred', ''))\n",
    "\n",
    "    co_occur_y_predict = df[co_occur_list].copy()\n",
    "    for old, x in zip(co_occur_list, range(len(co_occur_list))): \n",
    "        co_occur_y_predict = co_occur_y_predict.rename(columns={old: rename[x]})\n",
    "        \n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.style.use('seaborn-dark-palette')\n",
    "        \n",
    "    text = \"Predicted Co-occurance Matrix with Predicted and Actual Cardinality \"\n",
    "    display (HTML(\"<font color = green><h3><left>\" + text + \"</left></h3></font>\"))\n",
    "    ax1 = plt.subplot(1, 2, 1)\n",
    "    ax1 = model_helpers.co_occurance_matrix (co_occur_y_predict)\n",
    "\n",
    "    ax2 = plt.subplot(1, 2, 2)\n",
    "    ax2 = model_helpers.label_cardinality_bar(co_occur_y_predict, title = \"Predicted Cardinality\") \n",
    "    plt.show()   \n",
    "    return df.fillna(0), thresholds\n",
    "\n",
    "\n",
    "def clean_descriptions (text):\n",
    "\n",
    "    #remove snippets of code\n",
    "    text = re.sub(\"```.*?```\", \" \", text)\n",
    "    \n",
    "    #https://regex101.com/r/cO8lqs/2\n",
    "    text = re.sub(\"b'\", ' ', text)\n",
    "    \n",
    "    text = re.sub(\"i\\.e\\.\", 'i.e', text)\n",
    "    text = re.sub(\"e\\.g\\.\", 'e.g', text)\n",
    "    \n",
    "    # remove anything with this format \"[![text]....\"\n",
    "    text = re.sub(r'\\[\\!.*?\\)', ' ', text)\n",
    "    \n",
    "    \n",
    "        \n",
    "    # remove anything with this format \"([.....)).\"\n",
    "    text = re.sub(r'\\(\\[.*?\\)\\)\\.', ' ', text)\n",
    "    \n",
    "    \n",
    "    # remove anything with this format \"([.....))\"\n",
    "    text = re.sub(r'\\(\\[.*?\\)\\)', ' ', text)\n",
    "    \n",
    "    \n",
    "    # remove headers and replace with full stop, in case a previous sentence doed not have it\n",
    "    text = re.sub(r'\\#.*?\\n', ' ', text)\n",
    "    \n",
    "    #remove HTML tags\n",
    "    text = re.sub(\"(<.*?>)\",\"\",text)\n",
    "    \n",
    "    #remove URL's\n",
    "    text = re.sub(r'\\(http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    #remove new lines as they are not correctly formatted\n",
    "    text = text.replace('\\\\n', \" \")\n",
    "    text = text.replace('\\n', \" \")\n",
    "\n",
    "\n",
    "\n",
    "    #r\n",
    "    text = text.replace(\"\\\\'\", \"'\")\n",
    "    text = text.replace(\"\\'\", \"'\")\n",
    "    \n",
    "    # remove anything with this format \"[![text]\"\n",
    "    text = re.sub(r'\\[\\!.*?\\]', ' ', text)\n",
    "    \n",
    "    \n",
    "    #remove tables\n",
    "    text = re.sub(r\"[\\|].*?[\\|]\", \"||\", text)\n",
    "    \n",
    "    text = re.sub(r\"[^a-zA-Z0-9.,'-|]\",' ', text)    \n",
    "    text = text.replace(\"[\", \"\")\n",
    "    text = text.replace(\"]\", \"\")\n",
    "    text = text.replace(\"--\", \"\")\n",
    "    text = text.replace(\" -\", \"\")\n",
    "    text = text.replace(\"- \", \"\")\n",
    "    text = re.sub(' +', ' ', text)\n",
    "   \n",
    "    new_list = []\n",
    "    text_list = text.split('||')\n",
    "    for line in text_list:\n",
    "        if len(line) > 25:\n",
    "            new_list.append(line +'\\n')\n",
    "    text = ''   \n",
    "    text = text.join(new_list)\n",
    "    text = text.replace(\"*\", \"\")\n",
    "    text = text.replace(\"\\\\n\", \".\")\n",
    "    text = text.replace(\"\\n\", \".\")\n",
    "\n",
    "    text = text.replace(\" .\", \".\")\n",
    "    text = text.replace(\"..\", \".\")\n",
    "    text = text.lstrip()\n",
    "    text = text.rstrip()\n",
    "    text = text.strip()\n",
    "    \n",
    "    text = re.sub(r\"[\\\\][x][0-9][0-9][0-9]\", \"\", text)\n",
    "    text = re.sub(r\"[\\\\][x][a-z][0-9]\", \"\", text)\n",
    "    text = re.sub(r\"[\\\\][x][a-z][a-z]\", \"\", text)\n",
    "    text = re.sub(r\"[\\\\][x][0-9][0-9]\", \"\", text)\n",
    "    text = re.sub(r\"[\\\\][x][a-z]\", \"\", text)\n",
    "    text = re.sub(r\"[\\\\][x][0-9]\", \"\", text)\n",
    "    text = re.sub(r\"[\\\\][x]\", \" \", text)\n",
    "    text_list = text.split('. ')\n",
    "    new_list = []\n",
    "    for line in text_list:\n",
    "        \n",
    "        line = line.strip()\n",
    "        line = line.capitalize() + '.\\n'\n",
    "        new_list.append(line)\n",
    "    text = ''   \n",
    "    text = text.join(new_list)\n",
    "    \n",
    "    text = re.sub('\\.__', ' ', text)\n",
    "    text = re.sub('__\\.', ' ', text)\n",
    "    text = re.sub('__' ,'', text)\n",
    "    text = re.sub('>_' ,'', text)\n",
    "    text = re.sub('/' ,' ', text)\n",
    "    text = re.sub('{' ,'', text)\n",
    "    text = re.sub('Image::' ,'', text)\n",
    "    text = re.sub(r\"=\",\"\",text)\n",
    "    text = re.sub(r\"\\r\",\"\",text)\n",
    "    text = re.sub(r\"\\(\\)\",\"\",text)\n",
    "    text = text.replace(\" .\", \".\")\n",
    "    text = text.replace(\"..\", \".\")\n",
    "    text = text.replace(\">\", \"\")\n",
    "    text = text.replace(\":height:\", \"\")\n",
    "    text = text.replace(\".. image::\", \"\")\n",
    "    text = text.replace(\":target:\", \"\")\n",
    "    text = text.replace(\":alt:\", \"\")\n",
    "    text = text.replace(\"`_\", \"`\")\n",
    "    text = text.replace(\"^\", \"\")\n",
    "    text = text.replace(\"Code-block::\", \"\")\n",
    "    text = text.replace(\":start-offset\", \"\")\n",
    "    text = text.replace(\":end-offset\", \"\")\n",
    "    text = text.replace(\"\\\\\\\\\", \"\\\\\")\n",
    "    text = text.replace(\"\\t\", \" \")\n",
    "    text = text.replace(\"image::\", \" \")\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    \n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def create_pd_from_repo(AI_texts, AI_repo_id, AI_repo_name ):\n",
    "    # for every repo in the AI_texts file, \n",
    "    # get the raw description\n",
    "    # process it\n",
    "    # convert to an array of sentences\n",
    "    # add each individual sentence to a pandas\n",
    "    # add the repo ID to the pandas.  in the end we will \n",
    "    # have a pandas DB with repo_id and text to be predicted.\n",
    "    # only add the text if it is more than 40 letters long\n",
    "    print (\"num AI repos\", len ( AI_texts))\n",
    "    \n",
    "    df_all = pd.DataFrame()\n",
    "    for x in range(len(AI_texts)):\n",
    "        df = pd.DataFrame()\n",
    "        sent = AI_texts[x]\n",
    "        repo_id = AI_repo_id[x]\n",
    "        repo_name = AI_repo_name[x]\n",
    "        sen_arr = clean_descriptions(sent).split(\"\\n\")\n",
    "        df['text'] = sen_arr\n",
    "        df['repo_id'] = repo_id\n",
    "        df['repo'] = repo_name\n",
    "        df=df[df.text.apply(lambda x: len(str(x))>50)]\n",
    "        df_all = pd.concat([df_all, df])\n",
    "    print (\"full list length\",df_all.shape[0] )\n",
    "    \n",
    "    return df_all\n",
    "\n",
    "def analyze_git_repo(url, thresholds, tokenizer, DLModels, DLModelsIsotonicRegression, model_helpers):\n",
    "    AI_repos = pd.read_csv(url, sep=\";\")\n",
    "    \n",
    "    AI_texts = AI_repos['repo_raw_description'].values\n",
    "    AI_repo_id = AI_repos['repo_id'].values\n",
    "    AI_repo_name = AI_repos['repo_name'].values\n",
    "    AI_repos = 0\n",
    "    gc.collect()\n",
    "    res, _ = predict_scores_and_classify(thresholds, create_pd_from_repo(AI_texts, AI_repo_id, AI_repo_name ), tokenizer, DLModels, DLModelsIsotonicRegression, model_helpers.MAX_SEQUENCE_LENGTH)\n",
    "    AI_repos = 0\n",
    "    AI_texts = 0\n",
    "    AI_repo_id = 0\n",
    "\n",
    "    text = \"Probabilistic count v's Classification count\"\n",
    "    display (HTML(\"<font color = green><h4><left>\" + text + \"</left></h4></font>\"))\n",
    "    display (model_helpers.get_count(res, _DLModels, thresholds, num_prob_buckets = 8, actual=False ))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify GitHub AI repo with latest models\n",
    "\n",
    "    _DLModelsIsotonicRegression \n",
    "    _DLModels\n",
    "    _thresholds\n",
    "    _tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T02:24:48.839588Z",
     "start_time": "2019-12-04T01:56:35.381286Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num AI repos 8609\n",
      "full list length 134608\n",
      "Num Dupes: 5489\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<font color = green><h3><left>Predicted Co-occurance Matrix with Predicted and Actual Cardinality </left></h3></font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>***Co-Occurance matrix (calculated with df_class_only.T.dot(df_class_only)***</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>civic</th>\n",
       "      <th>domestic</th>\n",
       "      <th>green</th>\n",
       "      <th>industrial</th>\n",
       "      <th>inspired</th>\n",
       "      <th>market</th>\n",
       "      <th>project</th>\n",
       "      <th>renown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>civic</th>\n",
       "      <td>397.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>domestic</th>\n",
       "      <td>6.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>green</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>industrial</th>\n",
       "      <td>31.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>80608.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>1743.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inspired</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>385.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>market</th>\n",
       "      <td>19.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1743.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3805.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>project</th>\n",
       "      <td>8.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>469.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>renown</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            civic  domestic  green  industrial  inspired  market  project  \\\n",
       "civic       397.0       6.0    2.0        31.0       1.0    19.0      8.0   \n",
       "domestic      6.0     237.0    0.0        14.0       9.0    85.0     23.0   \n",
       "green         2.0       0.0  128.0         7.0       0.0     3.0      0.0   \n",
       "industrial   31.0      14.0    7.0     80608.0     185.0  1743.0    159.0   \n",
       "inspired      1.0       9.0    0.0       185.0     385.0    17.0      3.0   \n",
       "market       19.0      85.0    3.0      1743.0      17.0  3805.0     56.0   \n",
       "project       8.0      23.0    0.0       159.0       3.0    56.0    469.0   \n",
       "renown        3.0       0.0    0.0         1.0       0.0     1.0      1.0   \n",
       "\n",
       "            renown  \n",
       "civic          3.0  \n",
       "domestic       0.0  \n",
       "green          0.0  \n",
       "industrial     1.0  \n",
       "inspired       0.0  \n",
       "market         1.0  \n",
       "project        1.0  \n",
       "renown        22.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>***Co-Occurance matrix Normalised with Jaccard similarity***</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The Jaccard similarity index:(Sometimes called the Jaccard similarity coefficient) compares members for two sets to see which members are shared and which are distinct. It’s a measure of similarity for the two sets of data, with a range from 0% to 100%. The higher the percentage, the more similar the two populations. Although it’s easy to interpret, it is extremely sensitive to small samples sizes and may give erroneous results, especially with very small samples or data sets with missing observations.<br/>J(X,Y) = |X∩Y| / |X∪Y|"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>civic</th>\n",
       "      <th>domestic</th>\n",
       "      <th>green</th>\n",
       "      <th>industrial</th>\n",
       "      <th>inspired</th>\n",
       "      <th>market</th>\n",
       "      <th>project</th>\n",
       "      <th>renown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>civic</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>domestic</th>\n",
       "      <td>0.01</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>green</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>industrial</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inspired</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>market</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>project</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>renown</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            civic  domestic  green  industrial  inspired  market  project  \\\n",
       "civic        1.00      0.01    0.0        0.00      0.00    0.00     0.01   \n",
       "domestic     0.01      1.00    0.0        0.00      0.01    0.02     0.03   \n",
       "green        0.00      0.00    1.0        0.00      0.00    0.00     0.00   \n",
       "industrial   0.00      0.00    0.0        1.00      0.00    0.02     0.00   \n",
       "inspired     0.00      0.01    0.0        0.00      1.00    0.00     0.00   \n",
       "market       0.00      0.02    0.0        0.02      0.00    1.00     0.01   \n",
       "project      0.01      0.03    0.0        0.00      0.00    0.01     1.00   \n",
       "renown       0.01      0.00    0.0        0.00      0.00    0.00     0.00   \n",
       "\n",
       "            renown  \n",
       "civic         0.01  \n",
       "domestic      0.00  \n",
       "green         0.00  \n",
       "industrial    0.00  \n",
       "inspired      0.00  \n",
       "market        0.00  \n",
       "project       0.00  \n",
       "renown        1.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>***Co-occurance heat map normalised using Jaccard similarity***</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<font color = green><h4><left>Probabilistic count v's Classification count</left></h4></font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Calibrated confidence",
         "textposition": "auto",
         "type": "bar",
         "x": [
          "civic",
          "domestic",
          "green",
          "industrial",
          "inspired",
          "market",
          "project",
          "renown"
         ],
         "y": [
          10320,
          13866,
          8279,
          75002,
          6564,
          13118,
          10104,
          10348
         ]
        },
        {
         "name": "Actual count",
         "textposition": "auto",
         "type": "bar",
         "x": [
          "civic",
          "domestic",
          "green",
          "industrial",
          "inspired",
          "market",
          "project",
          "renown"
         ],
         "y": [
          "not applicable",
          "not applicable",
          "not applicable",
          "not applicable",
          "not applicable",
          "not applicable",
          "not applicable",
          "not applicable"
         ]
        },
        {
         "name": "Predicted by classifier",
         "textposition": "auto",
         "type": "bar",
         "x": [
          "civic",
          "domestic",
          "green",
          "industrial",
          "inspired",
          "market",
          "project",
          "renown"
         ],
         "y": [
          397,
          237,
          128,
          80608,
          385,
          3805,
          469,
          22
         ]
        }
       ],
       "layout": {
        "barmode": "group",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Probabilistic count vs classifier predicted"
        },
        "xaxis": {
         "tickangle": -45
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"2601fc79-4c7e-4190-ac51-01bfb9e4c7e8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"2601fc79-4c7e-4190-ac51-01bfb9e4c7e8\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '2601fc79-4c7e-4190-ac51-01bfb9e4c7e8',\n",
       "                        [{\"name\": \"Calibrated confidence\", \"textposition\": \"auto\", \"type\": \"bar\", \"x\": [\"civic\", \"domestic\", \"green\", \"industrial\", \"inspired\", \"market\", \"project\", \"renown\"], \"y\": [10320, 13866, 8279, 75002, 6564, 13118, 10104, 10348]}, {\"name\": \"Actual count\", \"textposition\": \"auto\", \"type\": \"bar\", \"x\": [\"civic\", \"domestic\", \"green\", \"industrial\", \"inspired\", \"market\", \"project\", \"renown\"], \"y\": [\"not applicable\", \"not applicable\", \"not applicable\", \"not applicable\", \"not applicable\", \"not applicable\", \"not applicable\", \"not applicable\"]}, {\"name\": \"Predicted by classifier\", \"textposition\": \"auto\", \"type\": \"bar\", \"x\": [\"civic\", \"domestic\", \"green\", \"industrial\", \"inspired\", \"market\", \"project\", \"renown\"], \"y\": [397, 237, 128, 80608, 385, 3805, 469, 22]}],\n",
       "                        {\"barmode\": \"group\", \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Probabilistic count vs classifier predicted\"}, \"xaxis\": {\"tickangle\": -45}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('2601fc79-4c7e-4190-ac51-01bfb9e4c7e8');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "colorscale": [
          [
           0,
           "#00083e"
          ],
          [
           0.5,
           "#ededee"
          ],
          [
           1,
           "#ffffff"
          ]
         ],
         "hoverinfo": "none",
         "opacity": 0.75,
         "showscale": false,
         "type": "heatmap",
         "z": [
          [
           0,
           0,
           0,
           0,
           0
          ],
          [
           0.5,
           0.5,
           0.5,
           0.5,
           0.5
          ],
          [
           1,
           1,
           1,
           1,
           1
          ],
          [
           0.5,
           0.5,
           0.5,
           0.5,
           0.5
          ],
          [
           1,
           1,
           1,
           1,
           1
          ],
          [
           0.5,
           0.5,
           0.5,
           0.5,
           0.5
          ],
          [
           1,
           1,
           1,
           1,
           1
          ],
          [
           0.5,
           0.5,
           0.5,
           0.5,
           0.5
          ],
          [
           1,
           1,
           1,
           1,
           1
          ]
         ]
        }
       ],
       "layout": {
        "annotations": [
         {
          "align": "left",
          "font": {
           "color": "#ffffff"
          },
          "showarrow": false,
          "text": "<b>Convention</b>",
          "x": -0.45,
          "xanchor": "left",
          "xref": "x",
          "y": 0,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#ffffff"
          },
          "showarrow": false,
          "text": "<b>Calibrated Prob count</b>",
          "x": 0.55,
          "xanchor": "left",
          "xref": "x",
          "y": 0,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#ffffff"
          },
          "showarrow": false,
          "text": "<b>Classifier count</b>",
          "x": 1.55,
          "xanchor": "left",
          "xref": "x",
          "y": 0,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#ffffff"
          },
          "showarrow": false,
          "text": "<b>Threshold</b>",
          "x": 2.55,
          "xanchor": "left",
          "xref": "x",
          "y": 0,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#ffffff"
          },
          "showarrow": false,
          "text": "<b>True count</b>",
          "x": 3.55,
          "xanchor": "left",
          "xref": "x",
          "y": 0,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "civic",
          "x": -0.45,
          "xanchor": "left",
          "xref": "x",
          "y": 1,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "10320",
          "x": 0.55,
          "xanchor": "left",
          "xref": "x",
          "y": 1,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "397",
          "x": 1.55,
          "xanchor": "left",
          "xref": "x",
          "y": 1,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "0.5",
          "x": 2.55,
          "xanchor": "left",
          "xref": "x",
          "y": 1,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "not applicable",
          "x": 3.55,
          "xanchor": "left",
          "xref": "x",
          "y": 1,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "domestic",
          "x": -0.45,
          "xanchor": "left",
          "xref": "x",
          "y": 2,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "13866",
          "x": 0.55,
          "xanchor": "left",
          "xref": "x",
          "y": 2,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "237",
          "x": 1.55,
          "xanchor": "left",
          "xref": "x",
          "y": 2,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "0.5",
          "x": 2.55,
          "xanchor": "left",
          "xref": "x",
          "y": 2,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "not applicable",
          "x": 3.55,
          "xanchor": "left",
          "xref": "x",
          "y": 2,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "green",
          "x": -0.45,
          "xanchor": "left",
          "xref": "x",
          "y": 3,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "8279",
          "x": 0.55,
          "xanchor": "left",
          "xref": "x",
          "y": 3,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "128",
          "x": 1.55,
          "xanchor": "left",
          "xref": "x",
          "y": 3,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "0.5",
          "x": 2.55,
          "xanchor": "left",
          "xref": "x",
          "y": 3,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "not applicable",
          "x": 3.55,
          "xanchor": "left",
          "xref": "x",
          "y": 3,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "industrial",
          "x": -0.45,
          "xanchor": "left",
          "xref": "x",
          "y": 4,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "75002",
          "x": 0.55,
          "xanchor": "left",
          "xref": "x",
          "y": 4,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "80608",
          "x": 1.55,
          "xanchor": "left",
          "xref": "x",
          "y": 4,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "0.5",
          "x": 2.55,
          "xanchor": "left",
          "xref": "x",
          "y": 4,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "not applicable",
          "x": 3.55,
          "xanchor": "left",
          "xref": "x",
          "y": 4,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "inspired",
          "x": -0.45,
          "xanchor": "left",
          "xref": "x",
          "y": 5,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "6564",
          "x": 0.55,
          "xanchor": "left",
          "xref": "x",
          "y": 5,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "385",
          "x": 1.55,
          "xanchor": "left",
          "xref": "x",
          "y": 5,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "0.5",
          "x": 2.55,
          "xanchor": "left",
          "xref": "x",
          "y": 5,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "not applicable",
          "x": 3.55,
          "xanchor": "left",
          "xref": "x",
          "y": 5,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "market",
          "x": -0.45,
          "xanchor": "left",
          "xref": "x",
          "y": 6,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "13118",
          "x": 0.55,
          "xanchor": "left",
          "xref": "x",
          "y": 6,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "3805",
          "x": 1.55,
          "xanchor": "left",
          "xref": "x",
          "y": 6,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "0.5",
          "x": 2.55,
          "xanchor": "left",
          "xref": "x",
          "y": 6,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "not applicable",
          "x": 3.55,
          "xanchor": "left",
          "xref": "x",
          "y": 6,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "project",
          "x": -0.45,
          "xanchor": "left",
          "xref": "x",
          "y": 7,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "10104",
          "x": 0.55,
          "xanchor": "left",
          "xref": "x",
          "y": 7,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "469",
          "x": 1.55,
          "xanchor": "left",
          "xref": "x",
          "y": 7,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "0.5",
          "x": 2.55,
          "xanchor": "left",
          "xref": "x",
          "y": 7,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "not applicable",
          "x": 3.55,
          "xanchor": "left",
          "xref": "x",
          "y": 7,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "renown",
          "x": -0.45,
          "xanchor": "left",
          "xref": "x",
          "y": 8,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "10348",
          "x": 0.55,
          "xanchor": "left",
          "xref": "x",
          "y": 8,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "22",
          "x": 1.55,
          "xanchor": "left",
          "xref": "x",
          "y": 8,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "0.5",
          "x": 2.55,
          "xanchor": "left",
          "xref": "x",
          "y": 8,
          "yref": "y"
         },
         {
          "align": "left",
          "font": {
           "color": "#000000"
          },
          "showarrow": false,
          "text": "not applicable",
          "x": 3.55,
          "xanchor": "left",
          "xref": "x",
          "y": 8,
          "yref": "y"
         }
        ],
        "height": 320,
        "margin": {
         "b": 0,
         "l": 0,
         "r": 0,
         "t": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "dtick": 1,
         "gridwidth": 2,
         "showticklabels": false,
         "tick0": -0.5,
         "ticks": "",
         "zeroline": false
        },
        "yaxis": {
         "autorange": "reversed",
         "dtick": 1,
         "gridwidth": 2,
         "showticklabels": false,
         "tick0": 0.5,
         "ticks": "",
         "zeroline": false
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"1f862b0a-74b6-4ebd-9f59-e7325252dc19\" class=\"plotly-graph-div\" style=\"height:320px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    \n",
       "                if (document.getElementById(\"1f862b0a-74b6-4ebd-9f59-e7325252dc19\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '1f862b0a-74b6-4ebd-9f59-e7325252dc19',\n",
       "                        [{\"colorscale\": [[0, \"#00083e\"], [0.5, \"#ededee\"], [1, \"#ffffff\"]], \"hoverinfo\": \"none\", \"opacity\": 0.75, \"showscale\": false, \"type\": \"heatmap\", \"z\": [[0, 0, 0, 0, 0], [0.5, 0.5, 0.5, 0.5, 0.5], [1, 1, 1, 1, 1], [0.5, 0.5, 0.5, 0.5, 0.5], [1, 1, 1, 1, 1], [0.5, 0.5, 0.5, 0.5, 0.5], [1, 1, 1, 1, 1], [0.5, 0.5, 0.5, 0.5, 0.5], [1, 1, 1, 1, 1]]}],\n",
       "                        {\"annotations\": [{\"align\": \"left\", \"font\": {\"color\": \"#ffffff\"}, \"showarrow\": false, \"text\": \"<b>Convention</b>\", \"x\": -0.45, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 0, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#ffffff\"}, \"showarrow\": false, \"text\": \"<b>Calibrated Prob count</b>\", \"x\": 0.55, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 0, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#ffffff\"}, \"showarrow\": false, \"text\": \"<b>Classifier count</b>\", \"x\": 1.55, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 0, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#ffffff\"}, \"showarrow\": false, \"text\": \"<b>Threshold</b>\", \"x\": 2.55, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 0, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#ffffff\"}, \"showarrow\": false, \"text\": \"<b>True count</b>\", \"x\": 3.55, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 0, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"civic\", \"x\": -0.45, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 1, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"10320\", \"x\": 0.55, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 1, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"397\", \"x\": 1.55, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 1, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"0.5\", \"x\": 2.55, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 1, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"not applicable\", \"x\": 3.55, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 1, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"domestic\", \"x\": -0.45, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 2, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"13866\", \"x\": 0.55, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 2, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"237\", \"x\": 1.55, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 2, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"0.5\", \"x\": 2.55, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 2, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"not applicable\", \"x\": 3.55, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 2, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"green\", \"x\": -0.45, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 3, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"8279\", \"x\": 0.55, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 3, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"128\", \"x\": 1.55, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 3, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"0.5\", \"x\": 2.55, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 3, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"not applicable\", \"x\": 3.55, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 3, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"industrial\", \"x\": -0.45, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 4, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"75002\", \"x\": 0.55, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 4, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"80608\", \"x\": 1.55, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 4, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"0.5\", \"x\": 2.55, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 4, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"not applicable\", \"x\": 3.55, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 4, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"inspired\", \"x\": -0.45, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 5, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"6564\", \"x\": 0.55, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 5, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"385\", \"x\": 1.55, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 5, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"0.5\", \"x\": 2.55, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 5, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"not applicable\", \"x\": 3.55, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 5, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"market\", \"x\": -0.45, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 6, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"13118\", \"x\": 0.55, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 6, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"3805\", \"x\": 1.55, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 6, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"0.5\", \"x\": 2.55, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 6, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"not applicable\", \"x\": 3.55, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 6, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"project\", \"x\": -0.45, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 7, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"10104\", \"x\": 0.55, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 7, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"469\", \"x\": 1.55, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 7, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"0.5\", \"x\": 2.55, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 7, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"not applicable\", \"x\": 3.55, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 7, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"renown\", \"x\": -0.45, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 8, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"10348\", \"x\": 0.55, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 8, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"22\", \"x\": 1.55, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 8, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"0.5\", \"x\": 2.55, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 8, \"yref\": \"y\"}, {\"align\": \"left\", \"font\": {\"color\": \"#000000\"}, \"showarrow\": false, \"text\": \"not applicable\", \"x\": 3.55, \"xanchor\": \"left\", \"xref\": \"x\", \"y\": 8, \"yref\": \"y\"}], \"height\": 320, \"margin\": {\"b\": 0, \"l\": 0, \"r\": 0, \"t\": 0}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"dtick\": 1, \"gridwidth\": 2, \"showticklabels\": false, \"tick0\": -0.5, \"ticks\": \"\", \"zeroline\": false}, \"yaxis\": {\"autorange\": \"reversed\", \"dtick\": 1, \"gridwidth\": 2, \"showticklabels\": false, \"tick0\": 0.5, \"ticks\": \"\", \"zeroline\": false}},\n",
       "                        {\"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('1f862b0a-74b6-4ebd-9f59-e7325252dc19');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp = pd.DataFrame()\n",
    "ai_repo_analysis_df = analyze_git_repo(AI_GITHUB_GIT, _thresholds, _tokenizer['tokenizer'], _DLModels, _DLModelsIsotonicRegression, model_helpers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# group by repo-id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T02:24:52.940595Z",
     "start_time": "2019-12-04T02:24:50.584841Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_id</th>\n",
       "      <th>text</th>\n",
       "      <th>repo</th>\n",
       "      <th>civic</th>\n",
       "      <th>civic_prob</th>\n",
       "      <th>civic_y_pred</th>\n",
       "      <th>civic_prob_1</th>\n",
       "      <th>domestic</th>\n",
       "      <th>domestic_prob</th>\n",
       "      <th>domestic_y_pred</th>\n",
       "      <th>...</th>\n",
       "      <th>project_prob</th>\n",
       "      <th>project_y_pred</th>\n",
       "      <th>project_prob_1</th>\n",
       "      <th>renown</th>\n",
       "      <th>renown_prob</th>\n",
       "      <th>renown_y_pred</th>\n",
       "      <th>renown_prob_1</th>\n",
       "      <th>lbl_cnt</th>\n",
       "      <th>set_conf</th>\n",
       "      <th>pos_sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45717250</td>\n",
       "      <td>Tensorflow is an end-to-end open source platfo...</td>\n",
       "      <td>tensorflow/tensorflow</td>\n",
       "      <td>0</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>0</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45717250</td>\n",
       "      <td>It has a comprehensive, flexible ecosystem of ...</td>\n",
       "      <td>tensorflow/tensorflow</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45717250</td>\n",
       "      <td>Tensorflow was originally developed by researc...</td>\n",
       "      <td>tensorflow/tensorflow</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45717250</td>\n",
       "      <td>The system is general enough to be applicable ...</td>\n",
       "      <td>tensorflow/tensorflow</td>\n",
       "      <td>0</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>0</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0</td>\n",
       "      <td>0.051724</td>\n",
       "      <td>0</td>\n",
       "      <td>0.957447</td>\n",
       "      <td>0</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    repo_id                                               text  \\\n",
       "0  45717250  Tensorflow is an end-to-end open source platfo...   \n",
       "1  45717250  It has a comprehensive, flexible ecosystem of ...   \n",
       "2  45717250  Tensorflow was originally developed by researc...   \n",
       "3  45717250  The system is general enough to be applicable ...   \n",
       "\n",
       "                    repo  civic  civic_prob  civic_y_pred  civic_prob_1  \\\n",
       "0  tensorflow/tensorflow      0    0.953488             0      0.046512   \n",
       "1  tensorflow/tensorflow      0    1.000000             0      0.000000   \n",
       "2  tensorflow/tensorflow      0    1.000000             0      0.000000   \n",
       "3  tensorflow/tensorflow      0    0.953488             0      0.046512   \n",
       "\n",
       "   domestic  domestic_prob  domestic_y_pred  ...  project_prob  \\\n",
       "0         0       1.000000                0  ...      0.833333   \n",
       "1         0       1.000000                0  ...      1.000000   \n",
       "2         0       1.000000                0  ...      0.882353   \n",
       "3         0       0.964286                0  ...      0.948276   \n",
       "\n",
       "   project_y_pred  project_prob_1  renown  renown_prob  renown_y_pred  \\\n",
       "0               0        0.166667       0     0.944444              0   \n",
       "1               0        0.000000       0     1.000000              0   \n",
       "2               0        0.117647       0     0.980392              0   \n",
       "3               0        0.051724       0     0.957447              0   \n",
       "\n",
       "   renown_prob_1  lbl_cnt  set_conf  pos_sample  \n",
       "0       0.055556        1       0.0         0.0  \n",
       "1       0.000000        0       0.0         0.0  \n",
       "2       0.019608        0       0.0         0.0  \n",
       "3       0.042553        1       0.0         0.0  \n",
       "\n",
       "[4 rows x 38 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "132223"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "civic_y_pred\n",
      "domestic_y_pred\n",
      "green_y_pred\n",
      "industrial_y_pred\n",
      "inspired_y_pred\n",
      "market_y_pred\n",
      "project_y_pred\n",
      "renown_y_pred\n",
      "done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "132223"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_id</th>\n",
       "      <th>text</th>\n",
       "      <th>repo</th>\n",
       "      <th>civic</th>\n",
       "      <th>civic_prob</th>\n",
       "      <th>civic_y_pred</th>\n",
       "      <th>civic_prob_1</th>\n",
       "      <th>domestic</th>\n",
       "      <th>domestic_prob</th>\n",
       "      <th>domestic_y_pred</th>\n",
       "      <th>...</th>\n",
       "      <th>project_prob</th>\n",
       "      <th>project_y_pred</th>\n",
       "      <th>project_prob_1</th>\n",
       "      <th>renown</th>\n",
       "      <th>renown_prob</th>\n",
       "      <th>renown_y_pred</th>\n",
       "      <th>renown_prob_1</th>\n",
       "      <th>lbl_cnt</th>\n",
       "      <th>set_conf</th>\n",
       "      <th>pos_sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45717250</td>\n",
       "      <td>Tensorflow is an end-to-end open source platfo...</td>\n",
       "      <td>tensorflow/tensorflow</td>\n",
       "      <td>0</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>0</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45717250</td>\n",
       "      <td>It has a comprehensive, flexible ecosystem of ...</td>\n",
       "      <td>tensorflow/tensorflow</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45717250</td>\n",
       "      <td>Tensorflow was originally developed by researc...</td>\n",
       "      <td>tensorflow/tensorflow</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45717250</td>\n",
       "      <td>The system is general enough to be applicable ...</td>\n",
       "      <td>tensorflow/tensorflow</td>\n",
       "      <td>0</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>0</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0</td>\n",
       "      <td>0.051724</td>\n",
       "      <td>0</td>\n",
       "      <td>0.957447</td>\n",
       "      <td>0</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    repo_id                                               text  \\\n",
       "0  45717250  Tensorflow is an end-to-end open source platfo...   \n",
       "1  45717250  It has a comprehensive, flexible ecosystem of ...   \n",
       "2  45717250  Tensorflow was originally developed by researc...   \n",
       "3  45717250  The system is general enough to be applicable ...   \n",
       "\n",
       "                    repo  civic  civic_prob  civic_y_pred  civic_prob_1  \\\n",
       "0  tensorflow/tensorflow      0    0.953488             0      0.046512   \n",
       "1  tensorflow/tensorflow      0    1.000000             0      0.000000   \n",
       "2  tensorflow/tensorflow      0    1.000000             0      0.000000   \n",
       "3  tensorflow/tensorflow      0    0.953488             0      0.046512   \n",
       "\n",
       "   domestic  domestic_prob  domestic_y_pred  ...  project_prob  \\\n",
       "0         0       1.000000                0  ...      0.833333   \n",
       "1         0       1.000000                0  ...      1.000000   \n",
       "2         0       1.000000                0  ...      0.882353   \n",
       "3         0       0.964286                0  ...      0.948276   \n",
       "\n",
       "   project_y_pred  project_prob_1  renown  renown_prob  renown_y_pred  \\\n",
       "0               0        0.166667       0     0.944444              0   \n",
       "1               0        0.000000       0     1.000000              0   \n",
       "2               0        0.117647       0     0.980392              0   \n",
       "3               0        0.051724       0     0.957447              0   \n",
       "\n",
       "   renown_prob_1  lbl_cnt  set_conf  pos_sample  \n",
       "0       0.055556        1       0.0         0.0  \n",
       "1       0.000000        0       0.0         0.0  \n",
       "2       0.019608        0       0.0         0.0  \n",
       "3       0.042553        1       0.0         0.0  \n",
       "\n",
       "[4 rows x 38 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Tensorflow is an end-to-end open source platform for machine learning.[0,0,0,1,0,0,0,0,].'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display (ai_repo_analysis_df.head(4))\n",
    "display(ai_repo_analysis_df.shape[0])\n",
    "\n",
    "sum_list = []\n",
    "for conv in _thresholds:\n",
    "    sum_list.append(conv+'_y_pred')\n",
    "ai_repo_analysis_df['text'] = ai_repo_analysis_df['text'] + '['\n",
    "for conv_pred in sum_list:\n",
    "    print(conv_pred)\n",
    "    ai_repo_analysis_df['text'] = ai_repo_analysis_df[\"text\"].map(str) +  ai_repo_analysis_df[conv_pred].map(str) +','\n",
    "ai_repo_analysis_df['text'] = ai_repo_analysis_df['text'] + \"].\"\n",
    "print (\"done\") \n",
    "\n",
    "\n",
    "display(ai_repo_analysis_df.shape[0])\n",
    "display(ai_repo_analysis_df.head(4))\n",
    "display(ai_repo_analysis_df.iloc[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T02:25:17.818402Z",
     "start_time": "2019-12-04T02:24:54.794398Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of repos is 8457\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_id</th>\n",
       "      <th>repo</th>\n",
       "      <th>civic_y_pred</th>\n",
       "      <th>domestic_y_pred</th>\n",
       "      <th>green_y_pred</th>\n",
       "      <th>industrial_y_pred</th>\n",
       "      <th>inspired_y_pred</th>\n",
       "      <th>market_y_pred</th>\n",
       "      <th>project_y_pred</th>\n",
       "      <th>renown_y_pred</th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102637</td>\n",
       "      <td>cirg-up/cilib</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Cilib computational intelligence library is a ...</td>\n",
       "      <td>169</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>135191</td>\n",
       "      <td>igrigorik/decisiontree</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Decision tree a ruby library which implements ...</td>\n",
       "      <td>186</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   repo_id                    repo  civic_y_pred  domestic_y_pred  \\\n",
       "0   102637           cirg-up/cilib             0                0   \n",
       "1   135191  igrigorik/decisiontree             0                0   \n",
       "\n",
       "   green_y_pred  industrial_y_pred  inspired_y_pred  market_y_pred  \\\n",
       "0             0                  2                0              0   \n",
       "1             0                  8                0              0   \n",
       "\n",
       "   project_y_pred  renown_y_pred  \\\n",
       "0               0              0   \n",
       "1               0              0   \n",
       "\n",
       "                                                text  word_count  \\\n",
       "0  Cilib computational intelligence library is a ...         169   \n",
       "1  Decision tree a ruby library which implements ...         186   \n",
       "\n",
       "   sentence_count  \n",
       "0               5  \n",
       "1              10  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sum_list = []\n",
    "for conv in _thresholds:\n",
    "    sum_list.append(conv+'_y_pred')\n",
    "sum_list.append(\"text\")\n",
    "grouped_ai_repo_analysis_df = ai_repo_analysis_df.groupby(['repo_id', 'repo'])\n",
    "grouped_ai_repo_analysis_df = grouped_ai_repo_analysis_df[sum_list].sum(axis=0)\n",
    "\n",
    "def applyFunc(s): \n",
    "    return len(s.split(' '))\n",
    "def numSen(s): \n",
    "    return len(s.split('].'))\n",
    "\n",
    "grouped_ai_repo_analysis_df['word_count'] = grouped_ai_repo_analysis_df['text'].apply(applyFunc)\n",
    "grouped_ai_repo_analysis_df['sentence_count'] = grouped_ai_repo_analysis_df['text'].apply(numSen)\n",
    "\n",
    "\n",
    "print (\"The number of repos is\", grouped_ai_repo_analysis_df.shape[0])\n",
    "grouped_ai_repo_analysis_df = grouped_ai_repo_analysis_df.reset_index()\n",
    "grouped_ai_repo_analysis_df['word_count'] = grouped_ai_repo_analysis_df['text'].apply(applyFunc)\n",
    "display(grouped_ai_repo_analysis_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T02:25:22.581128Z",
     "start_time": "2019-12-04T02:25:19.054463Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************EXAMPLE 2 ************************************\n",
      "Number of words: 186\n",
      "Number of sentences: 10\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 8\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/igrigorik/decisiontree/blob/master/README.md\n",
      "URL: https://github.com/igrigorik/decisiontree/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Decision tree a ruby library which implements id3 (information gain) algorithm for decision tree learning</b>[<font color=red>industrial</font>]<b>.</b> <b>Currently, continuous and discrete datasets can be learned</b>[<font color=red>industrial</font>]<b>.</b> Discrete model assumes unique labels can be graphed and converted into a png for visual analysis continuous looks at all possible values for a variable and iteratively chooses the best threshold between all possible assignments. <b>This results in a binary tree which is partitioned by the threshold at every step</b>[<font color=red>industrial</font>]<b>.</b> <b>(e.g temperate 20c) features id3 algorithms for continuous and discrete cases, with support for inconsistent datasets</b>[<font color=red>industrial</font>]<b>.</b> <b>Graphviz component to visualize the learned tree support for multiple, and symbolic outputs and graphing of continuous trees</b>[<font color=red>industrial</font>]<b>.</b> <b>Returns default value when no branches are suitable for input implementation ruleset is a class that trains an id3tree with 2 3 of the training data, converts it into set of rules and prunes the rules with the remaining 1 3 of the training data (in a c4.5 way)</b>[<font color=red>industrial</font>]<b>.</b> <b>Bagging is a bagging-based trainer (quite obvious), which trains 10 ruleset trainers and when predicting chooses the best output based on voting</b>[<font color=red>industrial</font>]<b>.</b> <b>Blog post with explanation examples example license the mit license copyright (c) 2006 ilya grigorik '</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 3 ************************************\n",
      "Number of words: 315\n",
      "Number of sentences: 18\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 9\n",
      "Total inspired: 0\n",
      "Total market: 2\n",
      "Total project: 1\n",
      "Total renown: 0\n",
      "URL: https://github.com/febeling/rb-libsvm/blob/master/README.md\n",
      "URL: https://github.com/febeling/rb-libsvm/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Rb-libsvm ruby language bindings for libsvm package provides ruby bindings to the libsvm library</b>[<font color=red>industrial</font>]<b>.</b> Svm is a machine learning and classification algorithm, and libsvm is a popular free implementation of it, written by chih-chung chang and chih-jen lin, of national taiwan university, taipei. See the book programming collective intelligence, among others, for a usage example. <b>There is a jruby implementation of this gem named jrb-libsvm by andreas eger there exist some other ruby bindings for libsvm</b>[<font color=red>industrial</font>]<b>.</b> <b>One is named ruby svmruby-svm, written by rudi cilibrasi</b>[<font color=red>industrial</font>]<b>.</b> The other, more actively developed one is libsvm-ruby-swigsvmrubyswig by tom zeng, which is built using swig. <b>Libsvm includes a number of command line tools for preprocessing training data and finding parameters</b>[<font color=red>industrial</font>]<b>.</b> <b>You should install the original package if you need them</b>[<font color=violet>market</font>]<b>.</b> <b>It is helpful to consult the readme of the libsvmreadme package for reference when configuring the training parameters</b>[<font color=red>industrial</font>]<b>.</b> Currently this package includes libsvm version 3.23. <b>Just install and go installation for building this gem from source on os x (which is the default packaging) you will need to have xcode installed, and from within xcode you need to install the command line tools</b>[<font color=red>industrial</font>]<b>.</b> <b>Those contain the compiler which is necessary for the native code, and similar tools</b>[<font color=red>industrial</font>]<b>.</b> <b>To install the gem run this command gem install rb-libsvm usage this is a short example of how to use the gem</b>[<font color=red>industrial</font>]<b>.</b> <b>If you want to rely on bundler for loading dependencies in a project, (i.e use `bundler.require` or use an environment that relies on it, like rails), then you will need to specify rb-libsvm in the gemfile like this: this is because the loadable name (`libsvm`) is different from the gem's name (`rb-libsvm`)</b>[<font color=red>industrial, </font> <font color=orange>project</font>]<b>.</b> Florian ebeling contributors rimas silkaitis aleksander pohl andreas eger license this software can be freely used under the terms of the mit license, see file mit-license. <b>This package includes the source of libsvm, which is free to use under the license in the file libsvm-license</b>[<font color=violet>market</font>]<b>.</b> Posts about using svms with ruby cjlin libsvm svmrubyswig:."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 4 ************************************\n",
      "Number of words: 573\n",
      "Number of sentences: 29\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 20\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/rieck/malheur/blob/master/README.md\n",
      "URL: https://github.com/rieck/malheur/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Malheur automatic analysis of malware behavior this software belongs to the publication konrad rieck, philipp trinius, carsten willems, and thorsten holz</b>[<font color=red>industrial</font>]<b>.</b> <b> automatic analysis of malware behavior using machine learning</b>[<font color=red>industrial</font>]<b>.</b> <b> journal of computer security (jcs), 19 (4), 63968, june 2011</b>[<font color=red>industrial</font>]<b>.</b> <b> preprint(doc 2011-jcs.pdf) introduction malheur is a tool for the automatic analysis of malware behavior (program behavior recorded from malicious software in a sandbox environment)</b>[<font color=red>industrial</font>]<b>.</b> <b>It has been designed to support the regular analysis of malicious software and the development of detection and defense measures</b>[<font color=red>industrial</font>]<b>.</b> <b>Malheur allows for identifying novel classes of malware with similar behavior and assigning unknown malware to discovered classes</b>[<font color=red>industrial</font>]<b>.</b> It supports four basic actions for analysis which can be applied to reports of recorded behavior: 1. <b>Extraction of prototypes: from a given set of reports, malheur identifies a subset of prototypes representative for the full data set</b>[<font color=red>industrial</font>]<b>.</b> <b>The prototypes provide a quick overview of recorded behavior and can be used to guide manual inspection</b>[<font color=red>industrial</font>]<b>.</b> Clustering of behavior malheur automatically identifies groups (clusters) of reports containing similar behavior. Clustering allows for discovering novel classes of malware and provides the basis for crafting specific detection and defense mechanisms, such as anti-virus signatures. Classification of behavior: based on a set of previously clustered reports, malheur is able to assign unknown behavior to known groups of malware. <b>Classification enables identifying novel and unknown variants of malware and can be used to filter program behavior prior to manual inspection</b>[<font color=red>industrial</font>]<b>.</b> <b>Incremental analysis: malheur can be applied incrementally for analysis of large data sets</b>[<font color=red>industrial</font>]<b>.</b> <b>By processing reports in chunks, the run-time as well as memory requirements can be significantly reduced</b>[<font color=red>industrial</font>]<b>.</b> <b>This renders long-term application of malheur feasible, for example for daily analysis of incoming malware programs</b>[<font color=red>industrial</font>]<b>.</b> A detailed description of these techniques as well as technical background on analysis of malicious software is provided in the following articles: + automatic analysis of malware behavior using machine learning. Konrad rieck, philipp trinius, carsten willems, and thorsten holz journal of computer security (jcs), 19 (4) 639-668, 2011. <b>+ a malware instruction set for behavior-based analysis</b>[<font color=red>industrial</font>]<b>.</b> <b>Philipp trinius, carsten willems, thorsten holz, and konrad rieck technical report tr-2009-07, university of mannheim, 2009 dependencies + libconfig 1.4, + libarchive 3.1.2, debian ubuntu linux the following packages need to be installed for compiling malheur on debian and ubuntu linux gcc libconfig9-dev libarchive-dev for bootstrapping malheur from the git repository or manipulating the automake autoconf configuration, the following additional packages are necessary</b>[<font color=red>industrial</font>]<b>.</b> <b>Automake autoconf libtool mac os x for compiling malheur on mac os x a working installation of xcode is required including `gcc`</b>[<font color=red>industrial</font>]<b>.</b> <b>Additionally, the following packages need to be installed via homebrew libconfig libarchive (from homebrew-alt) openbsd for compiling malheur on openbsd the following packages are required</b>[<font color=red>industrial</font>]<b>.</b> <b>Note that you need to use `gmake` instead of `make` for building malheur</b>[<font color=red>industrial</font>]<b>.</b> <b>Gmake libconfig libarchive for bootstrapping malheur from the git repository, the following packages need be additionally installed autoconf automake libtool compilation installation from git repository first run. bootstrap from tarball run. configure options make make check make install options for configure prefixpath set directory prefix for installation by default malheur is installed into usr local</b>[<font color=red>industrial</font>]<b>.</b> <b>If you prefer a different location, use this option to select an installation directory</b>[<font color=red>industrial</font>]<b>.</b> License this program is free software; you can redistribute it and or modify it under the terms of the gnu general public license as published by the free software foundation; either version 3 of the license, or (at your option) any later version. See the gnu general public license for more details. <b>Copyright copyright (c) 2009-2015 konrad rieck (konrad@mlsec.org) university of goettingen, berlin institute of technology '</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 5 ************************************\n",
      "Number of words: 315\n",
      "Number of sentences: 15\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 10\n",
      "Total inspired: 0\n",
      "Total market: 2\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/jblas-project/jblas/blob/master/README.md\n",
      "URL: https://github.com/jblas-project/jblas/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Jblas is a matrix library for java which uses existing high performance blas and lapack libraries like atlas</b>[<font color=red>industrial</font>]<b>.</b> <b>Version 1.2.4, may 11, 2015 version 1.2.3, february 13, 2013 version 1.2.2, december 17, 2012 version 1.2.1 version 1.2.0, january 7, 2011 version 1.1.1 version 1.1, august 16, 2010 version 1.0.2, february 26, 2010 version 1.0.1, january 14, 2010 version 1.0, december 22, 2009 version 0.3, september 17, 2009 version 0.2, may 8, 2009 version 0.1, march 28, 2009 see also the file release_notes homepage: status page: principle, all you need is the jblas-1.2.4.jar in your classpath</b>[<font color=red>industrial</font>]<b>.</b> <b>Jblas-1.2.4.jar will then automagically extract your platform dependent native library to a tempfile and load it from there</b>[<font color=red>industrial</font>]<b>.</b> <b>You can also put that file somewhere in your load path ( ld_library_path for linux, path for windows)</b>[<font color=red>industrial</font>]<b>.</b> <b>Or, use the following dependency in maven building if you only work on the java part, you can use maven to recompile from the sources</b>[<font color=violet>market</font>]<b>.</b> <b>In addition to that you need an installation of ruby for some scripts which automaticall generate code</b>[<font color=red>industrial</font>]<b>.</b> <b>Then, you just type mvn package on the command line</b>[<font color=red>industrial</font>]<b>.</b> If you want to build jblas from the sources including the native part, you need to set up quite a few things: you will need some implementation of blas and lapack. Jblas is tested with either plain lapack, or atlas you also need the fortran sources for blas and lapack, available, for example from you still want to build the source your own, see install for further details. <b>How to get started have a look at javadoc index.html and javadoc org jblas doublematrix.html if you want to validate your installation and get some performance numbers, try javajar jblas-1.2.4.jar</b>[<font color=red>industrial</font>]<b>.</b> <b>License jblas is distributed under a bsd-style license</b>[<font color=red>industrial</font>]<b>.</b> <b>Bugs if you encounter any bugs, feel free to go to and register a ticket for them</b>[<font color=violet>market</font>]<b>.</b> <b>Make sure to include as much information as possible</b>[<font color=red>industrial</font>]<b>.</b> <b>For configuration problems it would also be helpful to include the file configure.log</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 6 ************************************\n",
      "Number of words: 265\n",
      "Number of sentences: 15\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 10\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/VowpalWabbit/vowpal_wabbit/blob/master/README.md\n",
      "URL: https://github.com/VowpalWabbit/vowpal_wabbit/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Why vowpal wabbit? vowpal wabbit is a machine learning system which pushes the frontier of machine learning with techniques such as online, hashing, allreduce, reductions, learning2search, active, and interactive learning</b>[<font color=red>industrial</font>]<b>.</b> There is a specific focus on reinforcement learning with several contextual bandit algorithms implemented and the online nature lending to the problem well. <b>Vowpal wabbit is a destination for implementing and maturing state of the art algorithms with performance in mind</b>[<font color=red>industrial</font>]<b>.</b> <b>The input format for the learning algorithm is substantially more flexible than might be expected</b>[<font color=red>industrial</font>]<b>.</b> Examples can have features consisting of free form text, which is interpreted in a bag-of-words way. There can even be multiple sets of free form text in different namespaces. <b>The learning algorithm is fast similar to the few other online algorithm implementations out there</b>[<font color=red>industrial</font>]<b>.</b> <b>There are several optimization algorithms available with the baseline being sparse gradient descent (gd) on a loss function</b>[<font color=red>industrial</font>]<b>.</b> <b>Instead, the important characteristic here is that the memory footprint of the program is bounded independent of data</b>[<font color=red>industrial</font>]<b>.</b> <b>This means the training set is not loaded into main memory before learning starts</b>[<font color=red>industrial</font>]<b>.</b> <b>In addition, the size of the set of features is bounded independent of the amount of training data using the hashing trick</b>[<font color=red>industrial</font>]<b>.</b> <b>Subsets of features can be internally paired so that the algorithm is linear in the cross-product of the subsets</b>[<font color=red>industrial</font>]<b>.</b> The alternative of explicitly expanding the features before feeding them into the learning algorithm can be both computation and space intensive, depending on how it's handled. <b>Getting started for the most up to date instructions for getting started on windows, macos or linux please see the wiki this includes: installing with a package manager dependencies building tutorial</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 7 ************************************\n",
      "Number of words: 177\n",
      "Number of sentences: 11\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 5\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/nltk/nltk/blob/master/README.md\n",
      "URL: https://github.com/nltk/nltk/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>B natural language toolkit (nltk) the natural language toolkit is a suite of open source python modules, data sets, and tutorials supporting research and development in natural language processing</b>[<font color=red>industrial</font>]<b>.</b> For documentation, please visit nltk.org contributing do you want to contribute to nltk development? great please read more details at contributing.md(contributing.md). See also how to contribute to nltk donate have you found the toolkit helpful? please support nltk development by donating to the project via paypal, using the link on the nltk homepage. Citing if you publish work that uses nltk, please cite the nltk book, as follows: bird, steven, edward loper and ewan klein (2009). <b>Copyright copyright (c) 2001-2019 nltk project for license information, see license.txt(license.txt)</b>[<font color=red>industrial</font>]<b>.</b> Authors.md(authors.md) have a list of everyone contributed to nltk. <b>Redistributing nltk source code is distributed under the apache 2.0 license</b>[<font color=red>industrial</font>]<b>.</b> <b>Nltk documentation is distributed under the creative commons attribution-noncommercial-no derivative works 3.0 united states license</b>[<font color=red>industrial</font>]<b>.</b> <b>Nltk corpora are provided under the terms given in the readme file for each corpus; all are redistributable and available for non-commercial use</b>[<font color=red>industrial</font>]<b>.</b> Nltk may be freely redistributed, subject to the provisions of these licenses."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 8 ************************************\n",
      "Number of words: 154\n",
      "Number of sentences: 11\n",
      "Total civic: 1\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 8\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/nipy/dipy/blob/master/README.md\n",
      "URL: https://github.com/nipy/dipy/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b> doc _static dipy-logo.png 180px dipy diffusion imaging in python |</b>[<font color=red>industrial</font>]<b>.</b> <b> dipyref_ is a python library for analysis of mr diffusion imaging</b>[<font color=red>industrial</font>]<b>.</b> <b>Dipy is for research only; please do not use results from dipy for clinical decisions</b>[<font color=red>industrial</font>]<b>.</b> Website current information can always be found from the dipy website lists please see the developers' list at see the users' forum at join the gitter chatroom `here `. <b>Code you can find our sources and single-click downloads: `main repository` on github</b>[<font color=red>industrial</font>]<b>.</b> <b>Documentation_ for all releases and current development tree</b>[<font color=red>industrial</font>]<b>.</b> <b>_main repository: _documentation: _current trunk: dipy dipy can be installed using `pip`:: pip install dipy or using `conda`:: conda installc conda-forge dipy for detailed installation instructions, including instructions for installing from source, please read our `installation documentation `</b>[<font color=red>industrial</font>]<b>.</b> <b>License dipy is licensed under the terms of the bsd license</b>[<font color=red>industrial</font>]<b>.</b> <b>Contributing we welcome contributions from the community</b>[<font color=blue>civic</font>]<b>.</b> <b>Nimmo-smith and dipy contributors, dipy, a library for the analysis of diffusion mri data , frontiers in neuroinformatics, vol</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 9 ************************************\n",
      "Number of words: 71\n",
      "Number of sentences: 5\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 3\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/kawatan/Apricot/blob/master/README.md\n",
      "URL: https://github.com/kawatan/Apricot/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Screenshot screenshot ) apricot this repository contains apricot implementation for windows</b>[<font color=red>industrial</font>]<b>.</b> <b>Apricot is a desktop mascot who reads rss atom news feeds and add comments on the news one by one</b>[<font color=red>industrial</font>]<b>.</b> Inspired by fed news articles, a character speaks to you in a way that human beings never can. <b>Features user interface based on wpf generating messages based on machine leraning describing an character using flexible character markup language please see for more information</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 10 ************************************\n",
      "Number of words: 362\n",
      "Number of sentences: 21\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 19\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/proycon/pynlpl/blob/master/README.md\n",
      "URL: https://github.com/proycon/pynlpl/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>B pynlpl python natural language processing library</b>[<font color=red>industrial</font>]<b>.</b> <b> pronounced as 'pineapple', is a python library for natural language processing</b>[<font color=red>industrial</font>]<b>.</b> <b>It contains various modules useful for common, and less common, nlp tasks</b>[<font color=red>industrial</font>]<b>.</b> <b>Pynlpl can be used for basic tasks such as the extraction of n-grams and frequency lists, and to build simple language model</b>[<font color=red>industrial</font>]<b>.</b> <b>There are also more complex data types and algorithms</b>[<font color=red>industrial</font>]<b>.</b> <b>Moreover, there are parsers for file formats common in nlp (e.g folia giza moses arpa timbl cql)</b>[<font color=red>industrial</font>]<b>.</b> <b>There are also clients to interface with various nlp specific servers</b>[<font color=red>industrial</font>]<b>.</b> <b>Pynlpl most notably features a very extensive library for working with folia xml (format for linguistic annotatation)</b>[<font color=red>industrial</font>]<b>.</b> <b>The library is a divided into several packages and modules</b>[<font color=red>industrial</font>]<b>.</b> <b>The following modules are available: ``pynlpl.datatypes`` extra datatypes (priority queues, patterns, tries) ``pynlpl.evaluation`` evaluation experiment classes (parameter search, wrapped progressive sampling, class evaluation (precision recall f-score auc), sampler, confusion matrix, multithreaded experiment pool) ``pynlpl.formats.cgn`` module for parsing cgn (corpus gesproken nederlands) part-of-speech tags ``pynlpl.formats.folia`` extensive library for reading and manipulating the documents in `folia ` format (format for linguistic annotation)</b>[<font color=red>industrial</font>]<b>.</b> <b>``pynlpl.formats.fql`` extensive library for the folia query language (fql), built on top of ``pynlpl.formats.folia``</b>[<font color=red>industrial</font>]<b>.</b> <b>``pynlpl.formats.cql`` parser for the corpus query language (cql), as also used by corpus workbench and sketch engine</b>[<font color=red>industrial</font>]<b>.</b> <b>``pynlpl.formats.giza`` module for reading giza++ word alignment data ``pynlpl.formats.moses`` module for reading moses phrase-translation tables</b>[<font color=red>industrial</font>]<b>.</b> <b>``pynlpl.formats.sonar`` largely obsolete module for pre-releases of the sonar corpus, use ``pynlpl.formats.folia`` instead</b>[<font color=red>industrial</font>]<b>.</b> <b>``pynlpl.formats.timbl`` module for reading timbl output (consider using `python-timbl ` instead though) ``pynlpl.lm.lm`` module for simple language model and reader for arpa language model data as well (used by srilm)</b>[<font color=red>industrial</font>]<b>.</b> <b>``pynlpl.search`` various search algorithms (breadth-first, depth-first, beam-search, hill climbing, a star, various variants of each) ``pynlpl.statistics`` frequency lists, levenshtein, common statistics and information theory functions ``pynlpl.textprocessors`` simple tokeniser, n-gram extraction installation download and install the latest stable version directly from the python package index with ``pip install pynlpl`` (or ``pip3`` for python 3 on most systems)</b>[<font color=red>industrial</font>]<b>.</b> <b>Alternatively, clone this repository and run ``python setup.py install`` (or ``python3 setup.py install`` for python 3 on most system</b>[<font color=red>industrial</font>]<b>.</b> <b>This software may also be found in the certain linux distributions, such as the latest versions as debian ubuntu, as ``python-pynlpl`` and ``python3-pynlpl``</b>[<font color=red>industrial</font>]<b>.</b> Pynlpl is also included in our `lamachine ` distribution. <b>Documentation api documentation can be found `here `</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 11 ************************************\n",
      "Number of words: 507\n",
      "Number of sentences: 24\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 20\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/rieck/sally/blob/master/README.md\n",
      "URL: https://github.com/rieck/sally/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Sally(sally.png) sally a tool for embedding strings in vector spaces this software belongs to the publication konrad rieck, christian wressnegger, and alexander bikadorov</b>[<font color=red>industrial</font>]<b>.</b> <b> sally: a tool for embedding strings in vector spaces journal of machine learning research (jmlr), 13(nov):3247-3251, 2012</b>[<font color=red>industrial</font>]<b>.</b> <b> sally is a small tool for mapping a set of strings to a set of vectors</b>[<font color=red>industrial</font>]<b>.</b> <b>This mapping is referred to as embedding and allows for applying techniques of machine learning and data mining for analysis of string data</b>[<font color=red>industrial</font>]<b>.</b> <b>Sally can be applied to several types of strings, such as text documents, dna sequences or log files, where it can handle common formats such as directories, archives and text files of string data</b>[<font color=red>industrial</font>]<b>.</b> <b>Sally implements a standard technique for mapping strings to a vector space that can be referred to as generalized bag-of-words model</b>[<font color=red>industrial</font>]<b>.</b> The strings are characterized by a set of features, where each feature is associated with one dimension of the vector space. <b>The following types of features are supported by sally: bytes, tokens (words), n-grams of bytes and n-grams of tokens</b>[<font color=red>industrial</font>]<b>.</b> <b>Sally proceeds by counting the occurrences of the specified features in each string and generating a sparse vector of count values</b>[<font color=red>industrial</font>]<b>.</b> <b>Alternatively, binary or tf-idf values can be computed and stored in the vectors</b>[<font color=red>industrial</font>]<b>.</b> <b>Sally then normalizes the vector, for example using the l1 or l2 norm, and outputs it in a specified format, such as plain text or in libsvm or matlab format</b>[<font color=red>industrial</font>]<b>.</b> <b>Consult the manual page(doc sally.pdf) of sally for more information</b>[<font color=red>industrial</font>]<b>.</b> Dependencies + zlib 1.2.1, + libconfig 1.3.2, + libarchive 3.1.2, debian ubuntu linux the following packages need to be installed for compiling sally on debian and ubuntu linux gcc libz-dev libconfig8-dev libarchive-dev the following packages need to be installed for compiling sally on centos and rhel linux gcc zlib-devel libconfig-devel libarchive-devel for bootstrapping sally from the git repository or manipulating the automake autoconf configuration, the following additional packages are necessary. <b>Automake autoconf libtool mac os x for compiling sally on mac os x a working installation of xcode is required including `gcc`</b>[<font color=red>industrial</font>]<b>.</b> <b>Additionally, the following packages need to be installed via homebrew libconfig libarchive (from homebrew-alt) openbsd for compiling sally on openbsd the following packages are required</b>[<font color=red>industrial</font>]<b>.</b> <b>Note that you need to use `gmake` instead of `make` for building sally</b>[<font color=red>industrial</font>]<b>.</b> <b>Gmake libconfig libarchive for bootstrapping sally from the git repository, the following packages need be additionally installed autoconf automake libtool compilation installation from git repository, run:. bootstrap. configure options make make check make install from tarball, run:. configure options make make check make install options for configure this option sets the prefix of the installation path</b>[<font color=red>industrial</font>]<b>.</b> <b>Prefixpath set directory prefix for installation this option enables support for openmp in sally</b>[<font color=red>industrial</font>]<b>.</b> <b>Sally will execute certain parts of the processing in parallel making use of multi-core architectures where possible</b>[<font color=red>industrial</font>]<b>.</b> <b>Enable-md5hash enable md5 as alternative hash sally uses a hash function for mapping different features to different dimensions in the vector space</b>[<font color=red>industrial</font>]<b>.</b> <b>By default the very efficient murmur hash is used for this task</b>[<font color=red>industrial</font>]<b>.</b> <b>In certain critical cases it may be useful to use a cryptographic hash as md5</b>[<font color=red>industrial</font>]<b>.</b> Copyright (c) 2010-2015 konrad rieck (konrad@mlsec.org); \\t\\t\\tchristian wressnegger (christian@mlsec.org); \\t\\t\\talexander bikadorov (abiku@cs.tu-berlin.de) '."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 12 ************************************\n",
      "Number of words: 403\n",
      "Number of sentences: 23\n",
      "Total civic: 1\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 12\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/scikit-learn/scikit-learn/blob/master/README.md\n",
      "URL: https://github.com/scikit-learn/scikit-learn/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b> _doi: is a python module for machine learning built on top of scipy and is distributed under the 3-clause bsd license</b>[<font color=red>industrial</font>]<b>.</b> The project was started in 2007 by david cournapeau as a google summer of code project, and since then many volunteers have contributed. See the `about us ` page for a list of core contributors. It is currently maintained by a team of volunteers. <b>Website: scikit-learn requires: python ( 3.5) numpy ( 1.11.0) scipy ( 0.17.0) joblib ( 0.11) scikit-learn 0.20 was the last version to support python 2.7 and python 3.4</b>[<font color=red>industrial</font>]<b>.</b> <b>Scikit-learn 0.21 and later require python 3.5 or newer</b>[<font color=red>industrial</font>]<b>.</b> <b>Scikit-learn plotting capabilities (i.e, functions start with plot_ and classes end with display ) require matplotlib ( 1.5.1)</b>[<font color=red>industrial</font>]<b>.</b> <b>For running the examples matplotlib 1.5.1 is required</b>[<font color=red>industrial</font>]<b>.</b> <b>A few examples require scikit-image 0.12.3, a few examples require pandas 0.18.0</b>[<font color=red>industrial</font>]<b>.</b> <b>User installation if you already have a working installation of numpy and scipy, the easiest way to install scikit-learn is using ``pip`` :: pip installu scikit-learn or ``conda``:: conda install scikit-learn the documentation includes more detailed `installation instructions `</b>[<font color=red>industrial</font>]<b>.</b> Changelog see the `changelog ` for a history of notable changes to scikit-learn. Development we welcome new contributors of all experience levels. <b>The scikit-learn community goals are to be helpful, welcoming, and effective</b>[<font color=blue>civic</font>]<b>.</b> <b>The `development guide ` has detailed information about contributing code, documentation, tests, and more</b>[<font color=red>industrial</font>]<b>.</b> <b>We've included some basic information in this readme</b>[<font color=red>industrial</font>]<b>.</b> Important links official source code repo: download releases: issue tracker: code you can check the latest sources with the command:: git clone to learn more about making a contribution to scikit-learn, please see our `contributing guide `. <b>Testing after installation, you can launch the test suite from outside the source directory (you will need to have ``pytest`` 3.3.0 installed):: pytest sklearn see the web page testing for more information</b>[<font color=red>industrial</font>]<b>.</b> Random number generation can be controlled during testing by setting the ``sklearn_seed`` environment variable. Submitting a pull request before opening a pull request, have a look at the full contributing page to make sure your code complies with our guidelines: history the project was started in 2007 by david cournapeau as a google summer of code project, and since then many volunteers have contributed. The project is currently maintained by a team of volunteers. <b>Note: `scikit-learn` was previously referred to as `scikits.learn`</b>[<font color=red>industrial</font>]<b>.</b> <b>Help and support documentation html documentation (stable release): html documentation (development version): faq: mailing list: irc channel: `` scikit-learn`` at ``webchat.freenode.net`` stack overflow: website: if you use scikit-learn in a scientific publication, we would appreciate citations: citing-scikit-learn '</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 13 ************************************\n",
      "Number of words: 284\n",
      "Number of sentences: 12\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 10\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/ramhiser/datamicroarray/blob/master/README.md\n",
      "URL: https://github.com/ramhiser/datamicroarray/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>B datamicroarray the r package `datamicroarray` provides a collection of scripts to download, process, and load small-sample, high-dimensional microarray data sets to assess machine learning algorithms and models</b>[<font color=red>industrial</font>]<b>.</b> <b>For each data set, we include a small set of scripts that automatically download, clean, and save the data set</b>[<font color=red>industrial</font>]<b>.</b> <b>Data sets each data set is listed below by the first author on the original paper</b>[<font color=red>industrial</font>]<b>.</b> The data sets are organized them by category; note that most of the data sets are cancer-related. <b>Click a data set to see its description, a link to the original paper, and additional information</b>[<font color=red>industrial</font>]<b>.</b> <b>Breast cancer chin (2006) chowdary (2006) gravier (2010) sorlie (2001) west (2001) central nervous system disorders pomeroy (2002) crohn's disease burczynski (2006) colon cancer alon (1999) glioma sun (2006) huntington's disease borovecki (2005) leukemia chiaretti (2004) golub (1999) yeoh (2002) lung cancer gordon (2002) lymphoma shipp (2002) myeloma tian (2003) prostate cancer singh (2002) sarcoma nakayama (2007) small round blue cell tumors khan (2001) miscellaneous christensen (2009) su (2002) subramanian (2005) installation you can install the latest package version by typing the following at the r console: note that you need to install the `devtools` package beforehand</b>[<font color=red>industrial</font>]<b>.</b> <b>Usage once you have installed and loaded the `datamicroarray` package, you can load a data set with the `data` command</b>[<font color=red>industrial</font>]<b>.</b> <b>(1999) colon cancer data set type the following at the r console: after loading the data set, the resulting object is a named `list` with two elements: 1</b>[<font color=red>industrial</font>]<b>.</b> <b>The rows are the `n` observations, and the columns are the `p` features</b>[<font color=red>industrial</font>]<b>.</b> <b>`y` a factor vector of length `n` with the corresponding class labels</b>[<font color=red>industrial</font>]<b>.</b> <b>(1999) colon cancer data set you can see all of the data sets available along with a brief summary of each with the `describe_data` helper function</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 14 ************************************\n",
      "Number of words: 106\n",
      "Number of sentences: 5\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 4\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/ramhiser/sparsediscrim/blob/master/README.md\n",
      "URL: https://github.com/ramhiser/sparsediscrim/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>B sparsediscrim r package `sparsediscrim` provides a collection of sparse and regularized discriminant analysis classifiers that are especially useful for when applied to small-sample, high-dimensional data sets</b>[<font color=red>industrial</font>]<b>.</b> <b>Installation you can install the stable version on cran if you prefer to download the latest version, instead type: classifiers the `sparsediscrim` package features the following classifier (the r function is included within parentheses): high-dimensional regularized discriminant analysis (`hdrda`) from ramey et al</b>[<font color=red>industrial</font>]<b>.</b> <b>(2015) the `sparsediscrim` package also includes a variety of additional classifiers intended for small-sample, high-dimensional data sets</b>[<font color=red>industrial</font>]<b>.</b> <b>We also include modifications to linear discriminant analysis (lda) with regularized covariance-matrix estimators: moore-penrose pseudo-inverse (`lda_pseudo`) schafer-strimmer estimator (`lda_schafer`) thomaz-kitani-gillies estimator (`lda_thomaz`)</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 15 ************************************\n",
      "Number of words: 157\n",
      "Number of sentences: 9\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 6\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/bytefish/opencv/blob/master/README.md\n",
      "URL: https://github.com/bytefish/opencv/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>B bytefish opencv this repository contains opencv code and documents</b>[<font color=red>industrial</font>]<b>.</b> <b>More (maybe) here: colormaps an implementation of various colormaps for opencv2 c++ in order to enhance visualizations</b>[<font color=red>industrial</font>]<b>.</b> <b>Related posts misc sample code that doesn't belong to a specific project</b>[<font color=red>industrial</font>]<b>.</b> Skin color detection pca tantriggs preprocessing machinelearning document and sourcecode about opencv c++ machine learning api including: support vector machines multi layer perceptron normal bayes k-nearest-neighbor decision tree related posts eigenfaces eigenfaces implementation using the opencv2 c++ api. <b>There's a very basic function for loading the dataset, you probably want to make this a bit more sophisticated</b>[<font color=red>industrial</font>]<b>.</b> The dataset is available at related posts lbp implements various local binary patterns with the opencv2 c++ api: original lbp circular lbp (also known as extended lbp) variance-based lbp basic code for spatial histograms and histogram matching with a chi-square distance is included, but it's not finished right now. <b>There's a tiny demo application you can experiment with</b>[<font color=red>industrial</font>]<b>.</b> <b>Related posts lda fisherfaces implementation with the opencv2 c++ api</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 16 ************************************\n",
      "Number of words: 125\n",
      "Number of sentences: 7\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 6\n",
      "Total inspired: 0\n",
      "Total market: 1\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/lutzroeder/netron/blob/master/README.md\n",
      "URL: https://github.com/lutzroeder/netron/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>B netron is a viewer for neural network, deep learning and machine learning models</b>[<font color=red>industrial</font>]<b>.</b> <b>Netron supports onnx (`.onnx`, `.pb`, `.pbtxt`), keras (`.h5`, `.keras`), core ml (`.mlmodel`), caffe (`.caffemodel`, `.prototxt`), caffe2 (`predict_net.pb`, `predict_net.pbtxt`), mxnet (`.model`, `-symbol.json`), torchscript (`.pt`, `.pth`), ncnn (`.param`) and tensorflow lite (`.tflite`)</b>[<font color=red>industrial</font>]<b>.</b> <b>Netron has experimental support for pytorch (`.pt`, `.pth`), torch (`.t7`), cntk (`.model`, `.cntk`), deeplearning4j (`.zip`), paddlepaddle (`.zip`, `model`), darknet (`.cfg`), scikit-learn (`.pkl`), tensorflow.js (`model.json`, `.pb`) and tensorflow (`.pb`, `.meta`, `.pbtxt`)</b>[<font color=red>industrial</font>]<b>.</b> <b>Install macos: download the `.dmg` file or run `brew cask install netron` linux: download the `.appimage` or `.deb` file</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> <b>Python server: run `pip install netron` and `netron file` or `import netron; netron.start('file')`</b>[<font color=red>industrial</font>]<b>.</b> <b>Download models sample model files to download and open: onnx: resnet-18 keras: tiny-yolo-voc coreml: faces_model tensorflow lite: smartreply mxnet: inception_v1 caffe: mobilenet_v2 tensorflow: inception_v3</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 17 ************************************\n",
      "Number of words: 158\n",
      "Number of sentences: 9\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 6\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/nilearn/nilearn/blob/master/README.md\n",
      "URL: https://github.com/nilearn/nilearn/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b> is a python module for fast and easy statistical learning on neuroimaging data</b>[<font color=red>industrial</font>]<b>.</b> <b>It leverages the `scikit-learn ` python toolbox for multivariate statistics with applications such as predictive modelling, classification, decoding, or connectivity analysis</b>[<font color=red>industrial</font>]<b>.</b> This work is made available by a community of people, amongst which the inria parietal project team and the scikit-learn folks, in particular p. Important links official source code repo: html documentation (stable release): required dependencies to use the software are: python 3.5, setuptools numpy 1.11 scipy 0.19 scikit-learn 0.19 joblib 0.11 nibabel 2.0.2 if you are using nilearn plotting functionalities or running the examples, matplotlib 1.5.1 is required. <b>If you want to run the tests, you need nose 1.2.1 and coverage 3.6</b>[<font color=red>industrial</font>]<b>.</b> <b>Install first make sure you have installed all the dependencies listed above</b>[<font color=red>industrial</font>]<b>.</b> <b>Then you can install nilearn by running the following command in a command prompt:: pip installu user nilearn more detailed instructions are available at installation</b>[<font color=red>industrial</font>]<b>.</b> <b>Development detailed instructions on how to contribute are available at</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 18 ************************************\n",
      "Number of words: 460\n",
      "Number of sentences: 19\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 12\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/mne-tools/mne-python/blob/master/README.md\n",
      "URL: https://github.com/mne-tools/mne-python/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b> _mne: `mne-python software` is an open-source python package for exploring, visualizing, and analyzing human neurophysiological data such as meg, eeg, seeg, ecog, and more</b>[<font color=red>industrial</font>]<b>.</b> <b>It includes modules for data input output, preprocessing, visualization, source estimation, time-frequency analysis, connectivity analysis, machine learning, and statistics</b>[<font color=red>industrial</font>]<b>.</b> <b>Documentation `mne documentation` for mne-python is available online</b>[<font color=red>industrial</font>]<b>.</b> <b>Installing mne-python to install the latest stable version of mne-python, you can use pip_ in a terminal:</b>[<font color=red>industrial</font>]<b>.</b> <b> bash pip installu mne note that mne-python 0.17 will be the last release to support python 2</b>[<font color=red>industrial</font>]<b>.</b> <b>From mne-python 0.18, only python 3 will be supported</b>[<font color=red>industrial</font>]<b>.</b> <b>For more complete instructions and more advanced installation methods (e.g for the latest development version), see the `getting started page`</b>[<font color=red>industrial</font>]<b>.</b> <b>Get the latest code to install the latest version of the code using pip_ open a terminal and type:</b>[<font color=red>industrial</font>]<b>.</b> <b> bash pip installu get the latest code using `git `, open a terminal and type:</b>[<font color=red>industrial</font>]<b>.</b> <b> bash git clone git: github.com mne-tools mne-python.git alternatively, you can also download a `zip file of the latest development version `</b>[<font color=red>industrial</font>]<b>.</b> <b>Dependencies the minimum required dependencies to run mne-python are: python 3.5 numpy 1.12.1 scipy 0.18.1 for full functionality, some functions require: matplotlib 2.0.2 mayavi 4.6 pysurfer 0.8 scikit-learn 0.18.2 numba 0.40 nibabel 2.1.0 pandas 0.19.2 picard 0.3 cupy 4.0 (for nvidia cuda acceleration) dipy 0.10.1 pyvista 0.20.1 contributing to mne-python please see the documentation on the mne-python homepage: list is bsd-licenced (3 clause): this software is osi certified open source software</b>[<font color=red>industrial</font>]<b>.</b> Osi certified is a certification mark of the open source initiative. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and or other materials provided with the distribution. Neither the names of mne-python authors nor the names of any contributors may be used to endorse or promote products derived from this software without specific prior written permission. This software is provided by the copyright holders and contributors as is and any express or implied warranties, including, but not limited to, the implied warranties of merchantability and fitness for a particular purpose are disclaimed. In no event shall the copyright owner or contributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages (including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits; or business interruption) however caused and on any theory of liability, whether in contract, strict liability, or tort (including negligence or otherwise) arising in any way out of the use of this software, even if advised of the possibility of such damage. <b>_mne-python software: _mne documentation: _getting started page: _pip:</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 19 ************************************\n",
      "Number of words: 655\n",
      "Number of sentences: 34\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 26\n",
      "Total inspired: 0\n",
      "Total market: 1\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/RaRe-Technologies/gensim/blob/master/README.md\n",
      "URL: https://github.com/RaRe-Technologies/gensim/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>B gensim topic modelling in python forum gensim) is a python library for topic modelling, document indexing and similarity retrieval with large corpora</b>[<font color=red>industrial</font>]<b>.</b> <b>Target audience is the natural language processing (nlp) and information retrieval (ir) community</b>[<font color=red>industrial</font>]<b>.</b> <b>Features all algorithms are memory-independent w.r.t</b>[<font color=red>industrial</font>]<b>.</b> <b>The corpus size (can process input larger than ram, streamed, out-of-core), intuitive interfaces easy to plug in your own input corpus datastream (trivial streaming api) easy to extend with other vector space algorithms (trivial transformation api) efficient multicore implementations of popular algorithms, such as online latent semantic analysis (lsa lsi svd), latent dirichlet allocation (lda), random projections (rp), hierarchical dirichlet process (hdp) or word2vec deep learning</b>[<font color=red>industrial</font>]<b>.</b> <b>Distributed computing: can run latent semantic analysis and latent dirichlet allocation on a cluster of computers</b>[<font color=red>industrial</font>]<b>.</b> <b>Extensive documentation and jupyter notebook tutorials</b>[<font color=red>industrial</font>]<b>.</b> <b>If this feature list left you scratching your head, you can first read more about the vector space model and unsupervised document analysis on wikipedia</b>[<font color=red>industrial</font>]<b>.</b> Support ask open-ended or research questions on the gensim mailing list forum gensim). Raise bugs on github but make sure you follow the issue template issues that are not bugs or fail to follow the issue template will be closed without inspection. <b>Installation this software depends on numpy and scipy, two python packages for scientific computing</b>[<font color=red>industrial</font>]<b>.</b> <b>You must have them installed prior to installing gensim</b>[<font color=red>industrial</font>]<b>.</b> <b>It is also recommended you install a fast blas library before installing numpy</b>[<font color=red>industrial</font>]<b>.</b> <b>This is optional, but using an optimized blas such as atlas or openblas is known to improve performance by as much as an order of magnitude</b>[<font color=red>industrial</font>]<b>.</b> <b>On os x, numpy picks up the blas that comes with it automatically, so you dont need to do anything special</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> <b>The simple way to install gensim is: pip installu gensim or, if you have instead downloaded and unzipped the source tar.gz package, youd run: python setup.py test python setup.py install for alternative modes of installation (without root privileges, development installation, optional install features), see the documentation</b>[<font color=red>industrial</font>]<b>.</b> <b>This version has been tested under python 2.7, 3.5 and 3.6</b>[<font color=red>industrial</font>]<b>.</b> <b>Gensims github repo is hooked against travis ci for automated testing on every commit push and pull request</b>[<font color=red>industrial</font>]<b>.</b> <b>Support for python 2.6, 3.3 and 3.4 was dropped in gensim 1.0.0</b>[<font color=red>industrial</font>]<b>.</b> <b>Install gensim 0.13.4 if you must use python 2.6, 3.3 or 3.4</b>[<font color=red>industrial</font>]<b>.</b> <b>Support for python 2.5 was dropped in gensim 0.10.0; install gensim 0.9.1 if you must use python 2.5)</b>[<font color=red>industrial</font>]<b>.</b> How come gensim is so fast and memory efficient? isnt it pure python, and isnt python slow and greedy? many scientific algorithms can be expressed in terms of large matrix operations (see the blas note above). Gensim taps into these low-level blas libraries, by means of its dependency on numpy. <b>So while gensim-the-top-level-code is pure python, it actually executes highly optimized fortran c under the hood, including multithreading (if your blas is so configured)</b>[<font color=red>industrial</font>]<b>.</b> <b>Memory-wise, gensim makes heavy use of pythons built-in generators and iterators for streamed data processing</b>[<font color=red>industrial</font>]<b>.</b> Memory efficiency was one of gensims design goals, and is a central feature of gensim, rather than something bolted on as an afterthought. <b>Documentation quickstart tutorials tutorial videos official api documentation quickstart: tutorials: tutorials tutorial videos: videos official documentation and walkthrough: official api documentation: adopters</b>[<font color=red>industrial</font>]<b>.</b> <b>Talent-pair(docs src readme_images talent-pair.png)</b>[<font color=red>industrial</font>]<b>.</b> <b>Post interesting and relevant content to pinterest</b>[<font color=red>industrial</font>]<b>.</b> <b>Search-metrics(docs src readme_images search-metrics.png)</b>[<font color=red>industrial</font>]<b>.</b> <b>Gensim word2vec used for entity disambiguation in search engine optimisation</b>[<font color=red>industrial</font>]<b>.</b> Document comprehension and association with word2vec. <b>Topic modeling for customer complaints exploration</b>[<font color=red>industrial</font>]<b>.</b> | citing gensim when citing gensim in academic papers and theses, please use this bibtex entry: @inproceedingsrehurek_lrec, title software framework for topic modelling with large corpora , author radim \\v r eh\\r u \\v r ek and petr sojka , booktitle proceedings of the lrec 2010 workshop on new challenges for nlp frameworks , pages 4550 , year 2010, month may, day 22, publisher elra , address valletta, malta , note\\url , languageenglish citing gensim in academic papers and theses: travis ci for automated testing: design goals: rare technologies: rare\\_tech: rare-technologies.com talentpair: citing gensim in academic papers and theses: documentation and jupyter notebook tutorials: documentation vector space model: unsupervised document analysis: numpy and scipy: atlas: openblas: source tar.gz: documentation:."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 20 ************************************\n",
      "Number of words: 226\n",
      "Number of sentences: 14\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 8\n",
      "Total inspired: 1\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/telmomenezes/synthetic/blob/master/README.md\n",
      "URL: https://github.com/telmomenezes/synthetic/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Synthetic symbolic generators for complex networks _note: if you are looking for the original java version of synthetic, you can find it here:</b>[<font color=red>industrial</font>]<b>.</b> This python library is under active development, please excuse the current lack of documentation. <b>We will make it available soon._ synthetic is a machine learning tool that can be used to discover plausible generators for complex networks</b>[<font color=red>industrial</font>]<b>.</b> Generators are simple computer programs that control the growth of a network from the bottom-up, in a similar fashion to the processes believed to underlie the emergence of many different types of networks, be them biological, social or technological. <b>Generators are useful as network growth models, both for their potentially explanatory and predictive powers</b>[<font color=red>industrial</font>]<b>.</b> <b>In a way, this tool automates the scientific method</b>[<font color=red>industrial</font>]<b>.</b> It creates and refines hypothesis, and tests them against real data, in a process that leads to increasingly plausible models. <b>Programs are represented in a very simple language that is suitable both for humans and the machine learning process</b>[<font color=red>industrial</font>]<b>.</b> <b>The machine learning algorithm used is genetic programming, belonging to the family of evolutionary algorithms, which are inspired by darwinian evolution</b>[<font color=red>industrial, </font> <font color=red>inspired</font>]<b>.</b> Programs are subject to random variations, much like the genetic material in biological entities. <b>The programs with the highest quality survive, leading to an ongoing refinement of the growth model</b>[<font color=red>industrial</font>]<b>.</b> For a more complete explanation see this article: related publications menezes, t. <b>And roth, c., symbolic regression of generative network models, scientific reports 4 (2014)</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 21 ************************************\n",
      "Number of words: 240\n",
      "Number of sentences: 16\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 14\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/shogun-toolbox/shogun/blob/master/README.md\n",
      "URL: https://github.com/shogun-toolbox/shogun/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>The shogun machine learning toolbox unified and efficient machine learning since 1999</b>[<font color=red>industrial</font>]<b>.</b> <b>Latest release: shogun: branch build status: to shogun via numfocus: see doc readme about.md(doc readme about.md) for a project description</b>[<font color=red>industrial</font>]<b>.</b> <b>See doc readme install.md(doc readme install.md) for installation instructions</b>[<font color=red>industrial</font>]<b>.</b> <b>See doc readme interfaces.md(doc readme interfaces.md) for calling shogun from its interfaces</b>[<font color=red>industrial</font>]<b>.</b> <b>See doc readme examples.md(doc readme examples.md) for details on creating api examples</b>[<font color=red>industrial</font>]<b>.</b> <b>See doc readme developing.md(doc readme developing.md) for how to hack shogun</b>[<font color=red>industrial</font>]<b>.</b> <b>See api examples for api examples for all interfaces</b>[<font color=red>industrial</font>]<b>.</b> <b>Interfaces shogun is implemented in c++ and offers automatically generated, unified interfaces to python, octave, java scala, ruby, c , r, lua</b>[<font color=red>industrial</font>]<b>.</b> <b>We are currently working on adding more languages including javascript, d, and matlab</b>[<font color=red>industrial</font>]<b>.</b> <b>Alpha (many examples work, string typemaps are unstable, overloaded methods unavailable)</b>[<font color=red>industrial</font>]<b>.</b> <b>Pre-alpha (work in progress quality) | see our website for examples in all languages</b>[<font color=red>industrial</font>]<b>.</b> <b>Platforms shogun is supported under gnu linux, macosx, freebsd, and windows</b>[<font color=red>industrial</font>]<b>.</b> See our buildfarm directory contents the following directories are found in the source distribution. <b>Note that some folders are submodules that can be checked out with `git submodule update init`</b>[<font color=red>industrial</font>]<b>.</b> <b>Src source code, separated into c++ source and interfaces doc readmes (doc reamde, submodule), ipython notebooks, cookbook (api examples), licenses examples example files for all interfaces data data sets (submodule, required for examples) tests unit tests and continuous integration of interface examples applications applications of shogun (outdated) benchmarks speed benchmarks cmake cmake build scripts license shogun is distributed under bsd 3-clause license(doc license license.md), with optional gpl3 components</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 22 ************************************\n",
      "Number of words: 110\n",
      "Number of sentences: 6\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 3\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/kevinjalbert/kmean_clustering/blob/master/README.md\n",
      "URL: https://github.com/kevinjalbert/kmean_clustering/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " Kmean clustering information author: kevin jalbert (blitzbolt@gmail.com) copyright: copyright (c) 2011 kevin jalbert license: mit license the program solves the kmean clustering problem, by grouping data points into _k_ clusters. <b>More details about the program are found in the wiki wiki )</b>[<font color=red>industrial</font>]<b>.</b> <b>Pre-requirements this program takes advantage of the following python 2.7 python ) libraries: numpy numpy ) matplotlib matplotlib ) _the python command is linked to the 2.7 version of python_ execution 1</b>[<font color=red>industrial</font>]<b>.</b> <b>Download the source code and place it into a directory of choice</b>[<font color=red>industrial</font>]<b>.</b> Execute using the following command options to see a list of the options run the following command all the options are detailed in the options options ) page."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 23 ************************************\n",
      "Number of words: 550\n",
      "Number of sentences: 25\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 14\n",
      "Total inspired: 0\n",
      "Total market: 1\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/JuliaLang/julia/blob/master/README.md\n",
      "URL: https://github.com/JuliaLang/julia/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Build status: code coverage: travis-img: the julia language julia is a high-level, high-performance dynamic language for technical computing</b>[<font color=red>industrial</font>]<b>.</b> <b>The main homepage for julia can be found at julialang.org this is the github repository of julia source code, including instructions for compiling and installing julia, below</b>[<font color=red>industrial</font>]<b>.</b> Resources homepage: binaries: source code: documentation: packages: discussion forum: slack: (get an invite from ) youtube: code coverage: new developers may find the notes in contributing to start contributing to the julia codebase. External resources stackoverflow twitter meetup binary installation if you would rather not compile the latest julia from source, platform-specific tarballs with pre-compiled binaries are also available for download the downloads page also provides details on the different tiers of support support-tiers) for os and platform combinations. If everything works correctly, you will see a julia banner and an interactive prompt into which you can enter expressions for evaluation. Note: although some system package managers provide julia, such installations are neither maintained nor endorsed by the julia project. <b>We recommend you use the official julia binaries instead</b>[<font color=red>industrial</font>]<b>.</b> <b>Building julia first, make sure you have all the required dependencies required-build-tools-and-external-libraries) installed</b>[<font color=red>industrial</font>]<b>.</b> <b>Then, acquire the source code by cloning the git repository: git clone git: github.com julialang julia.git by default you will be building the latest unstable version of julia</b>[<font color=red>industrial</font>]<b>.</b> <b>However, most users should use the most recent stable version of julia</b>[<font color=red>industrial</font>]<b>.</b> <b>You can get this version by changing to the julia directory and running: git checkout v1.2.0 now run `make` to build the `julia` executable</b>[<font color=red>industrial</font>]<b>.</b> <b>Building julia requires 2gib of disk space and approximately 4gib of virtual memory</b>[<font color=red>industrial</font>]<b>.</b> <b>Note: the build process will fail badly if any of the build directory's parent directories have spaces or other shell meta-characters such as ` ` or `:` in their names (this is due to a limitation in gnu make)</b>[<font color=violet>market</font>]<b>.</b> <b>Once it is built, you can run the `julia` executable after you enter your julia directory and run. julia your first test of julia determines whether your build is working properly</b>[<font color=red>industrial</font>]<b>.</b> <b>From the unix windows command prompt inside the `julia` source directory, type `make testall`</b>[<font color=red>industrial</font>]<b>.</b> You should see output that lists a series of running tests; if they complete without error, you should be in good shape to start using julia. In case this default build path did not work, detailed build instructions are included in the build documentation uninstalling julia julia does not install anything outside the directory it was cloned into. Julia can be completely uninstalled by deleting this directory. <b>Julia packages are installed in ` .julia` by default, and can be uninstalled by deleting ` .julia`</b>[<font color=red>industrial</font>]<b>.</b> Source code organization the julia source code is organized as follows: base source code for the base module (part of julia's standard library) stdlib source code for other standard library packages contrib editor support for julia source, miscellaneous scripts deps external dependencies doc src manual source for the user manual doc build detailed notes for building julia src source for julia language core test test suites ui source for various front ends usr binaries and shared libraries loaded by julia's standard libraries terminal, editors and ides the julia repl is quite powerful. See the section in the manual on the julia repl more details. <b>Support for editing julia is available for many widely used editors text and many others</b>[<font color=red>industrial</font>]<b>.</b> <b>Supported ides include: juno (atom plugin), julia-vscode (vs code plugin), and julia-intellij idea plugin)</b>[<font color=red>industrial</font>]<b>.</b> <b>The popular jupyter interface is available through ijulia</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 24 ************************************\n",
      "Number of words: 505\n",
      "Number of sentences: 19\n",
      "Total civic: 1\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 7\n",
      "Total inspired: 0\n",
      "Total market: 2\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/clips/pattern/blob/master/README.md\n",
      "URL: https://github.com/clips/pattern/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>It has tools for: data mining: web services (google, twitter, wikipedia), web crawler, html dom parser natural language processing: part-of-speech taggers, n-gram search, sentiment analysis, wordnet machine learning: vector space model, clustering, classification (knn, svm, perceptron) network analysis: graph centrality and visualization</b>[<font color=red>industrial</font>]<b>.</b> <b>It is well documented, thoroughly tested with 350+ unit tests and comes bundled with 50+ examples</b>[<font color=red>industrial</font>]<b>.</b> <b>The source code is licensed under bsd and available from</b>[<font color=red>industrial</font>]<b>.</b> <b>Example workflow example trains a classifier on adjectives mined from twitter using python 3</b>[<font color=red>industrial</font>]<b>.</b> First, tweets that contain hashtag win or fail are collected. For example: 20 tip off a sweet little old lady today win. The word part-of-speech tags are then parsed, keeping only adjectives. Each tweet is transformed to a vector, a dictionary of adjective count items, labeled `win` or `fail`. <b>The classifier uses the vectors to learn which other tweets look more like `win` or more like `fail`</b>[<font color=red>industrial</font>]<b>.</b> <b>Installation pattern supports python 2.7 and python 3.6</b>[<font color=red>industrial</font>]<b>.</b> To install pattern so that it is available in all your scripts, unzip the download and from the command line do: if you have pip, you can automatically download and install from the pypi repository if none of the above works, you can make python aware of the module in three ways: put the pattern folder in the same folder as your script. <b>Put the pattern folder in the standard location for modules so it is available to all scripts: `c:\\python36\\lib\\site-packages\\` (windows), ` library python 3.6 site-packages ` (mac os x), ` usr lib python3.6 site-packages ` (unix)</b>[<font color=red>industrial</font>]<b>.</b> <b>Add the location of the module to `sys.path` in your script, before importing it: documentation for documentation and examples see the user documentation if you are a developer, go check out the developer documentation see `license.txt` for further details</b>[<font color=violet>market</font>]<b>.</b> <b>Contribute the source code is hosted on github and contributions or donations are welcomed</b>[<font color=blue>civic</font>]<b>.</b> <b>Please have look at the developer documentation if you use pattern in your work, please cite our reference paper</b>[<font color=violet>market</font>]<b>.</b> Bundled dependencies pattern is bundled with the following data sets, algorithms and python packages: brill tagger, eric brill brill tagger for dutch, jeroen geertzen brill tagger for german, gerold schneider martin volk brill tagger for spanish, trained on wikicorpus (samuel reese gemma boleda et al.) brill tagger for french, trained on lefff (benot sagot lionel clment et al.) brill tagger for italian, mined from wiktionary english pluralization, damian conway spanish verb inflection, fred jehle french verb inflection, bob salita graph javascript framework, aslak hellesoy dave hoover libsvm, chih-chung chang chih-jen lin liblinear, rong-en fan et al. Networkx centrality, aric hagberg, dan schult pieter swart spelling corrector, peter norvig acknowledgements authors: tom de smedt (tom@organisms.be) walter daelemans (walter.daelemans@ua.ac.be) contributors (chronological): frederik de bleser jason wiener daniel friesen jeroen geertzen thomas crombez ken williams peteris erins rajesh nair f. De smedt radim ehek tom loredo john debovis thomas sileo gerold schneider martin volk samuel joseph shubhanshu mishra robert elwell fred jehle antoine mazires + fabelier.org rmi de zoeten + closealert.nl kenneth koch jens grivolla fabio marfia steven loria colin molter + tevizz.com peter bull maurizio sambati dan fu salvatore di dio vincent van asch frederik elwert '."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 25 ************************************\n",
      "Number of words: 99\n",
      "Number of sentences: 3\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 1\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/wellflat/imageprocessing-labs/blob/master/README.md\n",
      "URL: https://github.com/wellflat/imageprocessing-labs/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " Image processing and machine learning labs ensp; vision, image processing and machine learning on the web browser or node note fast fourier transform (1d 2d-fft) stereo matching poisson image editing line segment detector corner detection fish-eye transform image processing filters image histogram calculation image feature extraction decision tree learning k-means++ clustering logistic regression adaptive regularization of weight vectors (arow) soft confidence weighted learning (scw) gradient boosting decision tree (gbdt) neural network (denoising autoencoders) 3d shape drawing (mobius strip, klein bottle, heart surface.) webgl samples etc. <b>Demo demo site (mirror) license copyright copy; 2017 wellflat licensed under the mit licensemit mit:</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 26 ************************************\n",
      "Number of words: 574\n",
      "Number of sentences: 35\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 30\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/sqrlab/MutationScorePredictor/blob/master/README.md\n",
      "URL: https://github.com/sqrlab/MutationScorePredictor/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " Information author: kevin jalbert (kevin.j.jalbert@gmail.com) copyright: copyright (c) 2011 kevin jalbert license: mit license introduction the mutation score predictor is a technique that allows one to predict the mutation score of methods and classes within a java project. <b>The benefit of predicting the mutation score is that one does not need to actually execute the mutation testing process</b>[<font color=red>industrial</font>]<b>.</b> <b>Mutation testing is a costly procedure as it requires multiple executions of the test suite</b>[<font color=red>industrial</font>]<b>.</b> By predicting with a reasonable level of accuracy the mutation score of methods and classes mutation scores can be acquire with relatively low resource consumption. <b>Background some quick background material on mutation testing, support vector machines and software metrics are supplied in the following sections</b>[<font color=red>industrial</font>]<b>.</b> <b>Mutation testing mutation testing is a technique that evaluates the coverage achieved by a unit test suite for a project</b>[<font color=red>industrial</font>]<b>.</b> <b>The objective of mutation testing is to identify weak areas in the unit test suite, this is accomplished by seeding faults into the project</b>[<font color=red>industrial</font>]<b>.</b> A mutant is a copy of the project except for a newly introduce fault, as a result of a single change. <b>If a mutant is detected by the test suite then is was killed, otherwise it was undetected</b>[<font color=red>industrial</font>]<b>.</b> <b>A mutation score is given at the ending of the mutation testing process that quantifies the test suites ability to kill mutants (given as a percent of killed mutants)</b>[<font color=red>industrial</font>]<b>.</b> <b>Support vector machine support vector machine is a machine learning technique that falls under the category of supervised learning algorithms</b>[<font color=red>industrial</font>]<b>.</b> <b>This technique is capable of learning on a set of features dictate the category of a data item, essentially it is a classifier</b>[<font color=red>industrial</font>]<b>.</b> <b>Supervised learning algorithms will first be trained on known data (features and categories are known), and then are used on the test data (only features known)</b>[<font color=red>industrial</font>]<b>.</b> <b>The classifier will attempt to correctly classify the unknown data given a model that was created during the training phase</b>[<font color=red>industrial</font>]<b>.</b> <b>The selected support vector machine tool is libsvm9</b>[<font color=red>industrial</font>]<b>.</b> <b>Software metrics software metrics are measurements of software artifact attributes</b>[<font color=red>industrial</font>]<b>.</b> <b>These attributes commonly characterize structural properties such as size and complexity</b>[<font color=red>industrial</font>]<b>.</b> <b>Our approach uses source code and test suite metrics as these two software artifacts are components of the mutation testing process</b>[<font color=red>industrial</font>]<b>.</b> <b>We gather a set of source code metrics for both the system under test and the test suite</b>[<font color=red>industrial</font>]<b>.</b> <b>We further collect the test suite's coverage over the system under test</b>[<font color=red>industrial</font>]<b>.</b> <b>The selected source code metric tool is the eclipse metrics plugin5</b>[<font color=red>industrial</font>]<b>.</b> <b>The selected test suite coverage metric tool is emma10</b>[<font color=red>industrial</font>]<b>.</b> <b>Instructions this project requires multiple languages and tools to function as intended</b>[<font color=red>industrial</font>]<b>.</b> <b>The list of requirements are displayed below, as well as the method to execute this project as intended</b>[<font color=red>industrial</font>]<b>.</b> <b>To aid the user in executing the method, a _rakefile_ exists with the project to automate some of the steps</b>[<font color=red>industrial</font>]<b>.</b> <b>Requirements the following list enumerates the requirements for this project</b>[<font color=red>industrial</font>]<b>.</b> <b>The project was developed on linux and uses some linux-only features</b>[<font color=red>industrial</font>]<b>.</b> <b>Maven7 (optional, only if working with a project that uses maven) method 1</b>[<font color=red>industrial</font>]<b>.</b> Import project into eclipse (consider looking at selecting a project12) 2. <b>Enable the metrics reporting (within the project's properties in eclipse) 3</b>[<font color=red>industrial</font>]<b>.</b> <b>Run the rake task 'install' to install the necessary components 4</b>[<font color=red>industrial</font>]<b>.</b> <b>Run the rake task 'setup_svm' to build up the support vector machine 5</b>[<font color=red>industrial</font>]<b>.</b> <b>Run the rake task 'cross_validation' to test the cross validation accuracy usage see the usage11 page in the wiki for detailed explaination of the the various commands provided in the mutation_score_predictor</b>[<font color=red>industrial</font>]<b>.</b> <b>1: ruby 2: python 3: java 4: eclipse 5: eclipse metrics plugin 6: ant 7: maven 8: schuler javalanche javalanche 9: cjlin libsvm libsvm 10: emma 11: 12:</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 27 ************************************\n",
      "Number of words: 714\n",
      "Number of sentences: 22\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 15\n",
      "Total inspired: 0\n",
      "Total market: 3\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/inukshuk/anystyle/blob/master/README.md\n",
      "URL: https://github.com/inukshuk/anystyle/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Anystyle is a very fast and smart parser for academic references</b>[<font color=red>industrial</font>]<b>.</b> <b>It was originally inspired by parscit freecite anystyle uses machine learning algorithms and aims to make it easy to train the model with data that is relevant to your parsing needs</b>[<font color=red>industrial</font>]<b>.</b> <b>Using anystyle cli sudo gem install anystyle-cli anystyle help anystyle help find anystyle help parse see anystyle-cli for more details</b>[<font color=red>industrial</font>]<b>.</b> <b>Sudo gem install anystyle once installed, you can use the static parser and finder instances by calling the `anystyle.parse` or `anystyle.find` methods</b>[<font color=red>industrial</font>]<b>.</b> <b>For example: alternatively, you can create your own `anystyle::parser` or `anystyle::finder` with custom options</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> Web application and web service anystyle is available as a web-application and service at anystyle.io note that the web service is currently based on the legacy 0.x branch can train custom finder and parser models. <b>To do this, you need to prepare your own data sets for training</b>[<font color=red>industrial</font>]<b>.</b> <b>You can create your own data from scratch or build on anystyle's default sets</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> <b>The default parser model is based on the core set; the default finder model source data is not publicly available in its entirety, but you can find a number of tagged documents here you have compiled a data set for training, you will be ready to create your own model: anystyle train training-data.xml custom.mod this will save your new model as `custom.mod`</b>[<font color=red>industrial</font>]<b>.</b> <b>To use your model instead of anystyle's default, use the `-p` or `parser-model` flag and, respectively, `-f` or `finder-model` to use a custom finder model</b>[<font color=red>industrial</font>]<b>.</b> <b>For instance, the command below would parse all references in `bib.txt` using the custom model we just trained and print the result to stdout using the json output format: anystylep custom.modf json parse bib.txt when training your own models, it is good practice to check the quality using a second data set</b>[<font color=red>industrial</font>]<b>.</b> <b>For example, using anystyle's own gold set (a large, manually curated data set) we could check our custom model like this: anystylep x.mod check. res parser gold.xml checking gold.xml....</b>[<font color=red>industrial</font>]<b>.</b> 1 seq 0.06 3 tok 0.01 3s this command will print the sequence and token error rates; in the case of anystyle a the number of sequence errors is the number of references which were tagged differently by the parser than they were in the input; the number of token errors is the total number of words across all the references which were tagged differently. In the example above, we got one reference wrong (out of 1700 at the time); but even this one reference was mostly tagged correctly, because only a total of 3 words were tagged differently. <b>When working with training data, it is a good idea to use the `wapiti::dataset` api in ruby: it supports all the standard set operators and makes it very easy to combine or compare data sets</b>[<font color=red>industrial</font>]<b>.</b> Dictionary adapters during the statistical analysis of reference strings, anystyle relies on a large feature dictionary; by default, anystyle creates a persistent ruby hash in the folder of the `anystyle-data` gem. <b>This uses up about 2mb of disk space and keeps the entire dictionary in memory</b>[<font color=red>industrial</font>]<b>.</b> <b>If you prefer a smaller memory footprint, you can alternatively use anystyle's gdbm dictionary</b>[<font color=violet>market</font>]<b>.</b> Gdbm bindings are part of the ruby standard library and are supported on all platforms, but you may have to install gdbm on your platform before installing ruby. <b>If you do not want to use the the persistent ruyb hash nor the gbdm bindings, you can store your dictionary in memory (not recommended) or use a redis</b>[<font color=red>industrial</font>]<b>.</b> <b>The best way to change the default dictionary adapter is by adjusting anystyle's default configuration (when using the default parser instances you must set the default before using the parser): anystyle::dictionary.defaults:adapter :ruby use a persistent ruby hash; slower start-up than gdbm but no extra dependency anystyle::dictionary.defaults:adapter :hash use in-memory dictionary; slow start-up but uses no space on disk require 'anystyle dictionary gdbm' anystyle::dictionary.defaults:adapter :gdbm to use redis, install the `redis` and `redis namespace` (optional) gems and configure anystyle to use the redis adapter: anystyle::dictionary.defaults:adapter :redis adjust the redis-specifi configuration require 'anystyle dictionary redis' anystyle::dictionary::redis.defaults:host 'localhost' anystyle::dictionary::redis.defaults:port 6379 contributing the anystyle source code is hosted on github can check out a copy of the latest code using git: git clone you've found a bug or have a question, please open an issue on the anystyle issue tracker for extra credit, clone the anystyle repository, write a failing example, fix the bug and submit a pull request</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 28 ************************************\n",
      "Number of words: 131\n",
      "Number of sentences: 8\n",
      "Total civic: 1\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 3\n",
      "Total inspired: 0\n",
      "Total market: 1\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/evllabs/JGAAP/blob/master/README.md\n",
      "URL: https://github.com/evllabs/JGAAP/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Jgaap(logo.png) java graphical authorship attribution program jgaap is a tool to allow nonexperts to use cutting edge machine learning techniques on text attribution problems</b>[<font color=red>industrial</font>]<b>.</b> <b>Jgaap is developed by the evaluating variation in language (evl) lab at duquesne university</b>[<font color=red>industrial</font>]<b>.</b> <b>Getting started understanding authorship attribution(docs authorship_attribution.md) running your first experiment(docs running_jgaap.md) running large experiments(docs experiment_engine.md) extending jgaap(docs modules.md) download head over to our releases page to get the latest version of jgaap</b>[<font color=red>industrial</font>]<b>.</b> <b>Support if you need help with jgaap, please review our forum of past help requests here forum jgaap-support)</b>[<font color=blue>civic</font>]<b>.</b> You should always feel free to reach out to us at any time with questions or suggestions at jgaap-support@googlegroups.com(mailto:jgaap-support@googlegroups.com). License jgaap has been released under the agplv3.0 and a copy should be included with the source. <b>If it has not been included, a copy can be found at '</b>[<font color=violet>market</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 29 ************************************\n",
      "Number of words: 268\n",
      "Number of sentences: 10\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 8\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/xrobin/pROC/blob/master/README.md\n",
      "URL: https://github.com/xrobin/pROC/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " Xavier robin, natacha turck, alexandre hainard, et al. <b>(2011) cproc: an open-source package for r and s+ to analyze and compare roc curvesd</b>[<font color=red>industrial</font>]<b>.</b> <b>Doi: 10.1186 1471-2105-12-77 the official web page on expasy the cran page my blog the faq latest stable version is best installed from the cran: install.packages( proc ) getting started if you don't want to read the manual first, try the following: loading basic roc auc analysis smoothing more options, ci and plotting coordinates of the curve confidence intervals comparisons sample size getting help type `?proc` on the r command line make sure you've read the faq search for questions tagged with proc-r-package on stack overflow you still can't find an answer, you can: ask a question on stack overflow with the proc-r-package tag bug reports should be submitted to the github issue tracker installing the development version download the source code from git, unzip it if necessary, and then type `r cmd install proc`</b>[<font color=red>industrial</font>]<b>.</b> <b>Alternatively, you can use the devtools package by hadley wickham to automate the process (make sure you follow the full instructions to get started check to run all automated tests, including slow tests: vdiffr the vdiffr package is used for visual tests of plots</b>[<font color=red>industrial</font>]<b>.</b> <b>Slow ones) from the command line: to run the checks upon r cmd check, set environment variable `not_cran1`: release steps 1</b>[<font color=red>industrial</font>]<b>.</b> <b>Get new version to release: `version (grep version proc description | sed s.\\+ ) echo version` 1</b>[<font color=red>industrial</font>]<b>.</b> <b>Build check package: `r cmd build proc r cmd check as-cran proc_ version.tar.gz` 1</b>[<font color=red>industrial</font>]<b>.</b> <b>Check with slow tests: `not_cran1 run_slow_teststrue r cmd check proc_ version.tar.gz` 1</b>[<font color=red>industrial</font>]<b>.</b> <b>Check reverse dependencies: `revdepcheck::revdep_check(num_workers8, timeout as.difftime(60, units mins ))` 1</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 30 ************************************\n",
      "Number of words: 12\n",
      "Number of sentences: 2\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 1\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/sanity/quickml/blob/master/README.md\n",
      "URL: https://github.com/sanity/quickml/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>This is the source repository for the quickml java machine learning library</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 31 ************************************\n",
      "Number of words: 175\n",
      "Number of sentences: 7\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 2\n",
      "Total inspired: 0\n",
      "Total market: 1\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/ramhiser/activelearning/blob/master/README.md\n",
      "URL: https://github.com/ramhiser/activelearning/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>B activelearning active learning is a machine-learning paradigm for optimally choosing unlabeled observations in a training data set to query for their true labels</b>[<font color=red>industrial</font>]<b>.</b> The framework is particularly useful when there are very few labeled observations relative to a large number of unlabeled observations, and the user seeks to determine as few true labels as possible to achieve highly accurate classifiers. The `activelearning` r package is a collection of various active-learning methods from the literature to optimally query observations with respect to a variety of objective functions. Some active learning methods require posterior probability estimates of the unlabeled observations from a single classifier or a committee of classifiers; this package allows the user to specify custom classifiers or specify a variety of classifiers by interfacing with the `caret` r package `activelearning` package implements the active learning methods as defined by the excellent literature survey bsettles pub settles.activelearning.pdf) from dr. This literature survey is also available in book form is highly recommended. <b>Installation you can install the stable version on cran if you prefer to download the latest version, instead type:</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 32 ************************************\n",
      "Number of words: 247\n",
      "Number of sentences: 16\n",
      "Total civic: 1\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 12\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/jubatus/jubatus/blob/master/README.md\n",
      "URL: https://github.com/jubatus/jubatus/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "  jubatus library is an online machine learning framework which runs in distributed environment. <b>Quick start we officially support red hat enterprise linux (rhel) 6.2 or later (64-bit) and ubuntu server 14.04 lts 16.04 lts 18.04 lts (64-bit)</b>[<font color=red>industrial</font>]<b>.</b> <b>On supported systems, you can install all components of jubatus using binary packages</b>[<font color=red>industrial</font>]<b>.</b> <b>Red hat enterprise linux 6.2 or later (64-bit) run the following command to register jubatus yum repository to the system</b>[<font color=red>industrial</font>]<b>.</b> <b>:: for rhel 6 sudo rpmuvh for rhel 7 sudo rpmuvh install ``jubatus`` and ``jubatus-client`` package</b>[<font color=red>industrial</font>]<b>.</b> <b>:: sudo yum install jubatus jubatus-client now jubatus is installed in `` usr bin juba``</b>[<font color=red>industrial</font>]<b>.</b> <b>:: jubaclassifierf usr share jubatus example config classifier pa.json ubuntu server (64-bit) write the following line to `` etc apt sources.list.d jubatus.list`` to register jubatus apt repository to the system</b>[<font color=red>industrial</font>]<b>.</b> <b>:: for ubuntu 12.04 (precise) deprecated (unsupported) deb binary for ubuntu 14.04 (trusty) deb binary for ubuntu 16.04 (xenial) deb binary for ubuntu 18.04 (bionic) deb trustedyes now install ``jubatus`` package</b>[<font color=red>industrial</font>]<b>.</b> <b>:: sudo apt-get update sudo apt-get install jubatus now jubatus is installed in `` opt jubatus bin juba``</b>[<font color=red>industrial</font>]<b>.</b> <b>:: source opt jubatus profile jubaclassifierf opt jubatus share jubatus example config classifier pa.json other platforms for other platforms, refer to the `documentation `</b>[<font color=red>industrial</font>]<b>.</b> <b>License lgpl 2.1 third-party libraries included in jubatus jubatus source tree includes following third-party library</b>[<font color=blue>civic</font>]<b>.</b> <b>Eigen is licensed under mpl2 (partially in lgpl 2.1 or 2.1+)</b>[<font color=red>industrial</font>]<b>.</b> <b>The fork of pficommon is licensed under new bsd license</b>[<font color=red>industrial</font>]<b>.</b> <b>Update history update history can be found from `changelog ` or `wikipage `</b>[<font color=red>industrial</font>]<b>.</b> Contributors patches contributed by `those people `."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 33 ************************************\n",
      "Number of words: 9\n",
      "Number of sentences: 2\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 1\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/fcurella/django-recommends/blob/master/README.md\n",
      "URL: https://github.com/fcurella/django-recommends/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b> django app that builds item-based suggestions for users</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 34 ************************************\n",
      "Number of words: 1383\n",
      "Number of sentences: 50\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 28\n",
      "Total inspired: 0\n",
      "Total market: 1\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/Artificial-Engineering/lycheejs/blob/master/README.md\n",
      "URL: https://github.com/Artificial-Engineering/lycheejs/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " Lychee.js (2018-q3) guides asset qr-codes.png( guides asset qr-codes.png) brought to you as libre software with joy and pride by artificial engineering our libre bot cloud via btc 1cammuvrfu1qamebpodsl3jriovdoxezy2(bitcoin:1cammuvrfu1qamebpodsl3jriovdoxezy2?amount0.5 labellychee.js 20support). Work-in-progress (aka alpha) these are the things that we are currently working on: lychee.ai.neat (es hyperneat ai) is being refactored and unstable. <b>Lychee.js studio is being extended with entity scene editing features</b>[<font color=red>industrial</font>]<b>.</b> <b>Eli5 what is lychee.js? lychee.js is an application engine that has the core idea to reuse components and isomorphic code across all platforms it delivers to</b>[<font color=red>industrial</font>]<b>.</b> <b>These platforms can be incremental in its feature support, which means also low-end platforms like the arduino variants are supported</b>[<font color=red>industrial</font>]<b>.</b> <b>The really only thing lychee.js requires to deliver a platform is a working `settimeout(callback, timeout)`, `setinterval(callback, delay)` and an implemented stuff(. libraries crux source platform html stuff.js) data type that can load files</b>[<font color=red>industrial</font>]<b>.</b> <b>The shipped officially supported platforms are implemented by the lychee.js crux(. libraries crux) library</b>[<font color=red>industrial</font>]<b>.</b> <b>The lychee.js engine delivers to different platforms using so-called fertilizer adapters(. libraries lychee source platform) which allow feature detection and automated isomorphic builds from and to every single platform it supports using the lychee.js fertilizer(. libraries fertilizer)</b>[<font color=red>industrial</font>]<b>.</b> <b>If a new platform wants full application support, it requires an implementation of `lychee.input`, `lychee.renderer`, `lychee.stash`, `lychee.storage` and `lychee.viewport`</b>[<font color=red>industrial</font>]<b>.</b> If it wants full ai support it additionally requires `lychee.net.client`, `lychee.net.remote` and `lychee.net.server`. <b>Underneath the lychee.js engine has a strong serialization and deserialization concept that allows simulations and re-simulations across all platforms</b>[<font color=red>industrial</font>]<b>.</b> Errors can be reproduced everywhere, even network traffic and user interactions are put in event graphs and identified and learned by neural networks in order to figure out what module (in the flow-based programming sense) had corrupt states or data. The lychee.ai(. libraries lychee source ai) and lychee.policy(. libraries lychee source policy) stack allow generic plug play usage and creation of neural networks that can translate generically _every property_ into vectorized features that neural networks can understand. <b>As everything in lychee.js is serializable and uses a composite pattern, things like traffic sharding, automated layouting or animation learning and debugging are inclusive</b>[<font color=red>industrial</font>]<b>.</b> <b>The lychee.js breeder(. libraries breeder) is a tool to create new boilerplates and allows the reusage of projects as libraries and vice versa</b>[<font color=red>industrial</font>]<b>.</b> <b>Every lychee.js project or library is fully isomorphic and can be forked, modified, back-merged or included as a library; which also allows a b testing new composites or definitions in-place live in any environment or simulation</b>[<font color=red>industrial</font>]<b>.</b> The lychee.js strainer(. libraries strainer) is a tool that lints your code and translates the code into a knowledge graph that allows the identification of enums, events, methods, properties and states that adaptive neural networks can use in order to automatically learn and generate code based on existing data. <b>The lychee.js harvester(. libraries harvester) is a peer-to-peer server that allows the sharing of knowledge across the internet</b>[<font color=red>industrial</font>]<b>.</b> Every lychee.js project on this planet is part of a giant meshnet that helps every other lychee.js instance to learn and evolve more quickly; and stores the knowledge graph in a dht (a custom kademlia to allow shrinking similar to how gzip dictionaries work using murmur and bencode biton json under the hood). <b>The lychee.js ranger(. libraries ranger) is a tool for maintenance of local or remote lychee.js harvester instances, to quickly force-restart its watcher plugins or manage its profiles and started projector library-specific servers</b>[<font color=red>industrial</font>]<b>.</b> <b>The lychee.js studio(. libraries studio) is the idea of a zen coding like autocompletion ide that searches the local knowledge graph to quickly setup a project library and its definitions by leveraging both reinforced evolutionary and bayesian learning techniques</b>[<font color=red>industrial</font>]<b>.</b> <b>Oh, and lychee.js can compile, analyze, bugfix and improve itself, too</b>[<font color=red>industrial</font>]<b>.</b> <b>That's essentially what the. bin configure.sh(. bin configure.sh) script does when it builds and distributes the lychee.js crux(. libraries crux) library and the lychee.js engine(. libraries lychee) library</b>[<font color=red>industrial</font>]<b>.</b> Overview the lychee.js project started in 2012 and is in active development. <b>The following repositories are related to the lychee.js engine: lychee.js experiments contains all lychee.js experiments and prototypes</b>[<font color=red>industrial</font>]<b>.</b> <b>Lychee.js runtime contains all pre-compiled lychee.js runtimes and fertilizers</b>[<font color=red>industrial</font>]<b>.</b> <b>Lychee.js library contains the lychee.js library (installable via `bower` and `npm`, forked from ` libraries lychee`)</b>[<font color=red>industrial</font>]<b>.</b> Lychee.js website contains the lychee.js website (hosted at lychee.js.org lychee.js future contains all concepts and ideas not yet finished. The following accounts are related to the lychee.js engine: @cookiengineer is the core maintainer and founder of this project. <b>@humansneednotapply is the account used by our software bots</b>[<font color=red>industrial</font>]<b>.</b> <b>Features the lychee.js engine aims to deliver total automation through artificial intelligence and better software architecture</b>[<font color=red>industrial</font>]<b>.</b> Everything listed here requires zero lines of code overhead and is already fully integrated in the lychee.js boilerplate(. projects boilerplate): the lychee.js core and definition system : isomorphic application engine (runs pretty much everywhere) language is only es2018+ code, nothing else composite pattern inspired entity component system definition system embraces simplicity and feature detection sandboxing system embraces automated error reports, analytics and debugging serialization system allows re-simulation on any platform built-in offline storage management and network synchronization the lychee.js engine and software bots: graphical asset management and entity scene design tool graphical project management and server maintenance tool cross-platform compiler bootstrapping library cross-platform application engine command-line continous integration server command-line wizard for projects and libraries command-line builder and cross-compiler command-line fuzz-tester and code-refactorer features of the lychee.js software bots: automated code refactoring, bug fixing and code improvements automated design tracking, layout and flow optimization automated packaging for embedded, console, mobile, desktop and server apps automated deployment via git and live-updates automated reactive responsive ui ux components automated debugging, network and ui ux flow analysis automated testing and integration with the ai automated networking (peer-to-peer http1.1 2.0 and ws13 with local global discovery) automated network services and traffic balancing sharding platform fertilizer support the target platforms are described as so-called fertilizers. <b>Those fertilizers cross-compile everything automagically using a serialized `lychee.environment` that is setup in each project's or library's `lychee.pkg` file</b>[<font color=red>industrial</font>]<b>.</b> <b>Explanations of target matrix: the `binary` column describes whether there is a native binary built</b>[<font color=red>industrial</font>]<b>.</b> <b>The `package` column describes whether there is a native package built</b>[<font color=red>industrial</font>]<b>.</b> <b>The cpu architecture columns describe the target architecture, the host architecture is `x86_64` for all external sdks</b>[<font color=red>industrial</font>]<b>.</b> The `ios` target currently cannot be delivered to via external sdk on a `linux` development host; however it is still possible to create a webview-using app and use the `html` platform as a fertilizer target. <b>Alternatively, advanced users are encouraged to use the `nidium` runtime on ios and macos</b>[<font color=red>industrial</font>]<b>.</b> Quickstart guide if you want to install the lychee.js engine, the best way to do so is to follow through the quickstart guide( guides quickstart). Installation cli animation let us know if we can improve anything in these documents by opening up a documentation problem help if you have any questions, feel free to join us on artificial-engineering @ freenode are our official social media channels: twitter: reddit: irc: artificial-engineering @ freenode email: robot insert an at here artificial.engineering license the lychee.js engine (defined as ` libraries` and ` bin` inside the lycheejs is (c) 2012-2018 artificial-engineering and released under mit expat(. license_mit.txt) license. The projects (defined as ` projects` inside the lycheejs and their assets are licensed under cc by-sa 4.0(. license_cc4-by-sa.txt) license. The lychee.js runtimes (defined as ` bin runtime` or the lycheejs-runtime are owned and copyrighted by their respective owners and those may be shipped under a different license. <b>As of now, the runtimes are licensed under the following terms: mit license for `node` platform (node.js) mit license for `html-nwjs` platform (nw.js) mit license for `html-webview` platform and (c) 2012-2018 artificial engineering apache license for android sdk toolchain the generated code by our artificial intelligence (namely the github account @humansneednotapply or the commit's e-mail address `robot insert an at here artificial.engineering`) is released under gnu gpl 3(. license_gpl3.txt) license</b>[<font color=red>industrial</font>]<b>.</b> The date of each commit is equivalent to the date (central european timezone) of claimed copyright and license, no matter from which timezone or physical location they were commited from. The generated code by the artificial intelligence and its gnu gpl 3 license overrules the mit expat license in every case, with no exceptions. The code is distributed in a libre way to guarantee free and open knowledge distribution for our software bots. The owner of the gnu gpl 3 licensed code is the artificial-engineering project, though the legal entity as of today has to be a human person under european law and the directive 2006 116 ec (p.14 and art. <b>Hereby @cookiengineer grants you permission to reuse the generated code by the artificial intelligence under above terms</b>[<font color=red>industrial</font>]<b>.</b> <b>You are not allowed to change those terms without @cookiengineer</b>[<font color=violet>market</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 35 ************************************\n",
      "Number of words: 351\n",
      "Number of sentences: 17\n",
      "Total civic: 1\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 8\n",
      "Total inspired: 0\n",
      "Total market: 1\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/clojurenlp/core/blob/master/README.md\n",
      "URL: https://github.com/clojurenlp/core/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Org.clojurenlp.core language processing in clojure based on the stanford-corenlp parser</b>[<font color=red>industrial</font>]<b>.</b> Fb maintainers wanted we need help getting this project moving. <b>Please feel free to email to leontalbot@gmail.com to join the org, or drop a line in the chat room</b>[<font color=red>industrial</font>]<b>.</b> This is a work in progress, currently in the poc phase. <b>Usage tokenization (use 'org.clojurenlp.core) (tokenize this is a simple sentence</b>[<font color=red>industrial</font>]<b>.</b> <b>) ;; '(:token this , 0, 4 :token is , 5, 7 :token a , 8, 9 :token simple , 10, 16 :token sentence , 17, 25 :token</b>[<font color=violet>market</font>]<b>.</b> , 25, 26 ) part-of-speech tagging to get a list of `taggedword` objects: (use 'org.clojurenlp.core) ;; use any of these: (- short and sweet. To return a tag string from taggedword object: (- short and sweet. <b>Tokenize pos-tag first.tag) ;; jj (- short and sweet</b>[<font color=red>industrial</font>]<b>.</b> <b>) for more information, see the relevant javadoc named entity recognition to tag named entities utilizing standard stanford ner model: (use 'org.clojurenlp.core) (def pipeline (initialize-pipeline)) (def text the united states of america will be tagged as a location ) (tag-ner pipeline text) training your own model how to train your own model a) to tag named entities utilizing custom trained model: (use 'org.clojurenlp.core) (def pipeline (initialize-pipeline path-to-serialized-model )) (def text the united states of america will be tagged as a location ) (tag-ner pipeline text) utilizing either ner tagging strategy, a map containing the original text, sentences, tokens, and ner tags will be returned</b>[<font color=red>industrial</font>]<b>.</b> <b>Parsing to parse a sentence: \\t(use 'org.clojurenlp.core) \\t(parse (tokenize text)) you will get back a labeledscoredtreenode which you can plug in to other stanford corenlp functions or can convert to a standard treebank string with: \\t(str (parse (tokenize text))) stanford dependencies \\t(dependency-graph i like cheese</b>[<font color=red>industrial</font>]<b>.</b> <b>) will parse the sentence and return the dependency graph as a loom graph, which you can then traverse with standard graph algorithms like shortest path, etc</b>[<font color=red>industrial</font>]<b>.</b> You can also view it: \\t(def graph (dependency-graph i like cheese. <b>)) \\t(use 'loom.io) \\t(view graph) this requires graphviz to be installed</b>[<font color=red>industrial</font>]<b>.</b> License 2018 the clojurenlp organization and contributors distributed under the apache 2.0 license. <b>The clojurenlp organization leon talbot @leontalbot andrew mcloud @andrewmcloud contributors cory giles hans engel damien stanton andrew mcloud leon talbot marek owsikowski</b>[<font color=blue>civic</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 36 ************************************\n",
      "Number of words: 991\n",
      "Number of sentences: 30\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 27\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/foo123/FILTER.js/blob/master/README.md\n",
      "URL: https://github.com/foo123/FILTER.js/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Filter.js a pure javascript library for image video processing, filtering and computer vision this is a library for processing images video in pure javascript using html5 features like canvas, webworkers, webgl and svg (in progress) or analogs in node.js</b>[<font color=red>industrial</font>]<b>.</b> Version 0.9.6 filter.core.js filter.core.min.js filter.io.js filter.io.min.js filter.codecs.js filter.codecs.min.js filter.filters.js filter.filters.min.js filter.plugins.js filter.plugins.min.js filter.bundle.js, filter+io+codecs+filters+plugins+dependencies also: contemplate a light-weight template engine for node xpcom js, php, python, actionscript htmlwidget html widgets used as (template) plugins and or standalone for php, node xpcom js, python (can be used as plugins for contemplate engine as well) tao a simple, tiny, isomorphic, precise and fast template engine for handling both string and live dom based templates modelview a light-weight and flexible mvvm framework for javascript html5 modelview mvc jqueryui widgets plug-n-play, state-full, full-mvc widgets for jqueryui using modelview.js (e.g calendars, datepickers, colorpickers, tables grids, etc.) (in progress) dromeo a flexible, agnostic router for node xpcom js, php, python, actionscript publishsubscribe a simple and flexible publish-subscribe pattern implementation for node xpcom js, php, python, actionscript regex analyzer composer regular expression analyzer and composer for node xpcom js, php, python, actionscript xpresion a simple and flexible expression parser engine (with custom functions and variables support) for php, python, node xpcom js, actionscript grammartemplate versatile and intuitive grammar-based templating for php, python, node xpcom js, actionscript dialect a simple cross-platform sql construction for php, python, node xpcom js abacus a fast combinatorics and computation library for node xpcom js, php, python, actionscript asynchronous a simple manager for async, linearised, parallelised, interleaved and sequential tasks for javascript rt client-side real-time communication for node xpcom js with support for poll bosh websockets contents live examples( live-examples) browser support( browser-support) credits( credits) references( references.md) features( features) api reference( api-reference.md) changelog( changelog.md) todo( todo) live examples image processing with `filter.js` video processing with `filter.js` sound visualization with `filter.js` (trioptic) `filter.js` with `three.js` `filter.js` in `node.js`( examples node) browser support firefox( screenshots firefox.png) chrome( screenshots chrome.png) opera( screenshots opera.png) ie( screenshots ie.png) nodejs( screenshots node.png) credits some filters code has been adapted from open source libraries (mostly `c`, `java` and `flash`, plus a couple from `javascript` libraries), see the comments in the code for details. Image processing library in java as3 image processing library as3 colormatrix by @gskinner simplex noise and perlin noise by stefan gustavson glfx.js jviolajones haar.js opencv, haar cascades zlib ffmpeg some image processing computer vision theory, basics and tutorials (see references( references.md)): light, color, perception and color space theory williams cs422 color.pdf) the influence of history amp; culture on visual perception a beginners guide to bitmaps by paul burke opencv, open source computer vision general-purpose gpu scientific computing (.moving towards) features the library dependencies are: classy.js micro object-oriented framework. <b>Asynchronous simple manager for async parallel tasks</b>[<font color=red>industrial</font>]<b>.</b> <b>The framework defines an image proxy class( api-reference.md image-class), which represents an image, a number of utilities like `color` class, image loader classes( api-reference.md file-input-output), image codecs( api-reference.md codecs), and 17 generic `filter` types (some having `glsl` `svg` analogs) plus various plugins and extra filters (with support for parallel processing transparently both for `browser` and `nodejs`) 0</b>[<font color=red>industrial</font>]<b>.</b> <b>abstractfilter( api-reference.md generic-abstract-filter) 1</b>[<font color=red>industrial</font>]<b>.</b> <b>colortablefilter( api-reference.md color-table-filter) 2</b>[<font color=red>industrial</font>]<b>.</b> <b>colormatrixfilter( api-reference.md color-matrix-filter) (analogous to the actionscript filter) 3</b>[<font color=red>industrial</font>]<b>.</b> <b>colormapfilter( api-reference.md color-map-filter) 4</b>[<font color=red>industrial</font>]<b>.</b> <b>affinematrixfilter( api-reference.md affine-matrix-filter) 5</b>[<font color=red>industrial</font>]<b>.</b> <b>geometricmapfilter( api-reference.md geometric-map-filter) 6</b>[<font color=red>industrial</font>]<b>.</b> <b>displacementmapfilter( api-reference.md displacement-map-filter) (analogous to actionscript filter) 7</b>[<font color=red>industrial</font>]<b>.</b> <b>convolutionmatrixfilter( api-reference.md convolution-matrix-filter) (analogous to the actionscript filter) 8</b>[<font color=red>industrial</font>]<b>.</b> <b>morphologicalfilter( api-reference.md morphological-filter) 9</b>[<font color=red>industrial</font>]<b>.</b> <b>statisticalfilter( api-reference.md statistical-filter) (previously called `nonlinearfilter`) 10</b>[<font color=red>industrial</font>]<b>.</b> <b>compositefilter( api-reference.md composite-filter) (an abstraction of a container stack for multiple filters) 12</b>[<font color=red>industrial</font>]<b>.</b> <b>algebraicfilter( api-reference.md algebraic-filter) (an abstraction of algebraic combination of images or other filter outputs into an output image, to be added) 13</b>[<font color=red>industrial</font>]<b>.</b> <b>inlinefilter( api-reference.md inline-filter) (create inline filters dynamicaly at run-time using your custom functions) 14</b>[<font color=red>industrial</font>]<b>.</b> <b>dimensionfilter( api-reference.md dimension-filter) 15</b>[<font color=red>industrial</font>]<b>.</b> <b>glslfilter( api-reference.md glsl-filter) glsl-based (`webgl` `node-gl`) analogs of at least some of the generic filters (in progress, possibly in next update) 16</b>[<font color=red>industrial</font>]<b>.</b> <b>svgfilter( api-reference.md svg-filter) svg-based filters (todo) 17</b>[<font color=red>industrial</font>]<b>.</b> <b>plugins( api-reference.md plugins-and-extra-filters) (a number of plugin filters which cover a wide(r) range of functionality and use cases) each of the generic filters is prototype but it also includes a number of implementation filters like `grayscale` , `colorize` , `threshold` , `gaussblur` , `laplace` , `emboss` , `gamma`, `twirl` and so on</b>[<font color=red>industrial</font>]<b>.</b> <b>(depending on type of filter) parallel processing support (browser and node) (support parallel procesing filtering with filter workers in an intuitive and transparent way, see examples) gpu processing support (browser and node, in progress) (support gpu-based parallel procesing filtering with glsl filters in an intuitive and transparent way) image blending modes (analogous to photoshop blend modes) the filters, and the way they operate, naturaly represent a system of interconnected nodes which process and interchange (image) data (not necesarily synchronously), a.k.a a signal processing graph system</b>[<font color=red>industrial</font>]<b>.</b> <b>The result is a streamlined flow for image processing and computer vision in javascript</b>[<font color=red>industrial</font>]<b>.</b> <b>Tip: you can create your custom build of the library with the filters plugins you choose</b>[<font color=red>industrial</font>]<b>.</b> <b>Each filter and plugin is independent and can be used in a mix-n-match manner, as long as the core classes are always included</b>[<font color=red>industrial</font>]<b>.</b> <b>Change the dependencies file(s) to include your own selection of filters and plugins for your custom build todo add `glsl` (`webgl` `node-gl`) support for various generic filters (in progress, possibly in next update) add some needed signal processing graph node filters (eg `algebraic`, `switch`, `delay` etc.) (in progress) add active-shape geometric filters, color histogram-detector filters,</b>[<font color=red>industrial</font>]<b>.</b> <b>(todo) add `2d-fft` routines, frequency-domain filtering (todo) add `svg`, `css` filters interface support for some filters (todo) add machine learning (image) segmentation clustering algorithms (e.g `kmeans`, `kmedoids`, `connected components`, `deterministic annealing`, `svd`, `jade`,.) done partially implement some numeric routines (e.g `blas`, `filter` routines) using faster `asm.js` (browser amp; nodejs) and or `simd.js` done partially make convolutions statistics faster done partially add full support for `node.js` done add (generic native) codec support for image formats, e.g `.tga`, `.hdr` `.rgbe`, `.gif`, `.bmp`, `.png`, `.jpg` `.jpeg` etc</b>[<font color=red>industrial</font>]<b>.</b> <b>Done add support for `parallel processing` using `web workers` and or `asynchronous processing` done use fixed-point arithmetic, micro-optimizations where possible done add caching of filter parameters where applicable done increase performance for `opera`, `ie` done partially '</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 37 ************************************\n",
      "Number of words: 247\n",
      "Number of sentences: 10\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 4\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/mbq/rFerns/blob/master/README.md\n",
      "URL: https://github.com/mbq/rFerns/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>B rferns is an extended random ferns implementation for r in comparison to original, it can handle standard information system containing both categorical and continuous attributes</b>[<font color=red>industrial</font>]<b>.</b> <b>Moreover, it generates oob error approximation and permutation-based attribute importance measure similar to randomforest breiman randomforests cc_home.htm)</b>[<font color=red>industrial</font>]<b>.</b> Here is a paper with all the details is good for doing training fast and in predictable time; in general it is less accurate than random forest, yet not substantially, and obviously there are cases in which it is better. It is also nice as a very fast variable importance source; in fact it was created to speed-up the `boruta` all relevant feature selector and it did pretty well it is a very stochastic method, practically doing no optimisation at all; basically it is crazy that it works. Hence, it is theoretically interesting (; since v2.0.1, it supports merging of rferns models, making it possible to implement adaptive ensemble size or something like online learning. Since v2.0.0, it can do shadow importance, i.e, a heuristic way to reason about the significance of importance scores. <b>Since v0.3.2, it can do multi-label classification as well; here is a conference paper about that there is also a spark version (not mine), sparkling ferns also this to use quite fresh version should be on cran see the r docs for more</b>[<font color=red>industrial</font>]<b>.</b> <b>If you want to use it test it apart from r, it is quite possible consult `side_src test.c` to see how this may work</b>[<font color=red>industrial</font>]<b>.</b> Yet don't expect that this will ever become a standalone library."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 38 ************************************\n",
      "Number of words: 111\n",
      "Number of sentences: 5\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 2\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/GHamrouni/Recommender/blob/master/README.md\n",
      "URL: https://github.com/GHamrouni/Recommender/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Recommender c library for product recommendations suggestions using collaborative filtering (cf)</b>[<font color=red>industrial</font>]<b>.</b> Recommender analyzes the feedback of some users (implicit and explicit) and their preferences for some items. It learns patterns and predicts the most suitable products for a particular user. <b>Features collaborative filtering user and item based recommenders no external dependencies fast running time 81 seconds for 10 million ratings (on movielens data sets) memory footprint under 160 mb for 10 million ratings webpage compile recommender: make the compilation will produce librecommender.a to compile an example: gcc test test.c src librecommender.almo test t1i src alternatively you can use clang clang test test.c src librecommender.almo test t1i src keywords collaborative filtering, recommender system references 1</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 39 ************************************\n",
      "Number of words: 285\n",
      "Number of sentences: 13\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 9\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/scikit-learn-contrib/lightning/blob/master/README.md\n",
      "URL: https://github.com/scikit-learn-contrib/lightning/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b> is a library for large-scale linear classification, regression and ranking in python</b>[<font color=red>industrial</font>]<b>.</b> Highlights: follows the `scikit-learn ` api conventions supports natively both dense and sparse data representations computationally demanding parts implemented in `cython ` solvers supported: primal coordinate descent dual coordinate descent (sdca, prox-sdca) sgd, adagrad, sag, saga, svrg fista example example that shows how to learn a multiclass classifier with group lasso penalty on the news20 dataset (c.f., `blondel et al. <b> python from sklearn.datasets import fetch_20newsgroups_vectorized from lightning.classification import cdclassifier load news20 dataset from scikit-learn</b>[<font color=red>industrial</font>]<b>.</b> <b>Bunch fetch_20newsgroups_vectorized(subset all ) x bunch.data y bunch.target set classifier options</b>[<font color=red>industrial</font>]<b>.</b> <b>Clf cdclassifier(penalty l1 l2 , loss squared_hinge , multiclasstrue, max_iter20, alpha1e-4, c1.0 x.shape0, tol1e-3) train the model</b>[<font color=red>industrial</font>]<b>.</b> <b>Clf.fit(x, y) accuracy print(clf.score(x, y)) percentage of selected features print(clf.n_nonzero(percentagetrue)) dependencies lightning requires python 2.7, setuptools, numpy 1.3, scipy 0.7 and scikit-learn 0.15</b>[<font color=red>industrial</font>]<b>.</b> <b>Building from source also requires cython and a working c c++ compiler</b>[<font color=red>industrial</font>]<b>.</b> <b>Installation precompiled binaries for the stable version of lightning are available for the main platforms and can be installed using pip:: pip install sklearn-contrib-lightning or conda:: conda installc conda-forge sklearn-contrib-lightning the development version of lightning can be installed from its git repository</b>[<font color=red>industrial</font>]<b>.</b> <b>In this case it is assumed that you have the git version control system, a working c++ compiler, cython and the numpy development libraries</b>[<font color=red>industrial</font>]<b>.</b> <b>In order to install the development version, type:: git clone cd lightning python setup.py build sudo python setup.py install documentation github you use this software, please cite it</b>[<font color=red>industrial</font>]<b>.</b>  @misclightning_2016, author blondel, mathieu and pedregosa, fabian , title lightning: large-scale linear classification, regression and ranking in python , year 2016, doi 10.5281 zenodo.200504 , url other citing formats are available in `its zenodo entry `. Authors mathieu blondel, 2012-present manoj kumar, 2015-present arnaud rachez, 2016-present fabian pedregosa, 2016-present '."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 40 ************************************\n",
      "Number of words: 282\n",
      "Number of sentences: 15\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 11\n",
      "Total inspired: 0\n",
      "Total market: 1\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/kevincobain2000/sentiment_classifier/blob/master/README.md\n",
      "URL: https://github.com/kevincobain2000/sentiment_classifier/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " Iphone app for twitter sentiments is out no longer available. <b>Sorry due to lack of funds to run a seperate server app has been taken out of the app store</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> Use it free to build your own app tho sentiment classification using wsd, maximum entropy naive bayes classifiers pip install sentiment_classifier `home ` `pypi package ` `github ` overview sentiment classifier using word sense disambiguation using ``wordnet`` and word occurance statistics from movie review corpus ``nltk``. <b>For twitter sentiment analysis bigrams are used as features on naive bayes and maximum entropy classifier from the twitter data</b>[<font color=red>industrial</font>]<b>.</b> <b>Next is use senses instead of tokens from the respective data</b>[<font color=red>industrial</font>]<b>.</b> Raw:: html sentiment_classifier-0.5.tar.gz `download stats provided by` `pypi-github-stats ` sentiment classifiers and data the above online demo uses movie review corpus from nltk, twitter and amazon,on which naive bayes classifier is trained. <b>Classifier using wsd sentiwordnet is based on heuristics and uses wordnet and sentiwordnet</b>[<font color=red>industrial</font>]<b>.</b> <b>Test results on sentiment analysis on twitter and amazon customer reviews data features used for naivebayes will be `github `</b>[<font color=red>industrial</font>]<b>.</b> <b>Requirements in ``version 0.5`` all the following requirements are installed automatically</b>[<font color=red>industrial</font>]<b>.</b> <b>Numpy sentiwordnet how to install shell command :: python setup.py install documentation usage shell commands:: senti_classifierc file with review.txt python usage shell commands :: cd sentiment_classifier src senti_classifier python senti_classifier.pyc reviews.txt library usage</b>[<font color=red>industrial</font>]<b>.</b> <b> python from senti_classifier import senti_classifier sentences 'the movie was the worst movie', 'it was the worst acting by the actors' pos_score, neg_score senti_classifier.polarity_scores(sentences) print pos_score, neg_score</b>[<font color=red>industrial</font>]<b>.</b> <b> python from senti_classifier.senti_classifier import synsets_scores print synsets_scores'peaceful.a.01''pos'</b>[<font color=red>industrial</font>]<b>.</b> <b>0.25 history ``0.7`` python 3.0 suport thanks to @mrlokans ``0.6`` bug fixed upon nltk upgrade ``0.5`` no additional data required trained data is loaded automatically</b>[<font color=red>industrial</font>]<b>.</b> <b>``0.4`` added bag of words as a feature as occurance statistics ``0.3`` sentiment classifier first app, using wsd module</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 41 ************************************\n",
      "Number of words: 178\n",
      "Number of sentences: 6\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 1\n",
      "Total inspired: 0\n",
      "Total market: 1\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/nmap/nmap/blob/master/README.md\n",
      "URL: https://github.com/nmap/nmap/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " Nmap is released under a custom license, which is based on (but not compatible with) gplv2. <b>The nmap license allows free usage by end users, and we also offer a commercial license for companies that wish to redistribute nmap technology with their products</b>[<font color=violet>market</font>]<b>.</b> <b>The latest version of this software as well as binary installers for windows, macos, and linux (rpm) are available from nmap.org documentation is also available on the nmap.org website and suggestions may be sent to the nmap-dev mailing list you should be able to just type:. configure make make install for far more in-depth compilation, installation, and removal notes, read the nmap install guide on nmap.org</b>[<font color=red>industrial</font>]<b>.</b> Using nmap nmap has a lot of features, but getting started is as easy as running `nmap scanme.nmap.org`. Running `nmap` without any parameters will give a helpful list of the most common options, which are discussed in depth in the man page users who prefer a graphical interface can use the included zenmap front-end about filing bug reports and contributing to the nmap project can be found in the hacking(hacking) and contributing.md(contributing.md) files."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 42 ************************************\n",
      "Number of words: 728\n",
      "Number of sentences: 41\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 20\n",
      "Total inspired: 0\n",
      "Total market: 2\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/ptnplanet/Java-Naive-Bayes-Classifier/blob/master/README.md\n",
      "URL: https://github.com/ptnplanet/Java-Naive-Bayes-Classifier/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Java naive bayes classifier ptnplanet java-naive-bayes-classifier) nothing special</b>[<font color=red>industrial</font>]<b>.</b> It works and is well documented, so you should get it running without wasting too much time searching for other alternatives on the net. <b>Maven quick-start this java naive bayes classifier can be installed via the jitpack repository</b>[<font color=red>industrial</font>]<b>.</b> <b>For other build-tools (e.g gradle), visit for configuration snippets</b>[<font color=red>industrial</font>]<b>.</b> <b>Please also head to the release tab for further releases</b>[<font color=red>industrial</font>]<b>.</b> Overview i like talking about features and categories. Objects have features and may belong to a category. <b>The classifier will try matching objects to their categories by looking at the objects' features</b>[<font color=red>industrial</font>]<b>.</b> It does so by consulting its memory filled with knowledge gathered from training examples. Classifying a feature-set results in the highest product of 1) the probability of that category to occur and 2) the product of all the features' probabilities to occure in that category: this is a so-called maximum a posteriori estimation. Wikipedia actually does a good job explaining it: probabilistic_model learning from examples add knowledge by telling the classifier, that these features belong to a specific category: classify unknown objects use the gathered knowledge to classify unknown objects with their features. <b>The classifier will return the category that the object most likely belongs to</b>[<font color=red>industrial</font>]<b>.</b> <b>The classifier will classify sentences (arrays of features) as sentences with either positive or negative sentiment</b>[<font color=red>industrial</font>]<b>.</b> <b>Please refer to the full example for a more detailed documentation</b>[<font color=red>industrial</font>]<b>.</b> <b>This means, that the classifier will forget recent classifications it uses for future classifications after defaulting to 1.000 classifications learned</b>[<font color=red>industrial</font>]<b>.</b> <b>This will ensure, that the classifier can react to ongoing changes in the user's habbits</b>[<font color=red>industrial</font>]<b>.</b> <b>Interface the abstract serves as a base for the concrete</b>[<font color=red>industrial</font>]<b>.</b> <b>Returns a of categories the classifier knows about</b>[<font color=red>industrial</font>]<b>.</b> <b>Retrieves the total number of categories the classifier knows about</b>[<font color=red>industrial</font>]<b>.</b> <b>If the new value is less than the old value, the memory will be truncated accordingly</b>[<font color=violet>market</font>]<b>.</b> Increments the count of a given feature in the given category. This is equal to telling the classifier, that this feature has occurred in this category. This is equal to telling the classifier, that this category has occurred once more. Decrements the count of a given feature in the given category. <b>This is equal to telling the classifier that this feature was classified once in the category</b>[<font color=red>industrial</font>]<b>.</b> This is equal to telling the classifier, that this category has occurred once less. Retrieves the number of occurrences of the given feature in the given category. Retrieves the total number of occurrences of the given feature. Retrieves the number of occurrences of the given category. <b>(implements ) returns the probability that the given feature occurs in the given category</b>[<font color=red>industrial</font>]<b>.</b> Retrieves the weighed average with overall weight of and an assumed probability of. <b>The probability defaults to the overall feature probability</b>[<font color=red>industrial</font>]<b>.</b> <b>Retrieves the weighed average with overall weight of , an assumed probability of and the given object to use for probability calculation</b>[<font color=red>industrial</font>]<b>.</b> <b>Retrieves the weighed average with the given weight and an assumed probability of and the given object to use for probability calculation</b>[<font color=red>industrial</font>]<b>.</b> <b>Retrieves the weighed average with the given weight, the given assumed probability and the given object to use for probability calculation</b>[<font color=red>industrial</font>]<b>.</b> <b>Train the classifier by telling it that the given features resulted in the given category</b>[<font color=red>industrial</font>]<b>.</b> The class implements the following abstract method: it will retrieve the most likely category for the features given and depends on the concrete classifier implementation. Running the example possible performance issues performance improvements, i am currently thinking of: store the natural logarithms of the feature probabilities and add them together instead of multiplying the probability numbers the mit license (mit) copyright (c) 2012-2017 philipp nolte permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the software ), to deal in the software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and or sell copies of the software, and to permit persons to whom the software is furnished to do so, subject to the following conditions: the above copyright notice and this permission notice shall be included in all copies or substantial portions of the software. The software is provided as is , without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and noninfringement. <b>In no event shall the authors or copyright holders be liable for any claim, damages or other liability, whether in an action of contract, tort or otherwise, arising from, out of or in connection with the software or the use or other dealings in the software</b>[<font color=violet>market</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 43 ************************************\n",
      "Number of words: 53\n",
      "Number of sentences: 6\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 5\n",
      "Total inspired: 0\n",
      "Total market: 1\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/OpenANN/OpenANN/blob/master/README.md\n",
      "URL: https://github.com/OpenANN/OpenANN/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Openann an open source library for artificial neural networks</b>[<font color=red>industrial</font>]<b>.</b> <b>You can find the license text in the file `copying`</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> <b>Minimum requirements cmake 2.8 or higher c++ compiler, e</b>[<font color=red>industrial</font>]<b>.</b> <b>G++ build management tool that is supported by cmake, e</b>[<font color=red>industrial</font>]<b>.</b> <b>Make eigen 3 library shell, wget, unzip installation linux cd path to openann dir mkdir build cd build cmake</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 44 ************************************\n",
      "Number of words: 618\n",
      "Number of sentences: 12\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 2\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/accord-net/framework/blob/master/README.md\n",
      "URL: https://github.com/accord-net/framework/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " Accord.net framework\\r \\r \\r (license)\\r \\r \\r \\r the accord.net project provides machine learning, statistics, artificial intelligence, computer vision and image processing methods to.net. It can be used on microsoft windows, xamarin, unity3d, windows store applications, linux or mobile.\\r \\r after merging with the aforge.net project, the framework now offers a unified api for learning training machine learning models that is both easy to use and extensible. <b>It is based on the following pattern:\\r \\r choose a learning algorithm that provides a learn(x, y) or learn(x) method;\\r use the learn(x, y) to create a machine learning model learned from the data; \\r use the model's transform decide scores probabilities or loglikelihoods methods.\\r \\r for more information, please see the getting started guide and check the classfication wiki please do not hesitate to edit the wiki if you would like \\r \\r \\r installing\\r \\r to install the framework in your application, please use nuget</b>[<font color=red>industrial</font>]<b>.</b> If you are on visual studio, right-click on the references item in your solution folder, and select manage nuget packages. Search for accord.machinelearning and select install. <b>\\r \\r if you would like to install the framework on unity3d applications download the libsonly compressed archive from the framework releases page navigate to the releases mono folder, and copy the.dll files to the plugins folder in your unity project</b>[<font color=red>industrial</font>]<b>.</b> Finally, find and add the system.componentmodel.dataannotations.dll assembly that should be available from your system to the plugin folders as well.\\r \\r sample applications\\r \\r the framework comes with a wide range of sample applications to help get you started quickly. If you downloaded the framework sources or cloned the repository, open the samples.sln solution file in the samples folder.\\r \\r \\r building\\r \\r with visual studio 2015\\r \\r please download and install the following dependencies:\\r \\r t4 toolbox for visual studio 2015 sandcastle help file builder (with vs2015 extension) nunit 3 test adapter navigate to the sources directory, and open the accord.net.sln solution file. Note: the solution includes f unit test projects that can be disabled unloaded from the solution in case you do not have support for f tools in your version of visual studio.\\r \\r \\r with visual studio 2017\\r \\r please download and install the following dependencies:\\r \\r t4 toolbox for visual studio 2017 sandcastle help file builder (with vs2017 extension) nunit 3 test adapter visual c++ redistributable for visual studio 2015 (both x64 and x86)\\r \\r then navigate to the sources directory, and open the accord.net.sln solution file. Note: the solution includes f unit test projects that can be disabled unloaded from the solution in case you do not have support for f tools in your version of visual studio.\\r \\r \\r with mono in linux\\r \\r \\r \\r with mono in os x\\r \\r \\r \\r contributing\\r \\r if you would like to contribute, please do so by helping us update the project's wiki pages while you could also make a donation through paypal flattr or any of the cryptocurrencies shown below, as well as fill-in bug reports and contribute code in the form of pull requests, the most precious donation we could receive would be a bit of your time please take some minutes to submit us more documentation examples to our wiki pages :wink: \\r \\r donate using cryptocurrencies:\\r \\r \\r \\r \\r note: all donations are 100 invested towards improving the framework, including, but not limited to, the hiring of extra developers to work on issues currently present at the project's issue tracker. If you would like to donate resources towards the development of a particular issue, please let us know \\r \\r join the chat at but to have issues and questions answered, post it as an issue citing\\r \\r please cite this work as:\\r \\r bibtex.we0_zcyxeuk)\\r '."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 45 ************************************\n",
      "Number of words: 33\n",
      "Number of sentences: 2\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 1\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/bytefish/machinelearning-opencv/blob/master/README.md\n",
      "URL: https://github.com/bytefish/machinelearning-opencv/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>I am currently updating the guide to machine learning with opencv: while doing this i'll switch it from latex to restructured text, so it can be included in the official opencv tutorials section</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 46 ************************************\n",
      "Number of words: 1843\n",
      "Number of sentences: 82\n",
      "Total civic: 1\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 54\n",
      "Total inspired: 0\n",
      "Total market: 3\n",
      "Total project: 1\n",
      "Total renown: 0\n",
      "URL: https://github.com/bigmlcom/python/blob/master/README.md\n",
      "URL: https://github.com/bigmlcom/python/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Bigml python bindings `bigml ` makes machine learning easy by taking care of the details required to add data-driven decisions and predictive power to your company</b>[<font color=red>industrial</font>]<b>.</b> Unlike other machine learning services, bigml creates `beautiful predictive models ` that can be easily understood and interacted with. <b>These bigml python bindings allow you to interact with `bigml.io `, the api for bigml</b>[<font color=red>industrial</font>]<b>.</b> <b>You can use it to easily create, retrieve, list, update, and delete bigml resources (i.e, sources, datasets, models and, predictions)</b>[<font color=red>industrial</font>]<b>.</b> <b>For additional information, see the `full documentation for the python bindings on read the docs `</b>[<font color=red>industrial</font>]<b>.</b> <b>This module is licensed under the `apache license, version 2.0 `</b>[<font color=red>industrial</font>]<b>.</b> Support please report problems and bugs to our `bigml.io issue tracker `. Discussions about the different bindings take place in the general `bigml mailing list `. <b>Requirements python 2.7 and python 3 are currently supported by these bindings</b>[<font color=red>industrial</font>]<b>.</b> <b>The basic third-party dependencies are the `requests `, `poster `, `unidecode ` and `requests-toolbelt ` `bigml-chronos ` libraries</b>[<font color=blue>civic</font>]<b>.</b> <b>These libraries are automatically installed during the setup</b>[<font color=red>industrial</font>]<b>.</b> <b>Support for google app engine has been added as of version 3.0.0, using the `urlfetch` package instead of `requests`</b>[<font color=red>industrial</font>]<b>.</b> <b>The bindings will also use ``simplejson`` if you happen to have it installed, but that is optional: we fall back to python's built-in json libraries is ``simplejson`` is not found</b>[<font color=red>industrial, </font> <font color=orange>project</font>]<b>.</b> Additional `numpy ` and `scipy ` libraries are needed in case you want to use local predictions for regression models (including the error information) using proportional missing strategy. <b>As these are quite heavy libraries and they are not heavily used in these bindings, they are not included in the automatic installation dependencies</b>[<font color=red>industrial</font>]<b>.</b> <b>The test suite includes some tests that will need these libraries to be installed</b>[<font color=red>industrial</font>]<b>.</b> Also in order to use local `topic model` predictions, you will need to install `pystemmer `. <b>Using the `pip install` command for this library can produce an error if your system lacks the correct developer tools to compile it</b>[<font color=red>industrial</font>]<b>.</b> <b>In windows, the error message will include a link pointing to the needed visual studio version and in osx you'll need to install the xcode developer tools</b>[<font color=red>industrial</font>]<b>.</b> <b>Installation to install the latest stable release with `pip `</b>[<font color=red>industrial</font>]<b>.</b> <b> bash pip install bigml you can also install the development version of the bindings directly from the git repository</b>[<font color=red>industrial</font>]<b>.</b> <b> bash pip installe git: github.com bigmlcom python.git eggbigml_python running the tests the test will be run using `nose ` , that is installed on setup, and you'll need to set up your authentication via environment variables, as explained below</b>[<font color=red>industrial</font>]<b>.</b> <b>With that in place, you can run the test suite simply by issuing</b>[<font color=red>industrial</font>]<b>.</b> <b> bash python setup.py nosetests some tests need the `numpy ` and `scipy ` libraries to be installed too</b>[<font color=red>industrial</font>]<b>.</b> They are not automatically installed as a dependency, as they are quite heavy and very seldom used. <b> python import bigml.api alternatively you can just import the bigml class:</b>[<font color=red>industrial</font>]<b>.</b> <b> python from bigml.api import bigml authentication all the requests to bigml.io must be authenticated using your username and `api key ` and are always transmitted over https</b>[<font color=red>industrial</font>]<b>.</b> <b>This module will look for your username and api key in the environment variables ``bigml_username`` and ``bigml_api_key`` respectively</b>[<font color=red>industrial</font>]<b>.</b> <b>Unix and macos you can add the following lines to your ``.bashrc`` or ``.bash_profile`` to set those variables automatically when you log in:</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> <b> bash export bigml_usernamemyusername export bigml_api_keyae579e7e53fb9abd646a6ff8aa99d4afe83ac291 refer to the next chapters to know how to do that in other operating systems</b>[<font color=red>industrial</font>]<b>.</b> With that environment set up, connecting to bigml is a breeze:. <b> python from bigml.api import bigml api bigml otherwise, you can initialize directly when instantiating the bigml class as follows:</b>[<font color=red>industrial</font>]<b>.</b> <b> python api bigml('myusername', 'ae579e7e53fb9abd646a6ff8aa99d4afe83ac291') these credentials will allow you to manage any resource in your user environment</b>[<font color=red>industrial</font>]<b>.</b> <b>In bigml a user can also work for an ``organization``</b>[<font color=red>industrial</font>]<b>.</b> In this case, the organization administrator should previously assign permissions for the user to access one or several particular projects in the organization. Once permissions are granted, the user can work with resources in a project according to his permission level by creating a special constructor for each project. The connection constructor in this case should include the ``project id``:.  python api bigml('myusername', 'ae579e7e53fb9abd646a6ff8aa99d4afe83ac291', project'project 53739b98d994972da7001d4a') if the project used in a connection object does not belong to an existing organization but is one of the projects under the user's account, all the resources created or updated with that connection will also be assigned to the specified project. When the resource to be managed is a ``project`` itself, the connection needs to include the corresponding``organization id``:. <b> python api bigml('myusername', 'ae579e7e53fb9abd646a6ff8aa99d4afe83ac291', organization'organization 53739b98d994972da7025d4a') authentication on windows the credentials should be permanently stored in your system using</b>[<font color=red>industrial</font>]<b>.</b>  bash setx bigml_username myusername setx bigml_api_key ae579e7e53fb9abd646a6ff8aa99d4afe83ac291 note that ``setx`` will not change the environment variables of your actual console, so you will need to open a new one to start using them. <b>Authentication on jupyter notebook you can set the environment variables using the `` env`` command in your cells:</b>[<font color=red>industrial</font>]<b>.</b>  bash env bigml_usernamemyusername env bigml_api_keyae579e7e53fb9abd646a6ff8aa99d4afe83ac291 alternative domains the main public domain for the api service is ``bigml.io``, but there are some alternative domains, either for virtual private cloud setups or the australian subdomain (``au.bigml.io``). <b>You can change the remote server domain to the vpc particular one by either setting the ``bigml_domain`` environment variable to your vpc subdomain:</b>[<font color=red>industrial</font>]<b>.</b>  bash export bigml_domainmy_vpc.bigml.io or setting it when instantiating your connection:. <b> python api bigml(domain my_vpc.bigml.io ) the corresponding ssl rest calls will be directed to your private domain henceforth</b>[<font color=red>industrial</font>]<b>.</b> <b>You can also set up your connection to use a particular predictserver only for predictions</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> <b>In order to do so, you'll need to specify a ``domain`` object, where you can set up the general domain name as well as the particular prediction domain name</b>[<font color=red>industrial</font>]<b>.</b> <b> python from bigml.domain import domain from bigml.api import bigml domain_info domain(prediction_domain my_prediction_server.bigml.com , prediction_protocol http ) api bigml(domaindomain_info) finally, you can combine all the options and change both the general domain server, and the prediction domain server</b>[<font color=red>industrial</font>]<b>.</b> <b> python from bigml.domain import domain from bigml.api import bigml domain_info domain(domain my_vpc.bigml.io , prediction_domain my_prediction_server.bigml.com , prediction_protocol https ) api bigml(domaindomain_info) some arguments for the domain constructor are more unsual, but they can also be used to set your special service endpoints: protocol (string) protocol for the service (when different from https) verify (boolean) sets on off the ssl verification prediction_verify (boolean) sets on off the ssl verification for the prediction server (when different from the general ssl verification) note that the previously existing ``dev_mode`` flag:</b>[<font color=red>industrial</font>]<b>.</b>  python api bigml(dev_modetrue) that caused the connection to work with the sandbox ``development environment`` has been deprecated because this environment does not longer exist. The existing resources that were previously created in this environment have been moved to a special project in the now unique ``production environment``, so this flag is no longer needed to work with them. Quick start imagine that you want to use `this csv file ` containing the `iris flower dataset ` to predict the species of a flower whose ``petal length`` is ``2.45`` and whose ``petal width`` is ``1.75``. <b>It has 4 numeric fields: ``sepal length``, ``sepal width``, ``petal length``, ``petal width`` and a categorical field: ``species``</b>[<font color=red>industrial</font>]<b>.</b> By default, bigml considers the last field in the dataset as the objective field (i.e, the field that you want to generate predictions for). <b>:: sepal length,sepal width,petal length,petal width,species 5.1,3.5,1.4,0.2,iris-setosa 4.9,3.0,1.4,0.2,iris-setosa 4.7,3.2,1.3,0.2,iris-setosa</b>[<font color=red>industrial</font>]<b>.</b> 5.8,2.7,3.9,1.2,iris-versicolor 6.0,2.7,5.1,1.6,iris-versicolor 5.4,3.0,4.5,1.5,iris-versicolor. <b>6.8,3.0,5.5,2.1,iris-virginica 5.7,2.5,5.0,2.0,iris-virginica 5.8,2.8,5.1,2.4,iris-virginica you can easily generate a prediction following these steps:</b>[<font color=red>industrial</font>]<b>.</b> <b> python from bigml.api import bigml api bigml source api.create_source('. data iris.csv') dataset api.create_dataset(source) model api.create_model(dataset) prediction api.create_prediction(model, \\ petal width : 1.75, petal length : 2.45 ) you can then print the prediction using the ``pprint`` method:</b>[<font color=red>industrial</font>]<b>.</b>  python api.pprint(prediction) species for petal width : 1.75, petal length : 2.45 is iris-setosa certainly, any of the resources created in bigml can be configured using several arguments described in the `api documentation `. <b>Any of these configuration arguments can be added to the ``create`` method as a dictionary in the last optional argument of the calls:</b>[<font color=red>industrial</font>]<b>.</b>  python from bigml.api import bigml api bigml source_args name : my source , source_parser : missing_tokens : null source api.create_source('. data iris.csv', source_args) dataset_args name : my dataset dataset api.create_dataset(source, dataset_args) model_args objective_field : species model api.create_model(dataset, model_args) prediction_args name : my prediction prediction api.create_prediction(model, \\ petal width : 1.75, petal length : 2.45 , prediction_args) the ``iris`` dataset has a small number of instances, and usually will be instantly created, so the ``api.create_`` calls will probably return the finished resources outright. <b>As bigml's api is asynchronous, in general you will need to ensure that objects are finished before using them by using ``api.ok``</b>[<font color=red>industrial</font>]<b>.</b> <b> python from bigml.api import bigml api bigml source api.create_source('. data iris.csv') api.ok(source) dataset api.create_dataset(source) api.ok(dataset) model api.create_model(dataset) api.ok(model) prediction api.create_prediction(model, \\ petal width : 1.75, petal length : 2.45 ) note that the prediction call is not followed by the ``api.ok`` method</b>[<font color=red>industrial</font>]<b>.</b> Predictions are so quick to be generated that, unlike the rest of resouces, will be generated synchronously as a finished object. <b>The example assumes that your objective field (the one you want to predict) is the last field in the dataset</b>[<font color=red>industrial</font>]<b>.</b> If that's not he case, you can explicitly set the name of this field in the creation call using the ``objective_field`` argument:. <b> python from bigml.api import bigml api bigml source api.create_source('. data iris.csv') api.ok(source) dataset api.create_dataset(source) api.ok(dataset) model api.create_model(dataset, objective_field : species ) api.ok(model) prediction api.create_prediction(model, \\ 'sepal length': 5, 'sepal width': 2.5 ) you can also generate an evaluation for the model by using:</b>[<font color=red>industrial</font>]<b>.</b> <b> python test_source api.create_source('. data test_iris.csv') api.ok(test_source) test_dataset api.create_dataset(test_source) api.ok(test_dataset) evaluation api.create_evaluation(model, test_dataset) api.ok(evaluation) if you set the ``storage`` argument in the ``api`` instantiation:</b>[<font color=red>industrial</font>]<b>.</b> <b> python api bigml(storage'. storage') all the generated, updated or retrieved resources will be automatically saved to the chosen directory</b>[<font color=red>industrial</font>]<b>.</b> <b>Alternatively, you can use the ``export`` method to explicitly download the json information that describes any of your resources in bigml to a particular file:</b>[<font color=red>industrial</font>]<b>.</b> <b> python api.export('model 5acea49a08b07e14b9001068', filename my_dir my_model.json ) this example downloads the json for the model and stores it in the ``my_dir my_model.json`` file</b>[<font color=red>industrial</font>]<b>.</b> <b>In the case of models that can be represented in a `pmml` syntax, the export method can be used to produce the corresponding `pmml` file</b>[<font color=red>industrial</font>]<b>.</b> <b> python api.export('model 5acea49a08b07e14b9001068', filename my_dir my_model.pmml , pmmltrue) you can also retrieve the last resource with some previously given tag:</b>[<font color=red>industrial</font>]<b>.</b> <b> python api.export_last( foo , resource_type ensemble , filename my_dir my_ensemble.json ) which selects the last ensemble that has a ``foo`` tag</b>[<font color=red>industrial</font>]<b>.</b> <b>This mechanism can be specially useful when retrieving retrained models that have been created with a shared unique keyword as tag</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> For a descriptive overview of the steps that you will usually need to follow to model your data and obtain predictions, please see the `basic workflow sketch ` document. You can also check other simple examples in the following documents: `model 101 ` `logistic regression 101 ` `linear regression 101 ` `ensemble 101 ` `cluster 101 ` `anomaly detector 101 ` `association 101 ` `topic model 101 ` `deepnet 101 ` `time series 101 ` `fusion 101 ` `scripting 101 ` additional information we've just barely scratched the surface. <b>Alternatively, the same documentation can be built from a local checkout of the source by installing `sphinx ` (`` pip install sphinx``) and then running</b>[<font color=red>industrial</font>]<b>.</b> <b> bash cd docs make html then launch ``docs _build html index.html`` in your browser</b>[<font color=red>industrial</font>]<b>.</b> <b>For details on the underlying api, see the `bigml api documentation `</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 47 ************************************\n",
      "Number of words: 68\n",
      "Number of sentences: 3\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 1\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/loli/medpy/blob/master/README.md\n",
      "URL: https://github.com/loli/medpy/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " Contact(oskar.maier@gmail.com) medpy medical image processing in python medpy is an image processing library and collection of scripts targeted towards medical (i.e high dimensional) image processing. <b>Stable releases download (stable release): html documentation and installation instruction (stable release): development version download (development version): html documentation and installation instruction (development version): create this from doc folder following instructions in contained readme file python 2 version python 2 is no longer supported</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 48 ************************************\n",
      "Number of words: 17\n",
      "Number of sentences: 2\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 1\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/sebadorn/Machine-Learning--Connect-Four/blob/master/README.md\n",
      "URL: https://github.com/sebadorn/Machine-Learning--Connect-Four/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Course: machine learning project: training different types of artificial neural networks in the game of connect four</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 49 ************************************\n",
      "Number of words: 155\n",
      "Number of sentences: 6\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 2\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/comic/grand-challenge.org/blob/master/README.md\n",
      "URL: https://github.com/comic/grand-challenge.org/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b> and objective comparisons of machine learning algorithms improves the quality of research outputs in both academia and industry</b>[<font color=red>industrial</font>]<b>.</b> <b>This repo contains the source code behind `grand-challenge.org `, which serves as a resource for users to compare algorithms in biomedical image analysis</b>[<font color=red>industrial</font>]<b>.</b> This instance is maintained by developers at radboud university medical center in nijmegen, the netherlands and fraunhofer mevis in bremen, germany, but you can also create your own instance. This django powered website has been developed by the consortium for open medical image computing. It features: creation and management of challenges easy creation of challenge sites with wysiwyg editing fine grained permissions for challenge administrators and participants management and serving of datasets automated evaluation of predictions live leaderboards user profiles and social authentication teams if you would like to start your own website, or contribute to the development of the framework, please see `the docs ` slack you can join the development slack `using this link `."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 50 ************************************\n",
      "Number of words: 544\n",
      "Number of sentences: 35\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 21\n",
      "Total inspired: 0\n",
      "Total market: 1\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/yassersouri/classify-text/blob/master/README.md\n",
      "URL: https://github.com/yassersouri/classify-text/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Salam text classification with python this is an experiment</b>[<font color=red>industrial</font>]<b>.</b> <b>Dataset for dataset i used the famous twenty newsgrousps dataset</b>[<font color=red>industrial</font>]<b>.</b> <b>You can find the dataset freely here i've included a subset of the dataset in the repo, located at `dataset\\` directory</b>[<font color=red>industrial</font>]<b>.</b> <b>This subset includes 6 of the 20 newsgroups: `space`, `electronics`, `crypt`, `hockey`, `motorcycles` and `forsale`</b>[<font color=red>industrial</font>]<b>.</b> When you run `main.py` it asks you for the root of the dataset. <b>You can supply your own dataset assuming it has a similar directory structure</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> <b>Utf-8 incompatibility some of the supplied text files had incompatibility with utf-8 even textedit.app can't open those files</b>[<font color=red>industrial</font>]<b>.</b> Requirements python 2.7 python modules: scikit-learn (v 0.11) scipy (v 0.10.1) colorama termcolor matplotlib (for use in `plot.py`) the code the code is pretty straight forward and well documented. <b>Running the code \\tpython main.py experiments for experiments i used the subset of the dataset (as described above)</b>[<font color=red>industrial</font>]<b>.</b> I assume that we like `hockey`, `crypt` and `electronics` newsgroups, and we dislike the others. <b>For each experiment we use a feature vector , a classifier and a train-test splitting strategy</b>[<font color=red>industrial</font>]<b>.</b> Experiment 1: bow nb 20 test in this experiment we use a bag of words (bow) representation of each document. <b>We split the data, so that 20 of them remain for testing</b>[<font color=red>industrial</font>]<b>.</b> <b>results: experiment 2: tf nb 20 test in this experiment we use a term frequency (tf) representation of each document</b>[<font color=red>industrial</font>]<b>.</b> <b>results: experiment 3: tfidf nb 20 test in this experiment we use a tfidf representation of each document</b>[<font color=red>industrial</font>]<b>.</b> results: experiment 4: tfidf svm 20 test in this experiment we use a tfidf representation of each document. <b>And also a linear support vector machine (svm) classifier</b>[<font color=red>industrial</font>]<b>.</b> results: experiment 5: tfidf svm kfold in this experiment we use a tfidf representation of each document. <b>We split the data using stratified k-fold algorithm with k 5</b>[<font color=red>industrial</font>]<b>.</b> results: experiment 5: bow nb kfold in this experiment we use a tfidf representation of each document. results: experiment 6: tfidf svm 90 test in this experiment we use a tfidf representation of each document. We split the data, so that 90 of them remain for testing only 10 of the dataset is used for training results: experiment 7: tfidf svm kfold 20 classes in this experiment we use a tfidf representation of each document. <b>We also use the whole twenty newsgroups dataset, which has 20 classes</b>[<font color=red>industrial</font>]<b>.</b> results: experiment 7: bow nb kfold 20 classes in this experiment we use a bag of words (bow) representation of each document. <b>results: experiment 8: tfidf 5-nn distance weights 20 test in this experiment we use a tfidf representation of each document</b>[<font color=red>industrial</font>]<b>.</b> <b>And also a k nearest neighbors (knn) classifier with k 5 and distance weights</b>[<font color=red>industrial</font>]<b>.</b> <b>results: experiment 9: tfidf 5-nn uniform weights 20 test in this experiment we use a tfidf representation of each document</b>[<font color=red>industrial</font>]<b>.</b> <b>And also a k nearest neighbors (knn) classifier with k 5 and uniform weights</b>[<font color=red>industrial</font>]<b>.</b> <b>results: experiment 10: tfidf 5-nn distance weights kfold in this experiment we use a tfidf representation of each document</b>[<font color=red>industrial</font>]<b>.</b> <b>results: experiment 11: tfidf 5-nn distance weights kfold 20 classes in this experiment we use a tfidf representation of each document</b>[<font color=red>industrial</font>]<b>.</b> <b>results: so what? this experiments show that text classification can be effectively done by simple tools like tfidf and svm</b>[<font color=red>industrial</font>]<b>.</b> Any conclusion? we have found that tfidf with svm have the best performance. Tfidf with svm perform well both for 2-class problem and 20-class problem. I would say if you want suggestion from me, use tfidf with svm.'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 51 ************************************\n",
      "Number of words: 312\n",
      "Number of sentences: 20\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 15\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/profjsb/python-seminar/blob/master/README.md\n",
      "URL: https://github.com/profjsb/python-seminar/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>B python computing for data science a graduate seminar course at uc berkeley (ay 250) campbell hall: monday 2 5 pm spring 2018 synopsis python has become the de facto superglue language for modern scientific computing in this course we will learn pythonic interactions with databases, imaging processing, advanced statistical and numerical packages, web frameworks, machine-learning, and parallelism</b>[<font color=red>industrial</font>]<b>.</b> <b>Each week will involve lectures and coding projects</b>[<font color=red>industrial</font>]<b>.</b> <b>In the final project, students will build a working codebase useful for their own research domain</b>[<font color=red>industrial</font>]<b>.</b> This class is for any student working in a quantitative discipline and with familiarity with python. <b>Those who completed the python bootcamp or equivalent will be eligible</b>[<font color=red>industrial</font>]<b>.</b> <b>You should follow the steps to install the anaconda 3.6.x distribution as well as git</b>[<font color=red>industrial</font>]<b>.</b> <b>Advanced python language concepts (decorators, ordereddict, generators, iterables, context managers)</b>[<font color=red>industrial</font>]<b>.</b> Scipy sect; sect; 1.3 1.5 2.2 numpy skim chap 4 5 of mckinney. <b>Data vizualization (matplotlib, bokeh, altair, plotly, mayavi)</b>[<font color=red>industrial</font>]<b>.</b> <b>Database interaction (sqlite, postgres, sqlalchemy, peewee),large datasets (xarray, hdf5)</b>[<font color=red>industrial</font>]<b>.</b> <b>Stefan van der walt apr 9\\t.\\tbayesian programming symbolic math\\t</b>[<font color=red>industrial</font>]<b>.</b> Useful books elegant scipy (uc berkeley library link) mybinder version clean notebooks sidebar concepts throughout these lectures we will be peppering in sidebar knowledge concepts: jupyter juypterlab using git github docker data science workflows reproducible research application building debugging testing workflow each monday we will be introducing a resonably self-contained topic with two back-to-back lectures. <b>In between a short ( 20 minute) breakout coding session will be conducted</b>[<font color=red>industrial</font>]<b>.</b> <b>Homeworks will require you to write a large (several hundred line) codebase</b>[<font color=red>industrial</font>]<b>.</b> <b>Help sessions will be conducted interactively on the piazza site for the course</b>[<font color=red>industrial</font>]<b>.</b> <b>There is also an in-person help session every tuesday from 11am-noon at bids (in doe library)</b>[<font color=red>industrial</font>]<b>.</b> <b>Email josh(mailto:joshbloom@berkeley.edu) with any questions</b>[<font color=red>industrial</font>]<b>.</b> <b>Contact email us at ucbpythonclass@gmail.com(mailto:ucbpythonclass@gmail.com) or contact the professor directly you can also contact the gsi, chelsea harris, at (chelseaharris@berkeley.edu(chelseaharris@berkeley.edu)</b>[<font color=red>industrial</font>]<b>.</b> Auditing is not permitted by the university but those wishing to sit in on a class or two should contact the professor before attending."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 52 ************************************\n",
      "Number of words: 256\n",
      "Number of sentences: 8\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 2\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/mne-tools/mne-cpp/blob/master/README.md\n",
      "URL: https://github.com/mne-tools/mne-cpp/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "  clang 3.5 | dependencies qt 5.10 list of contact persons can be found here list: the latest release code from the mne-cpp releases page or the binaries from the mne-cpp download page is available under the bsd-3-clause open source license: copyright (c) 2010-2019, authors of mne-cpp. <b>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: 1</b>[<font color=red>industrial</font>]<b>.</b> <b>Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer</b>[<font color=red>industrial</font>]<b>.</b> Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and or other materials provided with the distribution. Neither the name of the mne-cpp authors nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. This software is provided by the copyright holders and contributors as is and any express or implied warranties, including, but not limited to, the implied warranties of merchantability and fitness for a particular purpose are disclaimed. In no event shall the copyright holder or contributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages (including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits; or business interruption) however caused and on any theory of liability, whether in contract, strict liability, or tort (including negligence or otherwise) arising in any way out of the use of this software, even if advised of the possibility of such damage."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 53 ************************************\n",
      "Number of words: 216\n",
      "Number of sentences: 15\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 11\n",
      "Total inspired: 0\n",
      "Total market: 1\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/PRML/PRMLT/blob/master/README.md\n",
      "URL: https://github.com/PRML/PRMLT/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Introduction this matlab package implements machine learning algorithms described in the great textbook: pattern recognition and machine learning by c</b>[<font color=red>industrial</font>]<b>.</b> <b>Note: this package requires matlab r2016b or latter, since it utilizes a new matlab syntax called implicit expansion (a.k.a</b>[<font color=red>industrial</font>]<b>.</b> <b>It also requires statistics toolbox (for some simple random number generator) and image processing toolbox (for reading image data)</b>[<font color=red>industrial</font>]<b>.</b> Design goal succinct: the code is extremely compact. <b>As a result, the core of the algorithms can be easily spotted</b>[<font color=red>industrial</font>]<b>.</b> <b>Efficient: many tricks to speedup matlab code are applied (eg</b>[<font color=red>industrial</font>]<b>.</b> <b>Usually, functions in this package are orders faster than matlab builtin ones (e.g kmeans)</b>[<font color=red>industrial</font>]<b>.</b> <b>Robust: many tricks for numerical stability are applied, such as computing probability in log domain, square root matrix update to enforce matrix symmetry\\pd, etc</b>[<font color=red>industrial</font>]<b>.</b> <b>Practical: the package is not only readable, but also meant to be easily used and modified to facilitate ml research</b>[<font color=red>industrial</font>]<b>.</b> <b>Many functions in this package are already widely used (see matlab file exchange download the package to a local folder (e.g prmlt ) by running: 2</b>[<font color=red>industrial</font>]<b>.</b> <b>Run matlab and navigate to the folder ( prmlt ), then run the init.m script</b>[<font color=red>industrial</font>]<b>.</b> <b>Enjoy feedback if you find any bug or have any suggestion, please do file issues</b>[<font color=violet>market</font>]<b>.</b> I am graceful for any feedback and will do my best to improve this package. <b>License released under mit license contact sth4nth at gmail dot com '</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 54 ************************************\n",
      "Number of words: 285\n",
      "Number of sentences: 11\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 3\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 1\n",
      "Total renown: 0\n",
      "URL: https://github.com/wvrossem/FIPS-Android-Offline/blob/master/README.md\n",
      "URL: https://github.com/wvrossem/FIPS-Android-Offline/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " Fips-android-offline project overview this repository is part of the project an extensible framework for indoor positioning on mobile devices , which is the master thesis that i did in 2011-2012 at the vrije universiteit brussel to achieve my master in applied computer science. <b>Beat signer the thesis document can be found here entire project is divided into several repositories: fips-datastore fips-server fips-tool fips-android-offline fips-android-online offline application usage this is an android application to collect fingerprints for a wlan indoor positioning system</b>[<font color=red>industrial</font>]<b>.</b> <b>The easiest way to use this project is to open it as an eclipse project and running it from there</b>[<font color=orange>project</font>]<b>.</b> The application is dependent on the fips-datastore for the data entities so the easiest way to resolve the dependencies is to link the projects in eclipse (or alternatively put the src files and libraries in the android project). The application works as follows: select a grid that denotes your location on the map and a wlan scan will be initiated. <b>The results from this scan are then saved to the folder `ips_samples` on your device</b>[<font color=red>industrial</font>]<b>.</b> <b>A menu to configure some settings is also provided</b>[<font color=red>industrial</font>]<b>.</b> The main interface of the application main interface main interface ) wifi scan in progress wifi scan wifi scan ) the settings menu settings menu settings menu ) license this program is free software: you can redistribute it and or modify it under the terms of the gnu general public license as published by the free software foundation, either version 3 of the license, or (at your option) any later version. This program is distributed in the hope that it will be useful, but without any warranty; without even the implied warranty of merchantability or fitness for a particular purpose. See the gnu general public license for more details."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 55 ************************************\n",
      "Number of words: 252\n",
      "Number of sentences: 10\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 5\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/wvrossem/FIPS-Android-Online/blob/master/README.md\n",
      "URL: https://github.com/wvrossem/FIPS-Android-Online/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " Fips-android-online project overview this repository is part of the project an extensible framework for indoor positioning on mobile devices , which is the master thesis that i did in 2011-2012 at the vrije universiteit brussel to achieve my master in applied computer science. <b>Beat signer the thesis document can be found here entire project is divided into several repositories: fips-datastore fips-server fips-tool fips-android-offline fips-android-online online application usage this is an android application to do positioning with a wlan indoor positioning system</b>[<font color=red>industrial</font>]<b>.</b> <b>It provides the posiblity for a user to locate himself at an indoor location</b>[<font color=red>industrial</font>]<b>.</b> <b>It requires that a fips-server is running with data in the fips-datastore that was collected with fips-android-offline application</b>[<font color=red>industrial</font>]<b>.</b> <b>In order for users to position themselves, a button is provided to iniate a wifi scan and send a request to the server</b>[<font color=red>industrial</font>]<b>.</b> <b>A settings menu is also available to configure what algorithm to run on the server</b>[<font color=red>industrial</font>]<b>.</b> The main interface of the application main interface main interface ) wifi scan in progress wifi scan wifi scan ) the settings menu settings menu settings menu ) license this program is free software: you can redistribute it and or modify it under the terms of the gnu general public license as published by the free software foundation, either version 3 of the license, or (at your option) any later version. This program is distributed in the hope that it will be useful, but without any warranty; without even the implied warranty of merchantability or fitness for a particular purpose. See the gnu general public license for more details."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 56 ************************************\n",
      "Number of words: 236\n",
      "Number of sentences: 9\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 3\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/wvrossem/FIPS/blob/master/README.md\n",
      "URL: https://github.com/wvrossem/FIPS/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " Fips project overview this repository is part of the project an extensible framework for indoor positioning on mobile devices , which is the master thesis that i did in 2011-2012 at the vrije universiteit brussel to achieve my master in applied computer science. <b>Beat signer the thesis document can be found here entire project is divided into several repositories: fips-datastore fips-server fips-tool fips-android-offline fips-android-online sample data this repository contains all the fingerprint samples that were collected during the project in the hope that they can be helpful to test the framework or can be used in other other projects</b>[<font color=red>industrial</font>]<b>.</b> Only the data in `new_format` is relevant to test the current framework, but the other formats could still be used in other projects (or transformed to the new fomat). <b>The fingerprinting was done at the brussels south railway station using the fips-android-offline application, running on a samsung galaxy s ii device</b>[<font color=red>industrial</font>]<b>.</b> <b>The data in `new_format` was captured at maps full, medium and small</b>[<font color=red>industrial</font>]<b>.</b> License this program is free software: you can redistribute it and or modify it under the terms of the gnu general public license as published by the free software foundation, either version 3 of the license, or (at your option) any later version. This program is distributed in the hope that it will be useful, but without any warranty; without even the implied warranty of merchantability or fitness for a particular purpose. See the gnu general public license for more details."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 57 ************************************\n",
      "Number of words: 1384\n",
      "Number of sentences: 35\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 21\n",
      "Total inspired: 0\n",
      "Total market: 1\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/kermitt2/grobid/blob/master/README.md\n",
      "URL: https://github.com/kermitt2/grobid/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Grobid latest docker hub image ) docker pulls ) grobid documentation visit the grobid documentation for more detailed information</b>[<font color=red>industrial</font>]<b>.</b> Purpose grobid (or grobid, but not grobid nor grobid) means generation of bibliographic data. <b>Grobid is a machine learning library for extracting, parsing and re-structuring raw documents such as pdf into structured xml tei encoded documents with a particular focus on technical and scientific publications</b>[<font color=red>industrial</font>]<b>.</b> <b>In 2011 the tool has been made available in open source</b>[<font color=red>industrial</font>]<b>.</b> <b>Work on grobid has been steady as a side project since the beginning and is expected to continue until at least 2020 :) the following functionalities are available: + header extraction and parsing from article in pdf format</b>[<font color=red>industrial</font>]<b>.</b> <b>The extraction here covers the usual bibliographical information (e.g title, abstract, authors, affiliations, keywords, etc.)</b>[<font color=red>industrial</font>]<b>.</b> <b>+ references extraction and parsing from articles in pdf format</b>[<font color=red>industrial</font>]<b>.</b> The different citation contexts in an article are recognized and linked to the full bibliographical references. + extraction of patent and non-patent references in patent publications. + parsing of names (e.g person title, fornames, middlename, etc.), in particular author names in header, and author names in references (two distinct models). <b>+ parsing of dates (iso normalized day, month, year)</b>[<font color=red>industrial</font>]<b>.</b> + full text extraction from pdf articles, including a model for the the overall document segmentation and a model for the structuring of the text body (paragraph, section titles, reference callout, figure, table, etc.). <b>+ in a complete pdf processing, grobid manages 55 final labels used to build relatively fine-grained structures, from traditional publication metadata (title, author first last middlenames, affiliation types, detailed address, journal, volume, issue, pages, etc.) to full text structures (section title, paragraph, reference markers, head foot notes, figure headers, etc.)</b>[<font color=red>industrial</font>]<b>.</b> <b>+ consolidation resolution of the extracted bibliographical references using the biblio-glutton service or the crossref rest api grobid includes a comprehensive web service api, batch processing, a java api, a docker image, a relatively generic evaluation framework (precision, recall, etc.) and the semi-automatic generation of training data</b>[<font color=red>industrial</font>]<b>.</b> Deployments in production includes researchgate, hal research archive, the european patent office, inist-cnrs, mendeley, cern (invenio), and many more. <b>Grobid should run properly out of the box on linux (64 bits), macos, and windows (32 and 64 bits)</b>[<font color=red>industrial</font>]<b>.</b> <b>For more information on how the tool works, on its key features and performance, visit the grobid documentation demo for testing purposes, a public grobid demo server is available at the following address: web services are documented here some quota and query limitation apply to the demo server please be courteous and do not overload the demo server</b>[<font color=red>industrial</font>]<b>.</b> <b>Clients for helping to exploit grobid service at scale, we provide clients written in python, java, node.js using the web services for parallel batch processing: python grobid client java grobid client node.js grobid client all these clients will take advantage of the multi-threading for scaling large set of pdf processing</b>[<font color=red>industrial</font>]<b>.</b> As a consequence, they will be much more efficient than the batch command lines (which use only one thread) and should be prefered. <b>We have been able recently to run the complete fulltext processing at around 10.6 pdf per second (around 915,000 pdf per day, around 20m pages per day) with the node.js client listed above during one week on a 16 cpu machine (16 threads, 32gb ram, no sdd, articles from mainstream publishers), see here issuecomment-505208132)</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> <b>In addition, a java example project is available to illustrate how to use grobid as a java library: the example project is using grobid java api for extracting header metadata and citations from a pdf and output the results in bibtex format</b>[<font color=red>industrial</font>]<b>.</b> Grobid modules a series of additional modules have been developed for performing structure aware text mining directly on scholar pdf, reusing grobid's pdf processing and sequence labelling weaponery: grobid-ner named entity recognition grobid-astro recognition of astronomical entities in scientific papers grobid-quantities recognition and normalization of physical quantities measurements software-mention recognition of software mentions and attributes in scientific literature grobid-bio a bio-entity tagger using bionlp nlpba 2004 dataset grobid-dictionaries structuring dictionaries in raw pdf format grobid-superconductors recognition of superconductor material and properties in scientific literature latest version the latest stable release of grobid is version. This version brings: + using pdfalto instead of pdf2xml for the first pdf parsing stage, with many improvements in robustness, icu support, unknown glyph font normalization + improvement and full review of the integration of consolidation services, supporting biblio-glutton (additional identifiers and open access links) and crossref rest api (add specific user agent, email and token for crossref metadata plus) + fix bounding box issues for some pdf 330 + updated lexicon 396 new in previous release : + transparent usage of delft deep learning models (bidlstm-crf) instead of wapiti crf models, native integration via jep support of biblio-glutton as doi metadata matching service, alternative to crossref rest api + improvement of citation context identification and matching (+9 recall with similar precision, for pmc sample 1943 articles, from 43.35 correct citation contexts per article to 49.98 correct citation contexts per article) + citation callout now in abstract, figure and table captions + structured abstract (including update of tei schema) + bug fixes and some more parameters: by default using all available threads when training (thanks @de-code and possibility to load models at the start of the service (more information in the release page) new in previous release : + improvement of consolidation options and processing (better handling of crossref api, but the best is coming soon ;) + better recall for figure and table identification (thanks to @detonator413) + support of proxy for calling crossref with apache httpclient (more information in the release page) new in previous release : + corrected back status codes from the rest api when no available engine (503 is back again to inform the client to wait, it was removed by error in version 0.5.0 and 0.5.1 for pdf processing services only, see documentation of the rest api) + added grobid clients clients-for-grobid-web-services) for java, python and nodejs + added metrics in the rest entrypoint (accessible via bugfixing (more information in the release page) new in previous release : + migrate from maven to gradle for faster, more flexible and more stable build, release, etc. <b>+ usage of dropwizard for web services + move the grobid service manual to readthedocs (thanks to @detonator413 and @lfoppiano for this release future work in versions 0.5</b>[<font color=red>industrial</font>]<b>.</b> Will focus again on improving pdf parsing and structuring accuracy) (more information in the release page) new in previous release : + new models: f-score improvement on the pubmed central sample, bibliographical references +2.5 , header +7 + new training data and features for bibliographical references, in particular for covering hep domain (inspire), arxiv identifier, doi and url (thanks @iorala and @michamos ) + support for crossref rest api (instead of the slow openurl-style api which requires a crossref account), in particular for multithreading usage (thanks @vi-dot) + improve training data generation and documentation (thanks @jfix) + unicode normalisation and more robust body extraction (thanks @aoboturov) + fixes, tests, documentation and update of the pdf2xml fork for windows (thanks @lfoppiano) (more information in the release page) new in previous release : + f-score improvement for the pubmed central sample: fulltext +10-14 , header +0.5 , citations +0.5 + more robust pdf parsing + identification of equations (with pdf coordinates) + end-to-end evaluation with pub2tei conversions + many fixes and refactoring new in previous release : + support for windows thanks to the contributions of christopher boumenot + support to docker. <b>+ new web services for pdf annotation and updated web console application</b>[<font color=red>industrial</font>]<b>.</b> + some improvements on figure table extraction but still experimental at this stage (work in progress, as the whole full text model). <b>New in previous release : + improvement of the recognition of citations thanks to refinements of crf features +4 in f-score for the pubmed central sample</b>[<font color=red>industrial</font>]<b>.</b> <b>+ improvement of the full text model, with new features and the introduction of two additional models for figures and tables</b>[<font color=red>industrial</font>]<b>.</b> <b>+ more robust synchronization of crf sequence with pdf areas, resulting in improved bounding box calculations for locating annotations in the pdf documents</b>[<font color=red>industrial</font>]<b>.</b> <b>+ improved general robustness thanks to better token alignments</b>[<font color=red>industrial</font>]<b>.</b> License grobid is distributed under apache 2.0 license main author and contact: patrice lopez sponsors ej-technologies provided us a free open-source license for its java profiler. Reference for citing this work, you can refer to the present github project, together with the software heritage project-level permanent identifier. <b>For example, with bibtex: see the grobid documentation for more related resources</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 58 ************************************\n",
      "Number of words: 497\n",
      "Number of sentences: 24\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 18\n",
      "Total inspired: 0\n",
      "Total market: 4\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/crcollins/chemtools-webapp/blob/master/README.md\n",
      "URL: https://github.com/crcollins/chemtools-webapp/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>B chemtools-webapp django webapp built around the functionality of chemtools deprecated this includes various utilities for parsing gaussian log files, creating molecular structures, submitting jobs to torque clusters, and predicting optoelectronic properties of benzobisazoles</b>[<font color=red>industrial</font>]<b>.</b> <b>These tools can be used through a django interface, or some of the tools can also just be used on the command line</b>[<font color=red>industrial</font>]<b>.</b> <b>Setup assumes you are on a machine with the apt package manager</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> <b>This will proceed to install the required dependencies and setup an nginx server to serve chemtools</b>[<font color=red>industrial</font>]<b>.</b> <b>Once this is done, you should be able to see it in your browser at cd chemtools-webapp source install.sh to remove chemtools, run the following commands</b>[<font color=red>industrial</font>]<b>.</b> <b>Cd chemtools-webapp source install.sh remove in addition to the parameter given to install.sh, there are also three environment variables that get used</b>[<font color=red>industrial</font>]<b>.</b> <b>They are `install_user`, `chemtools_dir`, and `https`</b>[<font color=red>industrial</font>]<b>.</b> <b>`install_user` is used to set the user that chemtools will install with (if no value is given this will default to `user`)</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> , `chemtools_dir` sets the path where chemtools is located (by default this is set to be the current directory). <b>`https` specifices whether or not to use the https nginx config or not</b>[<font color=red>industrial</font>]<b>.</b> <b>By giving `https` any non null value it will use the https configuration</b>[<font color=red>industrial</font>]<b>.</b> Vagrant deploy this assumes that you already have vagrant and virtualbox installed. <b>Cd chemtools-webapp vagrant vagrant up warning: this includes the test key from project media tests by default</b>[<font color=red>industrial</font>]<b>.</b> <b>This key must be removed if you plan on opening this server up to the internet</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> If you want to change the port, just change the value in the vagrant vagrantfile and reboot the vm. <b>Go to spin up a vm from the local copy of chemtools-webapp can set the environmental variable `dev` to `true` devtrue vagrant up development setup there are two ways to get a dev setup, either you can do a vagrant deployment, or you can run the install.sh script with the `dev` option</b>[<font color=red>industrial</font>]<b>.</b> <b>Cd chemtools-webapp source install.sh dev python manage.py runserver 0.0.0.0:8000 go to run the test server with ssl, you can run the following command: python manage.py runsslserver\\ keyproject media tests server.key\\ certificateproject media tests server.crt 0.0.0.0:8000 go to you want to get rid of the warning message when you connect to it, you can add the certificate to your browser's certificate manager</b>[<font color=violet>market</font>]<b>.</b> <b>Test currently, there are a few tests as a sanity check for some of the main features of chemtools</b>[<font color=red>industrial</font>]<b>.</b> <b>Note: for some of the tests to pass they require a test torque cluster</b>[<font color=red>industrial</font>]<b>.</b> <b>This requirement can be satisfied using this repository which contains a vagrant setup of a basic cluster</b>[<font color=red>industrial</font>]<b>.</b> <b>Note: these tests also assume that the test cluster is at localhost port 2222</b>[<font color=red>industrial</font>]<b>.</b> <b>Python manage.py test account chem chemtools cluster data docs if you have coverage.py installed (if not you can install with `pip install coverage`), you can also run the following commands to see which parts of the code are covered by the tests</b>[<font color=red>industrial</font>]<b>.</b> Coverage run manage.py test account chem chemtools cluster data docs coverage html database the database used can be changed in by going in project settings.py and changing the database dictionary."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 59 ************************************\n",
      "Number of words: 603\n",
      "Number of sentences: 18\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 13\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/CeON/CERMINE/blob/master/README.md\n",
      "URL: https://github.com/CeON/CERMINE/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Content extractor and miner cermine is a java library and a web service for extracting metadata and content from pdf files containing academic publications</b>[<font color=red>industrial</font>]<b>.</b> Cermine is written in java at centre for open science at interdisciplinary centre for mathematical and computational modelling university of warsaw code is licensed under gnu affero general public license version 3. How to cite cermine: \\tdominika tkaczyk, pawel szostek, mateusz fedoryszak, piotr jan dendek and lukasz bolikowski. <b>\\tcermine: automatic extraction of structured metadata from scientific literature</b>[<font color=red>industrial</font>]<b>.</b> \\tin international journal on document analysis and recognition (ijdar), 2015, \\tvol. <b>Doi of cermine release 1.13: cermine cermine can be used for: extracting metadata, full text and parsed references from a pdf file, extracting metadata from reference strings, extracting metadata from affiliation strings</b>[<font color=red>industrial</font>]<b>.</b> <b>In all tasks the default output format is nlm jats are three way of using cermine, depending on the user's needs: standalone application use this, if you need to process larger amounts of data locally on your laptop or server maven dependency allows to use cermine's api in your own java scala code web application for demonstration purposes and only small amounts (less than 50 files) of data refer to one of the sections below for details</b>[<font color=red>industrial</font>]<b>.</b> <b>Standalone application the easiest way to process files on a laptop server is using cermine as a standalone application</b>[<font color=red>industrial</font>]<b>.</b> <b>All you will need is a single jar file containing all the tools, external libraries and learned models</b>[<font color=red>industrial</font>]<b>.</b> <b>The latest release can be downloaded from the repository artifacts browse simple general kdd-releases pl edu icm cermine cermine-impl) (look for a file called `cermine-impljar-with-dependencies.jar`)</b>[<font color=red>industrial</font>]<b>.</b> <b>The current version is 1.13 pdf documents the basic command for processing pdf files is the following: javacp cermine-impljar-with-dependencies.jar pl.edu.icm.cermine.contentextractorpath path to directory with pdfs additional argument -outputs can be used to specify the types of the outputs</b>[<font color=red>industrial</font>]<b>.</b> <b>The value should be a comma-separated list of one or more of the following: jats document metadata and content in nlm jats format text raw document text with the reading order preserved zones text zones of the documents labeled with functional classes trueviz geometric structure of the document in trueviz format images images from the document bibtex references in bibtex format processing references to extract metadata from a reference string use the following: javacp cermine-impljar-with-dependencies.jar pl.edu.icm.cermine.metadata.affiliation.crfaffiliationparseraffiliation the text of the affiliation (optional) if you would like to build an executable jar yourself, clone the project and execute: cd cermine cermine-impl mvn compile assembly:single this will result in a file `cermine-impljar-with-dependencies.jar` in `cermine-impl target` directory</b>[<font color=red>industrial</font>]<b>.</b> <b>Maven dependency cermine can be used in java projects by adding the following dependency and repository to the project's `pom.xml` file: \\t \\t\\tpl.edu.icm.cermine \\t\\tcermine-impl \\t\\t cermine.version \\t \\t \\t\\ticm \\t\\ticm repository \\t\\t code to extract the content from a pdf file: \\tcontentextractor extractor new contentextractor; \\tinputstream inputstream new fileinputstream( path to pdf file ); \\textractor.setpdf(inputstream); \\telement result extractor.getcontentasnlm; example code to extract metadata from a reference string: \\tcrfbibreferenceparser parser crfbibreferenceparser.getinstance; \\tbibentry reference parser.parsebibreference(referencetext); example code to extract metadata from an affiliation string: \\t \\tcrfaffiliationparser parser new crfaffiliationparser; \\telement affiliation parser.parse(affiliationtext); rest service the third possibility is to use cermine's rest service with curl tool</b>[<font color=red>industrial</font>]<b>.</b> Note, however, that this should only be used for small amounts of data, as the server does not have a lot of resources. <b>Moreover, the web application might not use the latest code version</b>[<font color=red>industrial</font>]<b>.</b> <b>In most cases using the executable jar is a better choice</b>[<font color=red>industrial</font>]<b>.</b> <b>To extract the content from a pdf file: \\t curlx post data-binary @article.pdf \\ \\t header content-type: application binary \\ \\t extract metadata from a reference string: \\t curlx post data referencethe text of the reference \\ \\t extract metadata from an affiliation string: \\t curlx post data affiliationthe text of the affiliation \\ \\t</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 60 ************************************\n",
      "Number of words: 1586\n",
      "Number of sentences: 67\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 49\n",
      "Total inspired: 0\n",
      "Total market: 2\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/lisitsyn/tapkee/blob/master/README.md\n",
      "URL: https://github.com/lisitsyn/tapkee/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>B tapkee is a c++ template library for dimensionality reduction with some bias on spectral methods</b>[<font color=red>industrial</font>]<b>.</b> <b>The tapkee origins from the code developed during gsoc 2011 as the part of the shogun machine learning toolbox project aim is to provide efficient and flexible standalone library for dimensionality reduction which can be easily integrated to existing codebases</b>[<font color=red>industrial</font>]<b>.</b> <b>Tapkee leverages capabilities of effective eigen3 linear algebra library and optionally makes use of the arpack eigensolver library uses covertree and vp-tree data structures to compute nearest neighbors</b>[<font color=red>industrial</font>]<b>.</b> <b>To achieve greater flexibility we provide a callback interface which decouples dimension reduction algorithms from the data representation and storage schemes</b>[<font color=red>industrial</font>]<b>.</b> <b>The library is distributed under permissive bsd 3-clause license a few rather optional parts that are distributed under other open sources licenses, see licensing section of this document)</b>[<font color=red>industrial</font>]<b>.</b> If you use this software in any publication we would be happy if you cite the following paper: \\tsergey lisitsyn and christian widmer and fernando j. <b>Journal of machine learning research, 14: 2355-2359, 2013</b>[<font color=red>industrial</font>]<b>.</b> <b>To get started with dimension reduction you may try the borsch script embeds common datasets (swissroll, helix, scurve) using the tapkee library and outputs it with the help of matplotlib library</b>[<font color=red>industrial</font>]<b>.</b> <b>To use the script build the sample application (see the application section for more details) and call borsch with the following command: \\t. examples borsch swissroll.isomap|</b>[<font color=red>industrial</font>]<b>.</b> <b>You may also try out an minimal example using `make minimal` (examples minimal) and the rna example using `make rna` (examples rna)</b>[<font color=red>industrial</font>]<b>.</b> <b>To run mnist digits embedding example use `make mnist` (examples mnist), to run promoters embedding example use `make promoters` (examples promoters) and to run embedding for faces dataset use `make faces` (examples faces)</b>[<font color=red>industrial</font>]<b>.</b> <b>All graphical examples require matplotlib which can be usually installed with a package manager</b>[<font color=red>industrial</font>]<b>.</b> <b>The promoters example also has non-trivial dependency on shogun machine learning toolbox (minimal version is 2.1.0)</b>[<font color=red>industrial</font>]<b>.</b> We also provide some examples of usage tapkee in shogun as `make langs` (examples langs) example. <b>Api we provide an interface based on the method chaining technique</b>[<font color=red>industrial</font>]<b>.</b> <b>The chain starts from the call of the `initialize` method and followed with the `withparameters(const parametersset )` call which is used to provide parameters like the method to use and its settings</b>[<font color=red>industrial</font>]<b>.</b> The provided argument is formed with the following syntax: \\t(keyword1value1, keyword2value2) such syntax is possible due to comma operator overloading which groups all assigned keywords in the comma separated list. Currently, the following keywords are defined: `method`, `eigen_method`, `neighbors_method`, `num_neighbors`, `target_dimension`, `diffusion_map_timesteps`, `gaussian_kernel_width`, `max_iteration`, `spe_global_strategy`, `spe_num_updates`, `spe_tolerance`, `landmark_ratio`, `nullspace_shift`, `klle_shift`, `check_connectivity`, `fa_epsilon`, `progress_function`, `cancel_function`, `sne_perplexity`, `sne_theta`. <b>As an example of parameters setting, if you want to use the isomap algorithm with the number of neighbors set to 15: \\ttapkee::initialize.withparameters((methodisomap,num_neighbors15)) please note that the inner parentheses are necessary as it uses the comma operator which appears to be ambiguous in this case</b>[<font color=red>industrial</font>]<b>.</b> <b>Next, with initialized parameters you may either embed the provided matrix with: \\ttapkee::initialize.withparameters((methodisomap,num_neighbors15))</b>[<font color=red>industrial</font>]<b>.</b> <b>\\t.embedusing(matrix); or provide callbacks (kernel, distance and features) using any combination of the `withkernel(kernelcallback)`, `withdistance(distancecallback)` and `withfeatures(featurescallback)` member functions: \\ttapkee::initialize.withparameters((methodisomap,num_neighbors15)) \\t.withkernel(kernel_callback) \\t.withdistance(distance_callback) \\t.withfeatures(features_callback) once callbacks are initialized you may either embed data using an stl-compatible sequence of indices or objects (that supports the `begin` and `end` methods to obtain the corresponding iterators) with the `embedusing(sequence)` member function or embed the data using a sequence range with the `embedrange(randomaccessiterator, randomaccessiterator)` member function</b>[<font color=red>industrial</font>]<b>.</b> As a summary a few examples: \\ttapkeeoutput output initialize \\t.withparameters((methodisomap,num_neighbors15)) \\t.embedusing(matrix); \\ttapkeeoutput output initialize \\t.withparameters((methodisomap,num_neighbors15)) \\t.withdistance(distance_callback) \\t.embedusing(indices); \\ttapkeeoutput output initialize \\t.withparameters((methodisomap,num_neighbors15)) \\t.withdistance(distance_callback) \\t.embedrange(indices.begin,indices.end); minimal example a minimal working example of a program that uses the library is: \\t include \\t include \\tusing namespace std; \\tusing namespace tapkee; \\tstruct mydistancecallback \\t \\t\\tscalartype distance(indextype l, indextype r) return abs(l-r); \\t ; \\tint main(int argc, const char argv) \\t \\t\\tconst int n 100; \\t\\tvector indices(n); \\t\\tfor (int i0; i` please note that if you don't include eigen3 in your project there is no need to define that variable in this case eigen3 will be included by tapkee. <b>This issue comes from the need of including the eigen3 library only once when using some specific parameters (like debug and extensions)</b>[<font color=red>industrial</font>]<b>.</b> <b>If you are able to use less restrictive licenses (such as lgplv3) you may define the following variable: `tapkee_use_lgpl_covertree` to use covertree code by john langford</b>[<font color=red>industrial</font>]<b>.</b> <b>When compiling your software that includes tapkee be sure eigen3 headers are in include path and your code is linked against arpack library (-larpack key for g++ and clang++)</b>[<font color=red>industrial</font>]<b>.</b> For an example of integration you may check tapkee adapter in shogun working with installed headers you may check which version of the library do you have with checking the values of `tapkee_world_version`, `tapkee_major_version` and `tapkee_minor_version` defines. We welcome any integration so please contact authors if you have got any questions. If you have successfully used the library please also let authors know about that mentions of any applications are very appreciated. <b>Customization tapkee is designed to be highly customizable with preprocessor definitions</b>[<font color=red>industrial</font>]<b>.</b> If you want to use float as internal numeric type (default is double) you may do that with definition of ` define tapkee_custom_numtype float` before including defines header you use some non-standard stl-compatible realization of vector, map and pair you may redefine them with `tapkee_internal_vector`, `tapkee_internal_pair`, `tapkee_internal_map` (they are set to std::vector, std::pair and std::map by default otherwise). <b>You may define `tapkee_use_fibonacci_heap` or `tapkee_use_priority_queue` to select which data structure should be used in the shortest paths computing algorithm</b>[<font color=red>industrial</font>]<b>.</b> <b>Other properties can be loaded from some provided header file using ` define tapkee_custom_properties`</b>[<font color=red>industrial</font>]<b>.</b> <b>Currently such file should define only one variable `covertree_base` which defines the base of the covertree (default is 1.3)</b>[<font color=red>industrial</font>]<b>.</b> <b>Command line application tapkee comes with a sample application which can be used to construct low-dimensional representations of dense feature matrices</b>[<font color=red>industrial</font>]<b>.</b> For more information on its usage please run: `. bin tapkee_clih` the application takes plain ascii file containing dense matrix (each vector is a column and each line contains values of some feature). <b>The output of the application is stored into the provided file in the same format (each line is feature)</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> <b>To compile the application please use cmake the workflow of compilation tapkee with cmake is usual</b>[<font color=red>industrial</font>]<b>.</b> <b>When using unix-based systems you may use the following command to compile the tapkee application: `mkdir build cd build cmake definitions</b>[<font color=red>industrial</font>]<b>.</b> <b>Make` there are a few cases when you'd want to put some definitions: to enable unit-tests compilation add to `-dbuild_tests1` to `definitions` when building</b>[<font color=red>industrial</font>]<b>.</b> <b>Please note that building unit-tests require googletest</b>[<font color=red>industrial</font>]<b>.</b> <b>If you are running ubuntu you may install `libgtest-dev` package for that</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> Otherwise, if you have gtest sources around you may provide them as `-dgtest_source_dir` and `-dgtest_includes_dir`. <b>You may also download gtest with the following command: `wget tar xfv release-1.8.0.tar.gz` downloaded sources will be used by tapkee</b>[<font color=red>industrial</font>]<b>.</b> <b>To run tests use `make test` command (or better 'ctestvv')</b>[<font color=red>industrial</font>]<b>.</b> <b>To let make script store test coverage information using gcov and add a target for output test coverage in html with lcov add the `-duse_gcov1` flag to `definitions`</b>[<font color=red>industrial</font>]<b>.</b> <b>To enable precomputation of kernel distance matrices which can speed-up algorithms (but requires much more memory) add `-dprecomputed1` to `definitions` when building</b>[<font color=red>industrial</font>]<b>.</b> <b>To build application without parts licensed by lgplv3 use `-dgpl_free1` definition</b>[<font color=red>industrial</font>]<b>.</b> <b>The library requires eigen3 to be available in your path</b>[<font color=red>industrial</font>]<b>.</b> The arpack library is also highly recommended to achieve best performance. <b>On debian ubuntu these packages can be installed with \\tsudo apt-get install libeigen3-dev libarpack2-dev if you are using mac os x and macports you can install these packages with \\tsudo port install eigen3 sudo port install arpack` in case you want to use some non-default compiler use `ccyour-c-compiler cxxyour-c++-compiler cmake definitions.` when running cmake</b>[<font color=red>industrial</font>]<b>.</b> <b>Directory contents the repository of tapkee contains the following directories: `src ` that contains simple command-line application (`src cli`) and cmake module finders (`src cmake`)</b>[<font color=red>industrial</font>]<b>.</b> <b>`includes ` that contains the library itself in the `includes tapkee` subdirectory</b>[<font color=red>industrial</font>]<b>.</b> <b>`test ` that contains unit-tests in the `test unit` subdirectory and a few helper scripts</b>[<font color=red>industrial</font>]<b>.</b> `examples ` that contains a few examples including already mentioned (these examples are supposed to be called through `make` as described above, e.g `make minimal`). <b>`data ` a git submodule that contains data required for examples</b>[<font color=red>industrial</font>]<b>.</b> <b>To initialize this submodule use `git submodule update init`</b>[<font color=red>industrial</font>]<b>.</b> <b>`doc ` that contains doxygen interface file which is used to generate html documentation of the library</b>[<font color=red>industrial</font>]<b>.</b> <b>Calling `doxygen doc doxyfile` will generate it in this folder</b>[<font color=red>industrial</font>]<b>.</b> <b>Once built, the root will also contain the following directories: `bin` that contains binaries (`tapkee_cli` that is command line application and various tests with common naming `test_`) `lib` that contains gtest shared libraries</b>[<font color=red>industrial</font>]<b>.</b> <b>Need help? if you need any help or advice don't hesitate to send an email(mailto: lisitsyn.s.o@gmail.com) or fire an issue at github platforms tapkee is tested to be fully functional on linux (icc, gcc, clang compilers) and mac os x (gcc and clang compilers)</b>[<font color=red>industrial</font>]<b>.</b> <b>It also compiles under windows natively (msvs 2012 compiler) with a few known issues</b>[<font color=red>industrial</font>]<b>.</b> <b>In general, tapkee uses no platform specific code and should work on other systems as well</b>[<font color=red>industrial</font>]<b>.</b> Please let us know(mailto: lisitsyn.s.o@gmail.com) if you have successfully compiled or have got any issues on any other system not listed above. Supported dimension reduction methods tapkee provides implementations of the following dimension reduction methods (urls to descriptions provided): locally linear embedding and kernel locally linear embedding (lle klle) neighborhood preserving embedding (npe) local tangent space alignment (ltsa) linear local tangent space alignment (lltsa) hessian locally linear embedding (hlle) laplacian eigenmaps locality preserving projections diffusion map isomap and landmark isomap multidimensional scaling and landmark multidimensional scaling (mds lmds) stochastic proximity embedding (spe) pca and randomized pca kernel pca (kpca) random projection factor analysis t-sne barnes-hut-sne licensing the library is distributed under the bsd 3-clause license. Exceptions are: barnes-hut-sne code by laurens van der maaten which is distributed under the bsd 4-clause license. Covertree code by john langford and dinoj surendran which is distributed under the lgplv3 license ezoptionsparser by remik ziemlinski which is distributed under the mit license."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 61 ************************************\n",
      "Number of words: 203\n",
      "Number of sentences: 14\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 8\n",
      "Total inspired: 0\n",
      "Total market: 2\n",
      "Total project: 1\n",
      "Total renown: 0\n",
      "URL: https://github.com/malavbhavsar/sentimentalizer/blob/master/README.md\n",
      "URL: https://github.com/malavbhavsar/sentimentalizer/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Sentimentalizer by sentan node-sentiment gem can be used separately or integrated with rails app</b>[<font color=red>industrial</font>]<b>.</b> <b>Install gem using bundler `gem sentimentalizer ` 2</b>[<font color=red>industrial</font>]<b>.</b> <b>This will generate an initializer file with after_initialize hook for rails</b>[<font color=red>industrial</font>]<b>.</b> <b>It's basically training a model to use in the application</b>[<font color=red>industrial</font>]<b>.</b> <b>It will run everytime you start server or run any rake commands, would love some input on this</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> <b>Now, you can run following after you will get output like this instructions for vanilla ruby use 1</b>[<font color=red>industrial</font>]<b>.</b> <b>Either fire up `irb`, or require it in your project with `require 'sentimentalizer'` 3</b>[<font color=orange>project</font>]<b>.</b> <b>Now, you need to train the engine in order to use it and now you will get output like this contributing to sentimentalizer check out the latest master to make sure the feature hasn't been implemented or the bug hasn't been fixed yet</b>[<font color=red>industrial</font>]<b>.</b> Check out the issue tracker to make sure someone already hasn't requested it and or contributed it. Commit and push until you are happy with your contribution. This is important so i don't break it in a future version unintentionally. <b>Please try not to mess with the rakefile, version, or history</b>[<font color=red>industrial</font>]<b>.</b> <b>If you want to have your own version, or is otherwise necessary, that is fine, but please isolate to its own commit so i can cherry-pick around it</b>[<font color=violet>market</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 62 ************************************\n",
      "Number of words: 382\n",
      "Number of sentences: 9\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 6\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/nanxstats/protr/blob/master/README.md\n",
      "URL: https://github.com/nanxstats/protr/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Protr toolkit for generating various numerical features of protein sequences described in xiao et al</b>[<font color=red>industrial</font>]<b>.</b> <b>(2015) paper citation formatted citation: nan xiao, dong-sheng cao, min-feng zhu, and qing-song xu</b>[<font color=red>industrial</font>]<b>.</b> <b>Protr protrweb: r package and web server for generating various numerical representation schemes of protein sequences</b>[<font color=red>industrial</font>]<b>.</b> <b>Bibtex entry: installation to install `protr` from cran: or try the latest version on github: browse the package vignette for a quick-start</b>[<font color=red>industrial</font>]<b>.</b> <b>Shiny app protrweb, the shiny web application built on protr, can be accessed from is a user-friendly web application for computing the protein sequence descriptors (features) presented in the protr package</b>[<font color=red>industrial</font>]<b>.</b> List of supported descriptors commonly used descriptors amino acid composition descriptors amino acid composition dipeptide composition tripeptide composition autocorrelation descriptors normalized moreau-broto autocorrelation moran autocorrelation geary autocorrelation ctd descriptors composition transition distribution conjoint triad descriptors quasi-sequence-order descriptors sequence-order-coupling number quasi-sequence-order descriptors pseudo amino acid composition (pseaac) pseudo amino acid composition amphiphilic pseudo amino acid composition profile-based descriptors profile-based descriptors derived by pssm (position-specific scoring matrix) proteochemometric (pcm) modeling descriptors scales-based descriptors derived by principal components analysis scales-based descriptors derived by amino acid properties (aaindex) scales-based descriptors derived by 20+ classes of 2d and 3d molecular descriptors (topological, whim, vhse, etc.) scales-based descriptors derived by factor analysis scales-based descriptors derived by multidimensional scaling blosum and pam matrix-derived descriptors similarity computation local and global pairwise sequence alignment for protein sequences: between two protein sequences parallelized pairwise similarity calculation with a list of protein sequences go semantic similarity measures: between two groups of go terms two entrez gene ids parallelized pairwise similarity calculation with a list of go terms entrez gene ids miscellaneous tools and datasets retrieve protein sequences from uniprot read protein sequences in fasta format read protein sequences in pdb format sanity check of the amino acid types appeared in the protein sequences protein sequence segmentation auto cross covariance (acc) for generating scales-based descriptors of the same length 20+ pre-computed 2d and 3d descriptor sets for the 20 amino acids to use with the scales-based descriptors blosum and pam matrices for the 20 amino acids meta information of the 20 amino acids contribute to contribute to this project, please take a look at the contributing guidelines(contributing.md) first. <b>Please note that this project is released with a contributor code of conduct(conduct.md)</b>[<font color=red>industrial</font>]<b>.</b> By participating in this project you agree to abide by its terms."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 63 ************************************\n",
      "Number of words: 573\n",
      "Number of sentences: 20\n",
      "Total civic: 1\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 8\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/dnouri/nolearn/blob/master/README.md\n",
      "URL: https://github.com/dnouri/nolearn/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>B nolearn contains a number of wrappers and abstractions around existing neural network libraries, most notably `lasagne `, along with a few machine learning utility modules</b>[<font color=red>industrial</font>]<b>.</b> <b>All code is written to be compatible with `scikit-learn `</b>[<font color=red>industrial</font>]<b>.</b> <b> recommend using `venv ` (when using python 3) or `virtualenv ` (python 2) to install nolearn</b>[<font color=red>industrial</font>]<b>.</b> To install the latest release of nolearn from the python package index, do:: pip install nolearn at the time of this writing, nolearn works with the latest versions of its dependencies, such as numpy, scipy, theano, and lasagne (the latter `from git `). <b>But we also maintain a list of known good versions of dependencies that we support and test</b>[<font color=red>industrial</font>]<b>.</b> Should you run into hairy depdendency issues during installation or runtime, we recommend you try this same set of tested depdencencies instead:: pip installr pip install nolearn if you want to install the latest development version of nolearn directly from git, run:: pip installr pip install git+ eggnolearn0.7.git documentation if you're looking for how to use nolearn.lasagne, then there's two introductory tutorials that you can choose from: `using convolutional neural nets to detect facial keypoints tutorial ` with `code ` `training convolutional neural networks with nolearn ` for specifics around classes and functions out of the lasagne package, such as layers, updates, and nonlinearities, you'll want to look at the `lasagne project's documentation `. <b>Nolearn.lasagne comes with a `number of tests ` that demonstrate some of the more advanced features, such as networks with merge layers, and networks with multiple inputs</b>[<font color=red>industrial</font>]<b>.</b> `nolearn's own documentation ` is somewhat out of date at this point. Finally, there's a few presentations and examples from around the web. Note that some of these might need a specific version of nolearn and lasange to run: oliver drr's `convolutional neural nets ii hands on ` with `code ` roelof pieters' presentation `python for image understanding ` comes with nolearn.lasagne code examples benjamin bossan's `otto group product classification challenge using nolearn lasagne ` kaggle's `instructions on how to set up an aws gpu instance to run nolearn.lasagne ` and the facial keypoint detection tutorial `an example convolutional autoencoder ` winners of the saliency prediction task in the 2015 `lsun challenge ` have published their `lasagne nolearn-based code `  The winners of the 2nd place in the `kaggle diabetic retinopathy detection challenge ` have published their `lasagne nolearn-based code `  The winner of the 2nd place in the `kaggle right whale recognition challenge ` has published his `lasagne nolearn-based code `  <b>Support if you're seeing a bug with nolearn, please submit a bug report to the `nolearn issue tracker `</b>[<font color=red>industrial</font>]<b>.</b> Make sure to include information such as: how to reproduce the error: show us how to trigger the bug using a minimal example what versions you are using: include the git revision and or version of nolearn (and possibly lasagne) that you're using please also make sure to search the issue tracker to see if your issue has been encountered before or fixed. If you believe that you're seeing an issue with lasagne, which is a different software project, please use the `lasagne issue tracker ` instead. <b>There's currently no user mailing list for nolearn</b>[<font color=red>industrial</font>]<b>.</b> <b>However, if you have a question related to lasagne, you might want to try the `lasagne users list `, or use stack overflow</b>[<font color=red>industrial</font>]<b>.</b> <b>Please refrain from contacting the authors for non-commercial support requests directly; public forums are the right place for these</b>[<font color=blue>civic</font>]<b>.</b> Nolearn: scikit-learn compatible neural network library the `license.txt ` file for license rights and limitations (mit)."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 64 ************************************\n",
      "Number of words: 19\n",
      "Number of sentences: 2\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 1\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/ishalyminov/bag_of_words_classification/blob/master/README.md\n",
      "URL: https://github.com/ishalyminov/bag_of_words_classification/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>B bag_of_words_classification an experiment on text classification entirely based on word frequencies you'll need: `python2.7` `nltk` `numpy` `sklearn` `matplotlib`</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 65 ************************************\n",
      "Number of words: 761\n",
      "Number of sentences: 33\n",
      "Total civic: 1\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 7\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/openml/OpenML/blob/master/README.md\n",
      "URL: https://github.com/openml/OpenML/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " B open machine learning welcome to the openml github page :tada: contents: who are we?( who-are-we) what is openml?( what-is-openml) benefits for science( benefits-for-science) benefits for scientists( benefits-for-scientists) benefits for society( benefits-for-society) get involved( get-involved) who are we? we are a group of people who are excited about open science, open data and machine learning. <b>We want to make machine learning and data analysis simple, accessible, collaborative and open with an optimal division of labour between computers and humans</b>[<font color=red>industrial</font>]<b>.</b> What is openml? want to learn about openml or get involved? please do and get in touch(openmlhq@googlegroups.com) in case of questions or comments :incoming_envelope: getting started: check out the openml website to get a first impression of what openml is the openml documentation page gives an introduction in details and features, as well as openml's different apis and integrations so that everyone can work with their favorite tool. <b>How to contribute: citation and honor code: communication contact: is an online machine learning platform for sharing and organizing data, machine learning algorithms and experiments</b>[<font color=red>industrial</font>]<b>.</b> It is designed to create a frictionless, networked ecosystem, that you can readily integrate into your existing processes code environments, allowing people all over the world to collaborate and build directly on each others latest ideas, data and results, irrespective of the tools and infrastructure they happen to use. As an open science platform, openml provides important benefits for the science community and beyond. Benefits for science many sciences have made significant breakthroughs by adopting online tools that help organizing, structuring and analyzing scientific data online. Indeed, any shared idea, question, observation or tool may be noticed by someone who has just the right expertise to spark new ideas, answer open questions, reinterpret observations or reuse data and tools in unexpected new ways. Therefore, sharing research results and collaborating online as a (possibly cross-disciplinary) team enables scientists to quickly build on and extend the results of others, fostering new discoveries. Moreover, ever larger studies become feasible as a lot of data are already available. Questions such as cwhich hyperparameter is important to tune?d, cwhich is the best known workflow for analyzing this data set?d or cwhich data sets are similar in structure to my own?d can be answered in minutes by reusing prior experiments, instead of spending days setting up and running new experiments. Benefits for scientists scientists can also benefit personally from using openml. For example, they can save time, because openml assists in many routine and tedious duties: finding data sets, tasks, flows and prior results, setting up experiments and organizing all experiments for further analysis. Moreover, new experiments are immediately compared to the state of the art without always having to rerun other peoples experiments. Another benefit is that linking ones results to those of others has a large potential for new discoveries (see, for instance, feurer et al. 2017), leading to more publications and more collaboration with other scientists all over the world. Finally, openml can help scientists to reinforce their reputation by making their work (published or not) visible to a wide group of people and by showing how often ones data, code and experiments are downloaded or reused in the experiments of others. Benefits for society openml also provides a useful learning and working environment for students, citizen scientists and practitioners. Students and citizen scientist can easily explore the state of the art and work together with top minds by contributing their own algorithms and experiments. Teachers can challenge their students by letting them compete on openml tasks or by reusing openml data in assignments. Finally, machine learning practitioners can explore and reuse the best solutions for specific analysis problems, interact with the scientific community or efficiently try out many possible approaches. Get involved openml has grown into quite a big project. We could use many more hands to help us out :wrench:. You want to contribute?: awesome check out our wiki page on how to contribute or get in touch there may be unexpected ways for how you could help. You want to support us financially?: yes getting funding through conventional channels is very competitive, and we are happy about every small contribution. <b>Please send an email to openmlhq@googlegroups.com github organization structure openml's code distrubuted over different repositories to simplify development</b>[<font color=red>industrial</font>]<b>.</b> <b>Please see their individual readme's and issue trackers of you like to contribute</b>[<font color=blue>civic</font>]<b>.</b> These are the most important ones: openml openml the openml web application, including the rest api. <b>Openml openml-python the python api, to talk to openml from python scripts (including scikit-learn)</b>[<font color=red>industrial</font>]<b>.</b> <b>Openml openml-r the r api, to talk to openml from r scripts (inclusing mlr)</b>[<font color=red>industrial</font>]<b>.</b> <b>Openml java the java api, to talk to openml from java scripts</b>[<font color=red>industrial</font>]<b>.</b> <b>Openml openml-weka the weka plugin, to talk to openml from the weka toolbox</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 66 ************************************\n",
      "Number of words: 263\n",
      "Number of sentences: 15\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 8\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/tiny-dnn/tiny-dnn/blob/master/README.md\n",
      "URL: https://github.com/tiny-dnn/tiny-dnn/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " The project may be abandoned since the maintainer(s) are just looking to move on. In the case anyone is interested in continuing the project, let us know so that we can discuss next steps. Please visit: forum tiny-dnn-dev is a c++14 implementation of deep learning. It is suitable for deep learning on limited computational resource, embedded systems and iot devices.use openmp for parallelization.use intel avx instruction set.intel cpu which supports avx.use nnpack for convolution operation.acceleration package for neural networks on multi-core cpus.use greentea libdnn for convolution operation with gpu via opencl (experimental).an universal convolution implementation supporting cuda and opencl.use double precision computations instead of single precision. <b>1 tiny-dnn use c++14 standard library for parallelization by default</b>[<font color=red>industrial</font>]<b>.</b> <b>2 if you don't use serialization, you can switch off to speedup compilation time</b>[<font color=red>industrial</font>]<b>.</b> <b>3 tiny-dnn uses google test as default framework to run unit tests</b>[<font color=red>industrial</font>]<b>.</b> <b>No pre-installation required, it's automatically downloaded during cmake configuration</b>[<font color=red>industrial</font>]<b>.</b> <b>For example, type the following commands if you want to use intel tbb and build tests: customize configurations you can edit include config.h to customize default behavior</b>[<font color=red>industrial</font>]<b>.</b> <b>Examples construct convolutional neural networks construct multi-layer perceptron (mlp) another way to construct mlp for more samples, read examples main.cpp or mnist example page</b>[<font color=red>industrial</font>]<b>.</b> Contributing since deep learning community is rapidly growing, we'd love to get contributions from you to accelerate tiny-dnn development for a quick guide to contributing, take a look at the contribution documents(contributing.md). <b>Bengio, practical recommendations for gradient-based training of deep architectures</b>[<font color=red>industrial</font>]<b>.</b> <b>Haffner, gradient-based learning applied to document recognition</b>[<font color=red>industrial</font>]<b>.</b> Other useful reference lists: ufldl recommended readings deeplearning.net reading list license the bsd 3-clause license gitter rooms we have gitter rooms for discussing new features qa."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 67 ************************************\n",
      "Number of words: 1268\n",
      "Number of sentences: 51\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 37\n",
      "Total inspired: 0\n",
      "Total market: 2\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/proycon/python-timbl/blob/master/README.md\n",
      "URL: https://github.com/proycon/python-timbl/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b> readme: python-timbl :authors: sander canisius, maarten van gompel :contact: proycon@anaproy.nl :web site: is a python extension module wrapping the full timbl c++ programming interface</b>[<font color=red>industrial</font>]<b>.</b> <b>With this module, all functionality exposed through the c++ interface is also available to python scripts</b>[<font color=red>industrial</font>]<b>.</b> <b>Being able to access the api from python greatly facilitates prototyping timbl-based applications</b>[<font color=red>industrial</font>]<b>.</b> This is the 2013 release by maarten van gompel, building on the 2006 release by sander canisius. <b>For those used to the old library, there is one backwards-incompatible change, adapt your scripts to use ``import timblapi`` instead of ``import timbl``, as the latter is now a higher-level interface</b>[<font color=red>industrial</font>]<b>.</b> <b>License python-timbl is free software, distributed under the terms of the gnu `general public license`</b>[<font color=red>industrial</font>]<b>.</b> Please cite timbl in publication of research that uses timbl. _general public license: is distributed as part of lamachine which significantly simplifies compilation and installation. <b>The remainder of the instructions in this section refer to manual compilation and installation</b>[<font color=red>industrial</font>]<b>.</b> Python-timbl depends on two external packages, which must have been built and or installed on your system in order to successfully build python-timbl. <b>The first is timbl itself; download its tarball from timbl's homepage and follow the installation instructions, recent ubuntu debian users will find timbl in their distribution's package repository</b>[<font color=red>industrial</font>]<b>.</b> <b>In the remainder of this section, it is assumed that `` timbl_headers`` points to the directory that contains ``timbl timblapi.h``, and `` timbl_libs`` the directory that has contains the timbl libraries</b>[<font color=red>industrial</font>]<b>.</b> <b>Note that timbl itself depends on additional dependencies</b>[<font color=red>industrial</font>]<b>.</b> <b>The second prerequisite is boost.python, a library that facilitates writing python extension modules in c++</b>[<font color=red>industrial</font>]<b>.</b> Many linux distributions come with prebuilt packages of boost.python. <b>If so, install this package; on ubuntu debian this can be done as follows:: \\t sudo apt-get install libboost-python libboost-python-dev if not, refer to the `boost installation instructions` to build and install boost.python manually</b>[<font color=red>industrial</font>]<b>.</b> <b>In the remainder of this section, let `` boost_headers`` refer to the directory that contains the boost header files, and `` boost_libs`` to the directory that contains the boost library files</b>[<font color=red>industrial</font>]<b>.</b> <b>If you installed boost.python with your distribution's package manager, these directories are probably `` usr include`` and `` usr lib`` respectively</b>[<font color=red>industrial</font>]<b>.</b> <b>_boost installation instructions: both prerequisites have been installed on your system, python-timbl can be obtained through github:: \\t git clone git: github.com proycon python-timbl.git \\t cd python-timbl and can then be built and installed with the following command, use ``setup2.py`` for python 2 and ``setup3.py`` for python 3:: sudo python setup3.py \\ build_ext boost-include-dir boost_headers \\ boost-library-dir boost_libs \\ timbl-include-dir timbl_headers \\ timbl-library-dir timbl_libs \\ install prefix dir to install in this is the verbose variant, if default locations are used then the following may suffice already:: sudo python setup3.py install the ``prefix`` option to the install command denotes the directory in which the module is to be installed</b>[<font color=red>industrial</font>]<b>.</b> <b>If you have the appropriate system permissions, you can leave out this option</b>[<font color=violet>market</font>]<b>.</b> <b>The module will then be installed in the python system tree</b>[<font color=red>industrial</font>]<b>.</b> <b>Otherwise, make sure that the installation directory is in the module search path of your python system</b>[<font color=red>industrial</font>]<b>.</b> <b>Usage python-timbl offers two interface to the timbl api</b>[<font color=red>industrial</font>]<b>.</b> <b>A low-level interface contained in the module ``timblapi``, which is very much like the c++ library, and a high-level object oriented interface in the ``timbl`` module, which offers a ``timblclassifier`` class</b>[<font color=red>industrial</font>]<b>.</b> <b>Timbl.timblclassifier: high-level interface the high-level interface features as ``timblclassifier`` class which can be used for training and testing classifiers</b>[<font color=red>industrial</font>]<b>.</b> An example is provided in ``example.py``, parts of it will be discussed here. After importing the necessary module, the classifier is instantiated by passing it an identifier which will be used as prefix used for all filenames written, and a string containing options just as you would pass them to timbl:: \\timport timbl \\tclassifier timbl.timblclassifier( wsd-bank , a 0k 1 ) normalization of theclass distribution is enabled by default (regardless of the ``-g`` option to timbl), pass ``normalizefalse`` to disable it. Training instances can be added using the ``append(featurevector, classlabel)`` method:: \\tclassifier.append( (1,0,0), 'financial') \\tclassifier.append( (0,1,0), 'furniture') \\tclassifier.append( (0,0,1), 'geographic') subsequently, you invoke the actual training, note that at each step timbl may output considerable details about what it is doing to standard error output:: \\tclassifier.train the results of this training is an instance base file, which you can save to file so you can load it again later:: \\tclassifier.save \\tclassifier timbl.timblclassifier( wsd-bank , a 0k 1 ) \\tclassifier.load the main advantage of the python library is the fact that you can classify instances on the fly as follows, just pass a feature vector and optionally also a class label to ``classify(featurevector, classlabel)``:: \\tclasslabel, distribution, distance classifier.classify( (1,0,0) ) you can also create a test file and test it all at once:: \\tclassifier timbl.timblclassifier( wsd-bank , a 0k 1 ) \\tclassifier.load \\tclassifier.addinstance( testfile , (1,0,0),'financial' ) addinstance can be used to add instances to external files (use append for training) \\tclassifier.addinstance( testfile , (0,1,0),'furniture' ) \\tclassifier.addinstance( testfile , (0,0,1),'geograpic' ) \\tclassifier.addinstance( testfile , (1,1,0),'geograpic' ) this one will be wrongly classified as financial furniture \\tclassifier.test( testfile ) \\tprint accuracy: , classifier.getaccuracy real multithreading support if you are writing a multithreaded python application (i.e using the ``threading`` module) and want to benefit from actual concurrency, side-stepping python's global interpreter lock, add the parameter ``threadingtrue`` when invoking the ``timblclassifier`` constructor. Take care to instantiate ``timblclassifier`` before threading. <b>You can then call ``timblclassifier.classify`` from within your threads</b>[<font color=red>industrial</font>]<b>.</b> <b>Concurrency only exists for this ``classify`` method</b>[<font color=red>industrial</font>]<b>.</b> If you do not set this option, everything will still work fine, but you won't benefit from actual concurrency due to python's the global interpret lock. <b>Timblapi: low-level interface for documentation on the low level ``timblapi`` interface you can consult the timbl api guide</b>[<font color=red>industrial</font>]<b>.</b> <b>Although this document actually describes the c++ interface to timbl, the latter is similar enough to its python binding for this document to be a useful reference for python-timbl as well</b>[<font color=red>industrial</font>]<b>.</b> <b>For most part, the python timbl interface follows the c++ version closely</b>[<font color=red>industrial</font>]<b>.</b> <b>Naming style in the c++ interface, method names are in uppercamelcase; for example, ``classify``, ``setoptions``, etc</b>[<font color=red>industrial</font>]<b>.</b> <b>In contrast, the python interface uses lowercamelcase: ``classify``, ``setoptions``, etc</b>[<font color=red>industrial</font>]<b>.</b> <b>Method overloading timbl's ``classify`` methods use the c++ method overloading feature to provide three different kinds of outputs</b>[<font color=red>industrial</font>]<b>.</b> <b>Method overloading is non-existant in python though; therefore, python-timbl has three differently named methods to mirror the functionality of the overloaded classify method</b>[<font color=red>industrial</font>]<b>.</b> <b>The mapping is as follows:: \\t bool timblapi::classify(const std::string line, \\t std::string result); \\t \\tdef timblapi.classify(line) bool, result \\t \\t bool timblapi::classify(const std::string line, \\t std::string result, \\t double distance); \\t \\tdef timblapi.classify2(line) bool, string, distance \\t \\t bool timblapi::classify(const std::string line, \\t std::string result, \\t std::string distrib, \\t double distance); \\t \\tdef timblapi.classify3(line, bool normalizetrue,int requireddepth0) bool, string, dictionary, distance thread-safe version of the above, releases and reacquires python's global interprer lock \\tdef timblapi.classify3safe(line, normalize, requireddepth0) bool, string, dictionary, distance note that the ``classify3`` function returned a string representation of the distribution in versions of python-timbl prior to 2015.08.12, now it returns an actual dictionary</b>[<font color=red>industrial</font>]<b>.</b> <b>When using ``classify3safe`` (the thread-safe version) , ensure you first call initthreads after instantiating ``timblapi``, and manually call the ``initthreading`` method</b>[<font color=red>industrial</font>]<b>.</b> <b>Python-only methods three timbl api methods print information to a standard c++ output stream object (showbestneighbors, showoptions, showsettings, showsettings)</b>[<font color=red>industrial</font>]<b>.</b> <b>In the python interface, these methods will only work with python (stream) objects that have a fileno method returning a valid file descriptor</b>[<font color=red>industrial</font>]<b>.</b> <b>Alternatively, three new methods are provided (bestneighbo(u)rs, options, settings); these methods return the same information as a python string object</b>[<font color=red>industrial</font>]<b>.</b> Scikit-learn wrapper a wrapper for use in scikit-learn has been added. <b>It was designed for use in scikit-learn pipeline objects</b>[<font color=red>industrial</font>]<b>.</b> <b>The wrapper is not finished and has to date only been tested on sparse data</b>[<font color=red>industrial</font>]<b>.</b> Note that timbl does not work well with large amounts of features. <b>It is suggested to reduce the amount of features to a number below 100 to keep system performance reasonable</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> <b>Use on servers with large amounts of memory and processing cores advised</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 68 ************************************\n",
      "Number of words: 48\n",
      "Number of sentences: 4\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 1\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/lettier/boids/blob/master/README.md\n",
      "URL: https://github.com/lettier/boids/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Screenshot(screenshot.jpg) autonomous steering behaviors: seek arrive a boids clone done in 3d using the panda3d game engine</b>[<font color=red>industrial</font>]<b>.</b> You move the green ball with your mouse while the blue boid arrives at the ball and the red boid seeks out the ball. Special thanks to daniel shiffman 2016 david lettier._ '."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 69 ************************************\n",
      "Number of words: 213\n",
      "Number of sentences: 13\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 10\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/biolab/orange3/blob/master/README.md\n",
      "URL: https://github.com/biolab/orange3/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>B orange is a component-based data mining software</b>[<font color=red>industrial</font>]<b>.</b> <b>It includes a range of data visualization, exploration, preprocessing and modeling techniques</b>[<font color=red>industrial</font>]<b>.</b> <b>It can be used through a nice and intuitive user interface or, for more advanced users, as a module for the python programming language</b>[<font color=red>industrial</font>]<b>.</b> This is the latest version of orange (for python 3). <b>The deprecated version of orange 2.7 (for python 2.7) is still available (binaries and sources)</b>[<font color=red>industrial</font>]<b>.</b> <b>Orange: with miniconda anaconda orange requires python 3.6 or newer</b>[<font color=red>industrial</font>]<b>.</b> Create virtual environment for orange: conda create python3 name orange3 in your anaconda prompt add conda-forge to your channels: conda config add channels conda-forge this will enable access to the latest orange release. <b>Then install orange3: conda install orange3 miniconda: install the add-ons, follow a similar recipe: conda install orange3 see specific add-on repositories for details</b>[<font color=red>industrial</font>]<b>.</b> <b>Installing with pip to install orange with pip, run the following</b>[<font color=red>industrial</font>]<b>.</b> <b>Install some build requirements via your system's package manager sudo apt install virtualenv build-essential python3-dev create a separate python environment for orange and its dependencies</b>[<font color=red>industrial</font>]<b>.</b> <b>Virtualenv pythonpython3 system-site-packages orange3venv</b>[<font color=red>industrial</font>]<b>.</b> <b>And make it the active one source orange3venv bin activate install qt dependencies for the gui pip install pyqt5 pyqtwebengine install orange pip install orange3 starting orange gui to start orange gui from the command line, run: orange-canvas or python3m orange.canvas append `help` for a list of program options</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 70 ************************************\n",
      "Number of words: 470\n",
      "Number of sentences: 19\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 12\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/vascokk/NumEr/blob/master/README.md\n",
      "URL: https://github.com/vascokk/NumEr/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>This is a collection of erlang nif functions for blas operations on vectors and matrices with cuda</b>[<font color=red>industrial</font>]<b>.</b> <b>Both are natively implemented as thrust host device vectors and special buffer classes are used to transfer them from erlang to cuda and back</b>[<font color=red>industrial</font>]<b>.</b> Installation on windows x64 all the commands from this point forward should be executed in a vc++ 10.0 command-line window make sure you have the following bat file: with this line inside: execute the above bat file and compile: you should see: mac os x strictly follow nvidia mac os x getting started and set env variables: compile and run eunit: todo: linux operations with vectors and matrices as you see one of the parameters in the matrix buffer is ?row_major. It is kinda borrowed from boost library, but not yet fully implemented in numer. <b>However, under the hood in the thrust vectors the numbers are stored in column-major format</b>[<font color=red>industrial</font>]<b>.</b> I chose to do it in this way, because the cublas library is using column-major storage being a derivative of the fortran blas library. <b>There are several modules, which are wrappers for the nif functions, like: numer\\_blas.erl for blas operations, numer\\_buffer.erl for operations with buffers (new, delete, read, write), etc</b>[<font color=red>industrial</font>]<b>.</b> <b>Using numer\\_buffer module, the above example will look like: blas gemv example: using helpers module since using buffer operations can make the code awkward to read, there is also a helper module numer\\_helpers.erl, wich can be used for prototyping the algorithms</b>[<font color=red>industrial</font>]<b>.</b> <b>Warning do not use this module in iterative algorithms (e.g machine learning)</b>[<font color=red>industrial</font>]<b>.</b> <b>Use it for prototyping or one-off calculations with big matrices vectors</b>[<font color=red>industrial</font>]<b>.</b> Here is how: it is much more readable and useful for one-off calculations, but in the ml training stage (with hundreds of iterations) it will be unusable, due to the multiple buffer transfers. <b>Machine learning with erlang cuda logistic regression there is an implementation of the logistic regression (without regularization) learning function with gradient descent optimization</b>[<font color=red>industrial</font>]<b>.</b> <b>Take a look at learn_buf and gradient_descent in the numer\\_logreg.erl module and run the eunit test: windows 7, gpu quadro fx 1800m 1gb: in learn_buf2_test all the buffers needed are created upfront and passed to the nifs in order to avoid multiple buffer creations and transfers during the iterations</b>[<font color=red>industrial</font>]<b>.</b> <b>The same test with macbook pro, with geforce 9400m 256 mb: the numer\\_ml.erl module contains a c++ implementation (via single nif function) of logistic regression, while the numer\\_logreg.erl is using numer_blas.erl module</b>[<font color=red>industrial</font>]<b>.</b> <b>The first one i used to compare the speed between the native and using numer modules implementations</b>[<font color=red>industrial</font>]<b>.</b> <b>There is considerable difference between the two on windows (using numer modules 3.5 sec, using single nif under 1 sec) and almost no difference on mac (both under 0.5 sec)</b>[<font color=red>industrial</font>]<b>.</b> The project is still a work in progress and needs a lot of polishing and if anyone is willing to give a hand i'll be more than happy. Any suggestions to improve the framework are also very welcome."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 71 ************************************\n",
      "Number of words: 123\n",
      "Number of sentences: 9\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 8\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/PhDP/alchemy2/blob/master/README.md\n",
      "URL: https://github.com/PhDP/alchemy2/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>B alchemy 2 copy of the official svn repository very very few modifications (see changes)</b>[<font color=red>industrial</font>]<b>.</b> <b>Linux x86\\_64 binaries can be found here was developed tested on linux (fedora core 5) and</b>[<font color=red>industrial</font>]<b>.</b> <b>It fails to compile with modern versions of gcc, clang, and bison</b>[<font color=red>industrial</font>]<b>.</b> <b>I've tested this version of the code with with bison 2 and g++ 4.4, see install.sh for a simple install script used on ubuntu to get the right versions of bison and gcc</b>[<font color=red>industrial</font>]<b>.</b> <b>Changes since alchemy is the reference implementation for markov logic, i tried to make as few changes as possible</b>[<font color=red>industrial</font>]<b>.</b> <b>I only made a few modifications to simplify compilation: i added files for a full example in examples</b>[<font color=red>industrial</font>]<b>.</b> <b>I added install.sh, readme.md,.travis.cl, and.gitignore</b>[<font color=red>industrial</font>]<b>.</b> <b>I added the manual and tutorial pdf files in the doc folder</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 72 ************************************\n",
      "Number of words: 353\n",
      "Number of sentences: 7\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 3\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/pushkar/ABAGAIL/blob/master/README.md\n",
      "URL: https://github.com/pushkar/ABAGAIL/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Abagail library contains a number of interconnected java packages that implement machine learning and artificial intelligence algorithms</b>[<font color=red>industrial</font>]<b>.</b> <b>These are artificial intelligence algorithms implemented for the kind of people that like to implement algorithms themselves</b>[<font color=red>industrial</font>]<b>.</b> Commit your changes (`git commitam awesome feature `) 4. Push to the branch (`git push origin my_branch`) 5. <b>Enjoy a refreshing diet coke and wait features hidden markov models baum-welch reestimation algorithm, scaled forward-backward algorithm, viterbi algorithm support for input-output hidden markov models write your own output or transition probability distribution or use the provided distributions, including neural network based conditional probability distributions neural networks feed-forward backpropagation neural networks of arbitrary topology configurable error functions with sum of squares, weighted sum of squares multiple activation functions with logistic sigmoid, linear, tanh, and soft max choose your weight update rule with standard update rule, standard update rule with momentum, quickprop, rprop online and batch training support vector machines fast training with the sequential minimal optimization algorithm support for linear, polynomial, tanh, radial basis function kernels decision trees information gain or gini index split criteria binary or all attribute value splitting chi-square signifigance test pruning with configurable confidence levels boosted decision stumps with adaboost k nearest neighbors fast kd-tree implementation for instance based algorithms of all kinds knn classifier with weighted or non-weighted classification, customizable distance function linear algebra algorithms basic matrix and vector math, a variety of matrix decompositions based on the standard algorithms solve square systems, upper triangular systems, lower triangular systems, least squares singular value decomposition, qr decomposition, lu decomposition, schur decomposition, symmetric eigenvalue decomposition, cholesky factorization make your own matrix decomposition with the easy to use householder reflection and givens rotation classes optimization algorithms randomized hill climbing, simulated annealing, genetic algorithms, and discrete dependency tree mimic make your own crossover functions, mutation functions, neighbor functions, probability distributions, or use the provided ones</b>[<font color=red>industrial</font>]<b>.</b> Optimize the weights of neural networks and solve travelling salesman problems graph algorithms kruskals mst and dfs clustering algorithms em with gaussian mixtures, k-means data preprocessing pca, ica, lda, randomized projections convert from continuous to discrete, discrete to binary reinforcement learning value and policy iteration for markov decision processes 1:."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 73 ************************************\n",
      "Number of words: 353\n",
      "Number of sentences: 13\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 5\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/juandavm/em4gmm/blob/master/README.md\n",
      "URL: https://github.com/juandavm/em4gmm/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Em4gmm fast c implementation of the clustering expectation maximization (em) algorithm for estimating gaussian mixture models (gmms)</b>[<font color=red>industrial</font>]<b>.</b> Introduction in statistics, the expectationmaximization (em) algorithm is an iterative method for finding maximum likelihood or maximum a posteriori (map) estimates of parameters in statistical models, where the model depends on unobserved latent variables. For this reason em is frequently used for data clustering, verification and identification of the speaker (biometric tasks), author profiling based on his documents, automatic document categorization, and many more applications. <b>Download download the latest release of the source code on a zip file clone the repository: `git clone git: github.com juandavm em4gmm.git` installing in order to compile this program, you need first compile and install the zlib library from their website, or using your preferred software distribution channels (aptitude, yum, macports, etc) in order to install it (with the dev packages)</b>[<font color=red>industrial</font>]<b>.</b> <b>On some systems this library can be installed by default</b>[<font color=red>industrial</font>]<b>.</b> <b>On mac os x and linux distributions you can simple use the make command on the system shell to compile it, and then sudo make install to install it on yout system (by default on usr bin)</b>[<font color=red>industrial</font>]<b>.</b> <b>We recommend the use of the lastest version of the gcc compiler (because the code generated by llvm is, for now, much slower)</b>[<font color=red>industrial</font>]<b>.</b> Usage download the last version of the user manual in pdf format and bugs do you have a bug or a feature request? do not worry, open a new issue but please, before opening any new issue, search on existing the yours in order to avoid duplicates. And thanks you for your contribution authors juan daniel valor mir expectation maximization for gaussian mixture models. Copyright (c) 2012-2014 juan daniel valor miro this program is free software; you can redistribute it and or modify it under the terms of the gnu general public license as published by the free software foundation; either version 2 of the license, or (at your option) any later version. This program is distributed in the hope that it will be useful, but without any warranty; without even the implied warranty of merchantability or fitness for a particular purpose. See the gnu general public license for more details."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 74 ************************************\n",
      "Number of words: 320\n",
      "Number of sentences: 15\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 8\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/tangledpath/ruby-fann/blob/master/README.md\n",
      "URL: https://github.com/tangledpath/ruby-fann/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Rubyfann artifical intelligence in ruby or ruby-fann is a ruby gem that binds to fann (fast artificial neural network) from within a ruby rails environment</b>[<font color=red>industrial</font>]<b>.</b> Fann is a is a free open source neural network library, which implements multilayer artificial neural networks with support for both fully connected and sparsely connected networks. <b>It is easy to use, versatile, well documented, and fast</b>[<font color=red>industrial</font>]<b>.</b> Rubyfann makes working with neural networks a breeze using ruby, with the added benefit that most of the heavy lifting is done natively. <b>A talk given by our friend ethan from big-oh studios at lone star ruby 2013: installation add this line to your application's gemfile: gem 'ruby-fann' and then execute: bundle or install it yourself as: gem install ruby-fann usage first, go here read about fann</b>[<font color=red>industrial</font>]<b>.</b> <b>You don't need to install it before using the gem, but understanding fann will help you understand what you can do with the ruby-fann gem: documentation: ruby-fann documentation: example training subsequent execution: save training data to file and use it later (continued from above) save trained network to file and use it later (continued from above) custom training using a callback method this callback function can be called during training when using train_on_data, train_on_file or cascadetrain_on_data</b>[<font color=red>industrial</font>]<b>.</b> It is very useful for doing custom things during training. <b>It is recommended to use this function when implementing custom training procedures, or when visualizing the training in a gui etc</b>[<font color=red>industrial</font>]<b>.</b> <b>The args which the callback function takes is the parameters given to the train_on_data, plus an epochs parameter which tells how many epochs the training have taken so far</b>[<font color=red>industrial</font>]<b>.</b> <b>The callback method should return an integer, if the callback function returns1, the training will terminate</b>[<font color=red>industrial</font>]<b>.</b> The callback (training_callback) will be automatically called if it is implemented on your subclass as follows: a sample project using rubyfann to play tic-tac-toe contributors 1. <b>Create your feature branch (`git checkoutb my-new-feature`) 3</b>[<font color=red>industrial</font>]<b>.</b> Commit your changes (`git commitam 'add some feature'`) 4. Push to the branch (`git push origin my-new-feature`) 5."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 75 ************************************\n",
      "Number of words: 66\n",
      "Number of sentences: 4\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 2\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/linkerlin/MLRaptor/blob/master/README.md\n",
      "URL: https://github.com/linkerlin/MLRaptor/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Mlraptor : em variational inference for exponential family graphical models</b>[<font color=red>industrial</font>]<b>.</b> <b>Website: mike hughes (www.michaelchughes.com) please email all comments questions to mike michaelchughes.com the repository is organized as follows: expfam defines python module for learning exp</b>[<font color=red>industrial</font>]<b>.</b> Data example dataset modules for loading using toy data look for additional documentation and occasional updates on github: references: the canonical textbook is: pattern recognition and machine learning (prml), by christopher bishop '."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 76 ************************************\n",
      "Number of words: 156\n",
      "Number of sentences: 8\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 2\n",
      "Total inspired: 0\n",
      "Total market: 1\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/sagefy/sagefy/blob/master/README.md\n",
      "URL: https://github.com/sagefy/sagefy/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " Because anyone can contribute, you can learn anything you want. Sagefy optimizes based on what you already know and what your goal is. Xs4 | s1: what's next? visit the site for more information, watch this 3 minute youtube video or read the docs want to help? check out our want to help? doc. <b>Sign up for updates reach out to us on twitter contents in this project are licensed under apache 2.0 sagefy, open-content adaptive learning platform</b>[<font color=red>industrial</font>]<b>.</b> <b>Copyright 2019 kevin heis and contributors licensed under the apache license, version 2.0 (the license ); you may not use this file except in compliance with the license</b>[<font color=violet>market</font>]<b>.</b> You may obtain a copy of the license at unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an as is basis, without warranties or conditions of any kind, either express or implied. <b>See the license for the specific language governing permissions and limitations under the license</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 77 ************************************\n",
      "Number of words: 622\n",
      "Number of sentences: 33\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 17\n",
      "Total inspired: 0\n",
      "Total market: 1\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/menpo/menpo/blob/master/README.md\n",
      "URL: https://github.com/menpo/menpo/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>The menpo project python package for handling annotated data</b>[<font color=red>industrial</font>]<b>.</b>  what is menpo? menpo is a menpo project package designed from the ground up to make importing, manipulating and visualizing image and mesh data as simple as possible. <b>In particular, we focus on annotated data which is common within the fields of machine learning and computer vision</b>[<font color=red>industrial</font>]<b>.</b> All core types are `landmarkable` and visualizing these landmarks is very simple. Since landmarks are first class citizens within menpo, it makes tasks like masking images, cropping images inside landmarks and aligning images very simple.  menpo were facial armours which covered all or part of the face and provided a way to secure the top-heavy kabuto (helmet). The shinobi-no-o (chin cord) of the kabuto would be tied under the chin of the menpo. There were small hooks called ori-kugi or posts called odome located on various places to help secure the kabuto's chin cord. <b> wikipedia, menpo installation here in the menpo team, we are firm believers in making installation as simple as possible</b>[<font color=red>industrial</font>]<b>.</b> Unfortunately, we are a complex project that relies on satisfying a number of complex 3rd party library dependencies. <b>The default python packing environment does not make this an easy task</b>[<font color=red>industrial</font>]<b>.</b> Therefore, we evangelise the use of the conda ecosystem, provided by anaconda in order to make things as simple as possible, we suggest that you use conda too to try and persuade you, go to the menpo website to find installation instructions for all major platforms. If you feel strongly about using menpo with the most commonly used python package management system, `pip`, then you should be able to install menpo as follows: however, this may be difficult to achieve on platforms such as windows where a compiler would need to be correctly configured. <b>Therefore, we strongly advocate the use of conda which does not require compilation for installing menpo (or numpy, scipy or matplotlib for that matter)</b>[<font color=red>industrial</font>]<b>.</b> Installation via `conda` is as simple as build status and has the added benefit of installing a number of commonly used scientific packages such as scipy and numpy as menpo also makes use of these packages. <b>Ubuntu 16.04 (x64) and osx 10.12 (x64) | repository</b>[<font color=red>industrial</font>]<b>.</b> We strongly suggest that after installation you: 1. <b>Download the latest version of the notebooksnotebooks_gh 2</b>[<font color=red>industrial</font>]<b>.</b> <b>Conda install jupyter notebook and ipython: `conda install jupyter ipython notebook` 3</b>[<font color=red>industrial</font>]<b>.</b> <b>Notebooks_gh: to get a feel for menpo without installing anything? you can browse the notebooks straight from the menpo website menpo projects menpo is designed to be a core library for implementing algorithms within the machine learning and computer vision fields</b>[<font color=red>industrial</font>]<b>.</b> For example, we have developed a number of more specific libraries that rely on the core components of menpo: menpofitmf_gh: implementations of state-of-the-art deformable modelling algorithms including active appearance models, constrained local models and the supervised descent method. <b>Menpo3dm3d_gh: useful tools for handling 3d mesh data including visualization and an opengl rasterizer</b>[<font color=red>industrial</font>]<b>.</b> The requirements of this package are complex and really benefit from the use of conda menpodetectmd_gh: a package that wraps existing sources of object detection. The core project is under a bsd license, but since other projects are wrapped, they may not be compatible with this bsd license. Therefore, we urge caution be taken when interacting with this library for non-academic purposes. <b>Menpowidgetsmw_gh: this package provides high level object viewing classes using matplotlib and jupyter</b>[<font color=red>industrial</font>]<b>.</b> <b>Jupyter notebooks are therefore required to this package and menpo also implicitly relies on menpowidgets for any widget functionality</b>[<font color=red>industrial</font>]<b>.</b> <b>Mf_gh: our documentation on readthedocs use nose for unit tests</b>[<font color=red>industrial</font>]<b>.</b> <b>After installing `nose` and `mock`, running nosetests</b>[<font color=red>industrial</font>]<b>.</b> <b>From the top of the repository will run all of the unit tests</b>[<font color=red>industrial</font>]<b>.</b> <b>Some small parts of menpo are only available if the user has some optional dependency installed</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> <b>These are: 3d viewing methods, only available if `menpo3d` is installed `menpo.feature.dsift` only available if `cyvlfeat` is installed widgets are only available if `menpowidgets` is installed '</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 78 ************************************\n",
      "Number of words: 252\n",
      "Number of sentences: 10\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 6\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/svoit/Machine_Learning/blob/master/README.md\n",
      "URL: https://github.com/svoit/Machine_Learning/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " Machine learning coursera.org linear regression( linear-regression) logistic regression( logistic-regression) multi-class classification and neural networks( multi-class-classification-and-neural-networks) neural network learning( neural-network-learning) regularized linear regression and bias variance( regularized-linear-regression-and-biasvariance) support vector machines( support-vector-machines) k-means clustering and pca( k-means-clustering-and-pca) anomaly detection and recommender systems( anomaly-detection-and-recommender-systems) iv. <b>Linear regression with multiple variables (week 2) linear regression warm up exercise compute cost for one variable gradient descent for one variable feature normalization (optional) compute cost for multiple variables (optional) gradient descent for multiple variables (optional) normal equations (optional) vii</b>[<font color=red>industrial</font>]<b>.</b> <b>Regularization (week 3) logistic regression sigmoid function compute cost for logistic regression gradient for logistic regression predict function compute cost for regularized lr gradient for regularized lr viii</b>[<font color=red>industrial</font>]<b>.</b> Neural networks: representation (week 4) multi-class classification and neural networks regularied logistic regression one-vs-all classifier training one-vs-all classifier prediction neural network prediction function ix. <b>Neural networks: learning (week 5) neural network learning feedforward and cost function regularized cost function sigmoid gradient neural net gradient function (backpropagation) regularized gradient x</b>[<font color=red>industrial</font>]<b>.</b> Advice for applying machine learning (week 6) regularized linear regression and bias variance regularized linear regression cost function regularized linear regression gradient learning curve polynomial feature mapping cross validation curve xii. <b>Support vector machines (week 7) support vector machines gaussian kernel parameters (c, sigma) for dataset 3 email preprocessing email feature extraction xiv</b>[<font color=red>industrial</font>]<b>.</b> <b>Dimensionality reduction (week 8) k-means clustering and pca find closest centroids compute centroid means pca project data recover data xvi</b>[<font color=red>industrial</font>]<b>.</b> <b>Recommender systems (week 9) anomaly detection and recommender systems estimate gaussian parameters select threshold pca collaborative filtering cost collaborative filtering gradient regularized cost gradient with regularization '</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 79 ************************************\n",
      "Number of words: 106\n",
      "Number of sentences: 8\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 4\n",
      "Total inspired: 0\n",
      "Total market: 1\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/LearnLib/learnlib/blob/master/README.md\n",
      "URL: https://github.com/LearnLib/learnlib/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Select `file` `import.` and select existing maven projects</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> <b>Select the folder containing the development checkout as the root directory and click `finish`</b>[<font color=red>industrial</font>]<b>.</b> In order to have both development versions of automatalib and learnlib available at once, continue to import automatalib as documented in the project's readme. <b>Older versions mailing lists q a @ google groupslearnlib-qa general questions regarding the usage of learnlib</b>[<font color=red>industrial</font>]<b>.</b> Discussion @ google groupslearnlib-discussion discussions about the internals of learnlib. Internal (private) @ google groupslearnlib-internal discussions about future development plans. <b>Maintainers markus frohme6 (2017 ) falk howar5 (2013 ) malte isberner4 (2013 2015) 1: search 7cga 7c1 7cg 3a 22de.learnlib 22 maven-central-distr: search 7cga 7c1 7cg 3a 22de.learnlib.distribution 22 intellij:</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 80 ************************************\n",
      "Number of words: 367\n",
      "Number of sentences: 11\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 7\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/stan-dev/pystan/blob/master/README.md\n",
      "URL: https://github.com/stan-dev/pystan/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Pystan provides a python interface to stan, a package for bayesian inference using the no-u-turn sampler, a variant of hamiltonian monte carlo</b>[<font color=red>industrial</font>]<b>.</b> <b>For more information on `stan ` and its modeling language, see the stan user's guide and reference manual at ``</b>[<font color=red>industrial</font>]<b>.</b> Important links html documentation: issue tracker: source code repository: stan: stan user's guide and reference manual (pdf) available at projects arviz: `exploratory analysis of bayesian models with python ` by @arviz-devs prophet: `timeseries forecasting ` by @facebook scikit-learn integration: `pystan-sklearn ` by @rgerkin. <b>Bambi: `bayesian model-building interface ` by @bambinos jupyter tool: `stanmagic ` by @arvinds-ds jupyter tool: `jupyterstan ` by @janfreyberg similar projects pymc: stan3 the development of pystan3 with updated api can be found under `stan-dev pystan-next ` detailed installation instructions detailed installation instructions can be found in the `doc installation_beginner.md ` file</b>[<font color=red>industrial</font>]<b>.</b> <b>Windows installation instructions detailed installation instructions for windows can be found in docs under `pystan on windows ` quick installation (linux and macos) `numpy ` and `cython ` (version 0.22 or greater) are required</b>[<font color=red>industrial</font>]<b>.</b> <b>Arviz is recommended for visualization and analysis</b>[<font color=red>industrial</font>]<b>.</b> <b>Pystan and the required packages may be installed from the `python package index ` using ``pip``</b>[<font color=red>industrial</font>]<b>.</b> <b>:: pip install pystan alternatively, if cython (version 0.22 or greater) and numpy are already available, pystan may be installed from source with the following commands :: git clone recursive cd pystan python setup.py install to install latest development version user can also use ``pip`` :: pip install git+ you encounter an ``importerror`` after compiling from source, try changing out of the source directory before attempting ``import pystan``</b>[<font color=red>industrial</font>]<b>.</b> Example :: import pystan import numpy as np import matplotlib.pyplot as plt schools_code data int j; number of schools real yj; estimated treatment effects real sigmaj; s.e. Of effect estimates parameters real mu; real tau; real etaj; transformed parameters real thetaj; for (j in 1:j) thetaj mu + tau etaj; model eta normal(0, 1); y normal(theta, sigma); schools_dat 'j': 8, 'y': 28, 8,3, 7,1, 1, 18, 12, 'sigma': 15, 10, 16, 11, 9, 11, 10, 18 sm pystan.stanmodel(model_codeschools_code) fit sm.sampling(dataschools_dat, iter1000, chains4) print(fit) eta fit.extract(permutedtrue)'eta' np.mean(eta, axis0) if matplotlib is installed (optional, not required), a visual summary and traceplot are available fit.plot plt.show updated traceplot can be plotted with import arviz as az az.plot_trace(fit)."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 81 ************************************\n",
      "Number of words: 377\n",
      "Number of sentences: 14\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 10\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/root-project/root/blob/master/README.md\n",
      "URL: https://github.com/root-project/root/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>About the root system provides a set of oo frameworks with all the functionality needed to handle and analyze large amounts of data in a very efficient way</b>[<font color=red>industrial</font>]<b>.</b> Having the data defined as a set of objects, specialized storage methods are used to get direct access to the separate attributes of the selected objects, without having to touch the bulk of the data. <b>Included are histograming methods in an arbitrary number of dimensions, curve fitting, function evaluation, minimization, graphics and visualization classes to allow the easy setup of an analysis system that can query and process the data interactively or in batch mode, as well as a general parallel processing framework, proof, that can considerably speed up an analysis</b>[<font color=red>industrial</font>]<b>.</b> <b>Thanks to the built-in c++ interpreter cling, the command, the scripting and the programming language are all c++</b>[<font color=red>industrial</font>]<b>.</b> <b>The interpreter allows for fast prototyping of the macros since it removes the time consuming compile link cycle</b>[<font color=red>industrial</font>]<b>.</b> <b>If more performance is needed the interactively developed macros can be compiled using a c++ compiler via a machine independent transparent compiler interface called aclic</b>[<font color=red>industrial</font>]<b>.</b> <b>The system has been designed in such a way that it can query its databases in parallel on clusters of workstations or many-core machines</b>[<font color=red>industrial</font>]<b>.</b> <b>Root is an open system that can be dynamically extended by linking external libraries</b>[<font color=red>industrial</font>]<b>.</b> <b>This makes root a premier platform on which to build data acquisition, simulation and data analysis systems</b>[<font color=red>industrial</font>]<b>.</b> <b>| cite we are cite us as rene brun and fons rademakers, root an object oriented data analysis framework, proceedings aihenp'96 workshop, lausanne, sep</b>[<font color=red>industrial</font>]<b>.</b> See also root software, release vx.yy zz, dd mm yyyy, (select the right link for your release here: live demo for cern users screenshots these screenshots shows some of the plots (produced using root) presented when the higgs boson discovery was announced at cern data mc ratio plot p0 trends more screenshots on our gallery download and getting started see root.cern download page for the latest binary releases. <b>Building clone the repo git clone a directory for building mkdir build cd build run cmake and make cmake. root makej8 setup and run root source bin thisroot.sh root more information regarding building</b>[<font color=red>industrial</font>]<b>.</b> Help and support forum issue tracker report a bug (requires a cern lightweight account mailing lists documentation tutorials contribution guidelines how to contribute bug reporting guidelines coding conventions meetings."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 82 ************************************\n",
      "Number of words: 837\n",
      "Number of sentences: 48\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 36\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/DrugowitschLab/VBLinLogit/blob/master/README.md\n",
      "URL: https://github.com/DrugowitschLab/VBLinLogit/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>B vblinlogit library provides stand-alone matlab octave code to perform variational bayesian linear and logistic regression</b>[<font color=red>industrial</font>]<b>.</b> <b>In contrast to standard linear and logistic regression, the library assumes priors over the parameters which are tuned by variational bayesian inference, to avoid overfitting</b>[<font color=red>industrial</font>]<b>.</b> Specifically, it supports a fully bayesian version of automatic relevance determination (ard), which is a sparsity-promoting prior that prunes regression coefficients that are deemed irrelevant. Linear regression is available in the following two variants: variational bayesian linear regression with ard: assumes a zero-mean multivariate gaussian prior on the weight vector, for which each element along the diagonal of the covariance matrix is modeled separately by an inverse-gamma hyper-prior. <b>Variational bayesian linear regression without ard</b>[<font color=red>industrial</font>]<b>.</b> Logistic regression is available in the following two variants: variational bayesian logistic regression with ard: assumes a zero-mean multivariate gaussian prior on the weight vector, for which each element along the diagonal of the covariance matrix is modeled separately by an inverse-gamma hyper-prior. Variational bayesian logistic regression without ard: assumes the same model as for the ard variant, only that all elements of the diagonal covariance are modeled jointly by the same inverse-gamma hyper-prior. <b>Installation download v0.3 or the latest version of vblinlogit and extract the downloaded file to a folder of your choice, or clone the repository</b>[<font color=red>industrial</font>]<b>.</b> <b>To use within matlab octave, add the folder to the search path, either using the gui in matlab or by calling at the matlab octave command line</b>[<font color=red>industrial</font>]<b>.</b> <b>See the matlab octave documentation for how to save this search path for use in future matlab octave sessions</b>[<font color=red>industrial</font>]<b>.</b> <b>The installation can be checked by running the tests in the `test`(test) folder</b>[<font color=red>industrial</font>]<b>.</b> <b>Requirements all scripts have been tested in matlab r2018a and octave v5.1.0, but should work with earlier matlab octave versions</b>[<font color=red>industrial</font>]<b>.</b> <b>In particular, they should be compatible with matlab starting r2007a</b>[<font color=red>industrial</font>]<b>.</b> <b>Please file an issue if you identify incompatibility with earlier matlab octave versions</b>[<font color=red>industrial</font>]<b>.</b> <b>Some linear regression example scripts rely on the matlab statistics and machine learning toolbox to estimate the regression coefficient confidence intervals</b>[<font color=red>industrial</font>]<b>.</b> <b>These function won't plot confidence intervals if this toolbox isn't installed</b>[<font color=red>industrial</font>]<b>.</b> <b>Usage and documentation the library source code resides in the `src`(src) folder</b>[<font color=red>industrial</font>]<b>.</b> <b>The below provides a brief description of the api for the different functions</b>[<font color=red>industrial</font>]<b>.</b> <b>The header of each function file provides a more extended description of the function it performs</b>[<font color=red>industrial</font>]<b>.</b> <b>For a more extended discussion of the derivations and the use, please consult variational bayesian inference for linear and logistic regression, arxiv:1310.5438 stat.ml</b>[<font color=red>industrial</font>]<b>.</b> <b>See the `examples`(examples) folder for example use of the different scripts in the `src` folder</b>[<font color=red>industrial</font>]<b>.</b> <b>In all of the below, `d` is the dimensionality of the input, the output is one-dimensional, and `n` is the number of data points in the training set</b>[<font color=red>industrial</font>]<b>.</b> <b>For both linear and logistic regression, the training set is specified by the `n x d` matrix `x`, and the `n`-element column vector `y`</b>[<font color=red>industrial</font>]<b>.</b> <b>The `n`th row in `x` specifies one `d`-element input vector that corresponds to the output given by the `n`th element of `y`</b>[<font color=red>industrial</font>]<b>.</b> <b>For linear regression, these outputs are expected to be scalars</b>[<font color=red>industrial</font>]<b>.</b> <b>Variational bayesian linear regression model fitting fits variational bayesian linear regression without ard to the training data given by `x` and `y`</b>[<font color=red>industrial</font>]<b>.</b> <b>The optional scalars `a0`, `b0`, `c0`, and `d0` specify the prior and hyper-prior parameters</b>[<font color=red>industrial</font>]<b>.</b> <b>The function returns the posterior weight mean vector `w` and covariance matrix `v`, as well as its inverse `invv` and scalar log-determinant `logdetv`</b>[<font color=red>industrial</font>]<b>.</b> <b>It furthermore returns the scalar posterior precision parameters, `an` and `bn`, the hyper-posterior mean `e_a`, as well as the variational bound `l`</b>[<font color=red>industrial</font>]<b>.</b> <b>Is similar to `vb_linear_fit(.)`, but uses an ard prior</b>[<font color=red>industrial</font>]<b>.</b> Thus, it returns the hyper-posterior mean vector, `e_a`, rather than a scalar. <b>Model predictions for a fitted variational bayesian linear regression model, predicts the outputs for the given `k x d` input matrix `x`, with one input vector per row</b>[<font color=red>industrial</font>]<b>.</b> <b>The additional arguments `w`, `v`, `an`, and `bn` are those returned by `vb_linear_fit_ard`</b>[<font color=red>industrial</font>]<b>.</b> <b>The function returns the posterior predictive means `mu`, precisions `lambda`, and degrees of freedom `nu`</b>[<font color=red>industrial</font>]<b>.</b> <b>`mu` and `lambda` are `k`-element vectors, and `nu` is a scalar that is shared by all outputs</b>[<font color=red>industrial</font>]<b>.</b> Variational bayesian logistic regression model fitting fits variational bayesian logistic regression without ard, but a global shrinkage prior, to the training data given by `x` and `y`. <b>The optional scalars `a0` and `b0` specify the parameters of the shrinkage prior</b>[<font color=red>industrial</font>]<b>.</b> <b>It furthermore returns the scalar posterior shrinkage mean, `e_a`, as well as the variational bound `l`</b>[<font color=red>industrial</font>]<b>.</b> <b>Is similar to `vb_logit_fit(.)`, but uses only a weak pre-specified shrinkage prior</b>[<font color=red>industrial</font>]<b>.</b> Thus, it does not support specifying `a0` and `b0`, and doesn't return `e_a`. Furthermore, iterates over the inputs separately rather than processing them all at once, and is therefore slower, but also computationally more stable as it avoids computing the inverse of possibly close-to-singular matrices. <b>Is similar to `vb_logit_fit(.)`, but uses an ard prior</b>[<font color=red>industrial</font>]<b>.</b> Thus, it returns the posterior shrinkage mean vector, `e_a`, rather than a scalar. <b>Model predictions please note that the two logistic regression prediction functions return the probabilities `p(y1</b>[<font color=red>industrial</font>]<b>.</b> <b>For a fitted variational bayesian logistic regression model, predicts `p(y1</b>[<font color=red>industrial</font>]<b>.</b> Is similar to `vb_logit_pred`, but rather than computing all predictions simultaneously, it does so for each row of `x` separately by iterating over the rows of `x`. Contributing for contributions and bug reports, please see the contribution guidelines(contributing.md)."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 83 ************************************\n",
      "Number of words: 42\n",
      "Number of sentences: 3\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 1\n",
      "Total inspired: 0\n",
      "Total market: 1\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/uncomplicate/bayadera/blob/master/README.md\n",
      "URL: https://github.com/uncomplicate/bayadera/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Bayadera adopt your pet function and become a patron clojure library for bayesian data analysis and machine learning on the gpu</b>[<font color=red>industrial</font>]<b>.</b> <b>License copyright 2014 2017 dragan djuric distributed under the eclipse public license either version 1.0 or (at your option) any later version</b>[<font color=violet>market</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 84 ************************************\n",
      "Number of words: 883\n",
      "Number of sentences: 66\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 1\n",
      "Total industrial: 33\n",
      "Total inspired: 0\n",
      "Total market: 1\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/JustGlowing/minisom/blob/master/README.md\n",
      "URL: https://github.com/JustGlowing/minisom/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " Minisom self organizing maps minisom is a minimalistic and numpy based implementation of the self organizing maps (som). Som is a type of artificial neural network able to convert complex, nonlinear statistical relationships between high-dimensional data items into simple geometric relationships on a low-dimensional display. <b>Installation just use pip: pip install minisom or download minisom to a directory of your choice and use the setup script: git clone python setup.py install how to use it in order to use minisom you need your data organized as a numpy matrix where each row corresponds to an observation or as list of lists like the following: then you can run minisom just as follows: minisom implements two types of training</b>[<font color=red>industrial</font>]<b>.</b> The random training (implemented by the method `train_random`), where the model is trained picking random samples from your data, and the batch training (implemented by the method `train_batch`), where the samples are picked in the order they are stored. <b>The weights of the network are randomly initialized by default</b>[<font color=red>industrial</font>]<b>.</b> <b>Two additional methods are provided to initialize the weights in a data driven fashion: `random_weights_init` and `pca_weights_init`</b>[<font color=red>industrial</font>]<b>.</b> <b>Using the trained som for an overview of all the features implemented in minisom you can browse the following examples: export a som and load it again a model can be saved using pickle as follows and can be loaded as follows note that if a lambda function is used to define the decay factor minisom will not be pickable anymore</b>[<font color=red>industrial</font>]<b>.</b> <b>Examples here are some of the charts you'll see how to generate in the examples:</b>[<font color=red>industrial</font>]<b>.</b> Images mapping color quantization | outliers detection tutorials self organizing maps on the glowing python by me ;-) lecture notes from the machine learning course at the university of lisbon introduction to self-organizing by derrick mwiti discovering som, an unsupervised neural network by gisely alves video tutorials made by the geoengineerings school: part 1; part 2; part 3; part 4 video tutorial self organizing maps: introduction by superdatascience who uses minisom? gyrgy kovcs. <b>Smote-variants: a python implementation of 85 minority oversampling techniques</b>[<font color=red>industrial</font>]<b>.</b> Feature dynamic deep learning approach for ddos mitigation within the isp domain. International journal of information security, 2019. Marco casavantes, roberto lopez, and luis carlos gonzalez. <b>Uach at mex-a3t 2019: preliminary results on detecting aggressive tweets by adding author information via an unsupervised strategy</b>[<font color=red>industrial</font>]<b>.</b> <b>Proceedings of the iberian languages evaluation forum (iberlef 2019), 2019</b>[<font color=red>industrial</font>]<b>.</b> <b>Adaptive mapping of sound collections for data-driven musical interfaces</b>[<font color=red>industrial</font>]<b>.</b> Wind climatology of quesnel lake, british columbia. Master thesis, university of northern british columbia, prince george, bc. Florent forest, mustapha lebbah, hanene azzag and jrme lacaille. <b>Deep architectures for joint clustering and visualization with self-organizing maps</b>[<font color=red>industrial</font>]<b>.</b> <b>Ldrc@pakdd 2019 (learning data representation for clustering@pakdd) macau china</b>[<font color=red>industrial</font>]<b>.</b> <b>Search for highly ionizing particles with the pixel detector in the belle ii experiment</b>[<font color=red>industrial</font>]<b>.</b> <b>Machine learning quick reference: quick and essential machine learning hacks for training smart data models</b>[<font color=red>industrial</font>]<b>.</b> <b>Susi: supervised self-organizing maps for regression and classification in python</b>[<font color=red>industrial</font>]<b>.</b> <b>Electricity use profiling and forecasting at microgrid level</b>[<font color=red>industrial</font>]<b>.</b> <b>Ieee 59th international scientific conference on power and electrical engineering of riga technical university (rtucon), 2018</b>[<font color=red>industrial</font>]<b>.</b> <b>A hybrid approach of text summarization using latent semantic analysis and deep learning</b>[<font color=red>industrial</font>]<b>.</b> <b>2018 international conference on advances in computing, communications and informatics (icacci), 2018</b>[<font color=red>industrial</font>]<b>.</b> <b>Facial expression classification using photo-reflective sensors on smart eyewear</b>[<font color=red>industrial</font>]<b>.</b> Katsutoshi masai, kai kunze, yuta sugiura, maki sugimoto. <b>Mapping natural facial expressions using unsupervised learning and optical sensors on smart eyewear</b>[<font color=red>industrial</font>]<b>.</b> <b>Proceedings of the 2018 acm international joint conference and 2018 international symposium on pervasive and ubiquitous computing and wearable computers, 2018 acm</b>[<font color=green>green</font>]<b>.</b> <b>A lightweight ddos attack mitigation system within the isp domain utilising self-organizing map</b>[<font color=red>industrial</font>]<b>.</b> Proceedings of the future technologies, 2018 springer. <b>Self-organizing map-based approaches in ddos flooding detection using sdn</b>[<font color=red>industrial</font>]<b>.</b> 2018 international conference on information networking (icoin), 2018. Li yuan implementation of self-organizing maps with python. Vincent fortuin, matthias hser, francesco locatello, heiko strathmann, and gunnar rtsch. Deep self-organization: interpretable discrete representation learning on time series. <b>Self-organizing map quantization error approach for detecting temporal variations in image sets</b>[<font color=red>industrial</font>]<b>.</b> Birgitta dresp-langley, john mwangi wandeto, henry okola nyongesa. Usingthequantizationerrorfromselforganizingmap(som)outputforfastdetectionofcriticalvariationsinimagetimeseries. Detection of structural change in geographic regions of interest by self organized mapping: las vegas city and lake mead across the years. Denis mayr lima martins, gottfried vossen, fernando buarque de lima neto. <b>Learning database queries via intelligent semiotic machines</b>[<font color=red>industrial</font>]<b>.</b> <b>Ieee latin american conference on computational intelligence (la-cci), 2017</b>[<font color=red>industrial</font>]<b>.</b> Deep learning a-z: hands-on artificial neural networks fredrik broch elgaaen, nicholas mowatt larssen. <b>Data mining i banksektoren prediksjonsmodellering og analyse av kunder som sier opp boligln</b>[<font color=red>industrial</font>]<b>.</b> Scar clavera gonzlez, enric monte moreno, salvador torra porras. A self-organizing map analysis of survey-based agents expectations before impending shocks for model selection: the case of the 2008 financial crisis. Sameen mansha, faisal kamiran, asim karim, aizaz anwar. A self-organizing map for identifying influentialcommunities in speech-based networks. Proceeding cikm '16 proceedings of the 25th acm international on conference on information and knowledge management pages 1965-1968. Sameen mansha, zaheer babar, faisal kamiran, asim karim. Neural network based association rule mining from uncertain data. <b>Neural information processing volume 9950 of the series lecture notes in computer science pp 129-136</b>[<font color=red>industrial</font>]<b>.</b> <b>Text mining applied to sql queries: a case study for the sdss skyserver</b>[<font color=red>industrial</font>]<b>.</b> 2nd annual international symposium on information management and big data. <b>Royal institute of technology school of computer science and communication kth csc</b>[<font color=red>industrial</font>]<b>.</b> Ivana kaji, guido schillaci, saa bodiroa, verena v. <b>Hafner, learning hand-eye coordination for a humanoid robot using soms</b>[<font color=red>industrial</font>]<b>.</b> Proceedings of the 2014 acm ieee international conference on human-robot interaction pages 192-193. <b>Compatibility notes minisom has been tested under python 3.6.2</b>[<font color=red>industrial</font>]<b>.</b> <b>License minisom by giuseppe vettigli is licensed under the creative commons attribution 3.0 unported license</b>[<font color=red>industrial</font>]<b>.</b> <b>License( creative commons attribution 3.0 unported license ) '</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 85 ************************************\n",
      "Number of words: 28\n",
      "Number of sentences: 4\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 3\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/lastlegion/linearReg.js/blob/master/README.md\n",
      "URL: https://github.com/lastlegion/linearReg.js/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>B lineareg.js a javascript implementation of linear regression</b>[<font color=red>industrial</font>]<b>.</b> <b>Ng's mooc on machine learning sylvester for linear algebra</b>[<font color=red>industrial</font>]<b>.</b> <b>`npm install sylvester` installation lineareg can be installed using npm `npm install lineareg`</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 86 ************************************\n",
      "Number of words: 10\n",
      "Number of sentences: 2\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 1\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/kmsravindra/ML-AI-experiments/blob/master/README.md\n",
      "URL: https://github.com/kmsravindra/ML-AI-experiments/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>All my ai and machine learning experiments will go here</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 87 ************************************\n",
      "Number of words: 218\n",
      "Number of sentences: 9\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 6\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/nicococo/tilitools/blob/master/README.md\n",
      "URL: https://github.com/nicococo/tilitools/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " Tilitools travis-ci little machine learning toolbox tilitools is a collection of (non-mainstream) machine learning model and tools with a special focus on anomaly detection, one-class learning, and structured data. <b>Author: nico goernitz examples, description, and lectures examples can be found in the notebooks directory</b>[<font color=red>industrial</font>]<b>.</b> <b>Test data was collected from odds currently available models: bayesian data description support vector data description: 1</b>[<font color=red>industrial</font>]<b>.</b> <b>Multiple kernel learning one-class support vector machine: 1</b>[<font color=red>industrial</font>]<b>.</b> Multiple kernel learning lp-norm mkl wrapper structured output support vector machine (primal) latent variable principle components analysis loda structured objects: multi-class hidden markov model supported kernels and features: rbf, linear kernel histogram intersection kernel histogram features hog features lectures: lectures contains exercise and solution notebooks for various topics. <b>Right now the following are available: introduction to anomaly detection optimization i + ii learning with kernels i + ii geoscience project grainstones: detecting fossils in microscopic images notebooks: contains notebooks to various topics related to machine learning, anomaly detection, and structured output learning</b>[<font color=red>industrial</font>]<b>.</b> <b>Currently, the following are available: high-dimensional outlier detection the anomaly detection setting bayesian data description introduction to svdd and ocsvm data: the data sub-directory contains well-known benchmark datasets which are modified to fit in the anomaly detection setting</b>[<font color=red>industrial</font>]<b>.</b> <b>These modified datasets can be downloaded from odds website installation: there are two packages that need to be installed separately pyod (for loda) pytorch torchvision '</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 88 ************************************\n",
      "Number of words: 52\n",
      "Number of sentences: 3\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 2\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/kinshuk4/MoocX/blob/master/README.md\n",
      "URL: https://github.com/kinshuk4/MoocX/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Coursera repo with code and notes from various platforms</b>[<font color=red>industrial</font>]<b>.</b> <b>My main organized content: my content(. k2e readme.md) moocs coursera(. moocs coursera readme.md) udacity(. moocs udacity readme.md) edx(. moocs edx readme.md) udemy(. moocs udemy readme.md) awseducate(. moocs awseducate readme.md) khan-academy(. moocs khan-academy readme.md) companies mos(. mos readme.md) misc(. misc readme.md) university individual courses '</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 89 ************************************\n",
      "Number of words: 2301\n",
      "Number of sentences: 121\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 96\n",
      "Total inspired: 0\n",
      "Total market: 4\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/zhongkaifu/CRFSharp/blob/master/README.md\n",
      "URL: https://github.com/zhongkaifu/CRFSharp/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Crfsharp crfsharp is conditional random fields (crf) implemented by.net(c ), a machine learning algorithm for learning from labeled sequences of examples</b>[<font color=red>industrial</font>]<b>.</b> <b>Overview crfsharp is conditional random fields implemented by.net(c ), a machine learning algorithm for learning from labeled sequences of examples</b>[<font color=red>industrial</font>]<b>.</b> <b>It is widely used in natural language process (nlp) tasks, for example: word breaker, postaging, named entity recognized and so on</b>[<font color=red>industrial</font>]<b>.</b> <b>Crfsharp (aka crf ) is based on.net framework 4.0 and its mainly algorithm is similar with crf++ written by taku kudo</b>[<font color=red>industrial</font>]<b>.</b> <b>Moreover, it has many significant improvements than crf++, such as totally parallel encoding, optimizing memory usage and so on</b>[<font color=red>industrial</font>]<b>.</b> <b>Currently, when training corpus, compared with crf++, crfsharp can make full use of multi-core cpus and use memory effectively, especially for very huge size training corpus and tags</b>[<font color=red>industrial</font>]<b>.</b> <b>So in the same environment, crfsharp is able to encode much more complex models with less cost than crf++</b>[<font color=red>industrial</font>]<b>.</b> <b>The following screenshot is an example that crfsharp is running on a machine with 16 cores cpus and 96gb memory</b>[<font color=red>industrial</font>]<b>.</b> Training corpus has 1.24 million records with nearly 1.2 billion features. <b>From the screenshot, all cpu cores are full used and memory usage is stable</b>[<font color=red>industrial</font>]<b>.</b> <b>The average encoding time per iteration is 3 minutes and 33 seconds</b>[<font color=red>industrial</font>]<b>.</b> <b>Besides command line tool, crfsharp has also provided apis and these apis can be used into other projects and services for key techincal tasks</b>[<font color=red>industrial</font>]<b>.</b> For example: wordsegment project has used crfsharp to recognize named entity; query term analyzer project has used it to analyze query term important level in word formation and geography coder project has used it to detect geo-entity from text. <b>For detailed information about apis, please see section use crfsharp api in your project in below</b>[<font color=red>industrial</font>]<b>.</b> <b>To use crfsharp, we need to prepare corpus and design feature templates at first</b>[<font color=red>industrial</font>]<b>.</b> <b>Crfsharp's file formats are compatible with crf++(official website: the following paragraphs will introduce data formats and how to use crfsharp in both command line and apis training file format training corpus contains many records to describe what the model should be</b>[<font color=red>industrial</font>]<b>.</b> <b>For each record, it is split into one or many tokens and each token has one or many dimension features to describe itself</b>[<font color=red>industrial</font>]<b>.</b> <b>In training file, each record can be represented as a matrix and ends with an empty line</b>[<font color=red>industrial</font>]<b>.</b> In the matrix, each row describes one token and its features, and each column represents a feature in one dimension. In entire training corpus, the number of column must be fixed. <b>When crfsharp encodes, if the column size is n, according template file describes, the first n-1 columns will usually be used as input data to generate binary feature set and train model</b>[<font color=red>industrial</font>]<b>.</b> The nth column (aka last column) is the answer that the model should output. <b>The means, for one record, if we have an ideal encoded model, given all tokens the first n-1 columns, the model should output each tokens nth column data as the entire records answer</b>[<font color=red>industrial</font>]<b>.</b> <b>There is an example (a bigger training example file is at download section, you can see and download it there): word</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> E_organization the example is for labeling named entities in records. It has two records and each token has three columns. The first column is the term of a token, the second column is the tokens pos-tag result and the third column is to describe whether the token is a named entity or a part of named entity and its type. <b>The first and the second columns are input data for encoding model, and the third column is the model ideal output as answer</b>[<font color=red>industrial</font>]<b>.</b> <b>In above example, we designed output answer as pos_type</b>[<font color=red>industrial</font>]<b>.</b> <b>Pos means the position of the term in the chunk or named entity, type means the output type of the term</b>[<font color=red>industrial</font>]<b>.</b> <b>For pos, it supports four types as follows: s: the chunk has only one term b: the begin term of the chunk m: one of the middle term in the chunk e: the end term of the chunk for type, the example contains many types as follows: organization : the name of one organization location : the name of one location for output answer without type, it's just a normal term, not a named entity</b>[<font color=red>industrial</font>]<b>.</b> <b>Test file format test file has the similar format as training file</b>[<font color=red>industrial</font>]<b>.</b> <b>The only different between training and test file is the last column</b>[<font color=red>industrial</font>]<b>.</b> <b>In test file, all columns are features for crf model</b>[<font color=red>industrial</font>]<b>.</b> <b>Crfsharp command line tools crfsharpconsole.exe is a command line tool to encode and decode crf model</b>[<font color=red>industrial</font>]<b>.</b> <b>By default, the help information showed as follows: linear-chain crf encoder decoder by zhongkai fu (fuzhongkai@gmail.com) crfsharpconsole.exe parameter list</b>[<font color=red>industrial</font>]<b>.</b> <b>Encode crf model from given training corpus -decode parameter list</b>[<font color=red>industrial</font>]<b>.</b> <b>Decode crf model to label text -shrink parameter list</b>[<font color=red>industrial</font>]<b>.</b> <b>Shrink encoded crf model size as the above information shows, the tool provides two run modes</b>[<font color=red>industrial</font>]<b>.</b> <b>Encode mode is for training model, and decode mode is for testing model</b>[<font color=red>industrial</font>]<b>.</b> <b>The following paragraphs introduces how to use these two modes</b>[<font color=red>industrial</font>]<b>.</b> <b>Encode model this mode is used to train crf model from training corpus</b>[<font color=red>industrial</font>]<b>.</b> <b>Besidesencode parameter, the command line parameters as follows: crfsharpconsole.exeencode parameters list -template : template file name -trainfile : training corpus file name -modelfile : encoded model file name -maxiter : maximum iteration, when encoding iteration reaches this value, the process will be ended</b>[<font color=red>industrial</font>]<b>.</b> <b>Default value is 1000 -minfeafreq : minimum feature frequency, if one feature's frequency is less than this value, the feature will be dropped</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> <b>Default value is 2 -mindiff : minimum diff value, when diff less than the value consecutive 3 times, the process will be ended</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> <b>Default value is 0.0001 -thread : threads used to train model</b>[<font color=red>industrial</font>]<b>.</b> <b>Default value is 1 -slotrate : the maximum slot usage rate threshold when building feature set</b>[<font color=red>industrial</font>]<b>.</b> <b>The higher value means longer time to build feature set, but smaller feature set size</b>[<font color=red>industrial</font>]<b>.</b> <b>Default value is 0.95 -hugelexmem : build lexical dictionary in huge mode and shrink starts when used memory reaches this value</b>[<font color=red>industrial</font>]<b>.</b> <b>This mode can build more lexical items, but slowly</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> <b>Default is l2 -retrainmodel : the existing model for re-training</b>[<font color=red>industrial</font>]<b>.</b> <b>-debug: encode model as debug mode note: eithermaxiter reaches setting value ormindiff reaches setting value in consecutive three times, the training process will be finished and saved encoded model</b>[<font color=red>industrial</font>]<b>.</b> <b>Note:hugelexmem is only used for special task, and it is not recommended for common task, since it costs lots of time for memory shrink in order to load more lexical features into memory a command line example as follows: crfsharpconsole.exeencodetemplate template.1trainfile ner.trainmodelfile ner.modelmaxiter 100minfeafreq 1mindiff 0.0001thread 4 debug the entire encoding process contains four main steps as follows: 1</b>[<font color=red>industrial</font>]<b>.</b> <b>Load train corpus from file, generate and select feature set according templates</b>[<font color=red>industrial</font>]<b>.</b> <b>Build selected feature set index data as double array trie-tree format, and save them into file</b>[<font color=red>industrial</font>]<b>.</b> <b>Run encoding process iteratively to tune feature values until reach end condition</b>[<font color=red>industrial</font>]<b>.</b> <b>In step 3, after run each iteration, some detailed encoding information will be show</b>[<font color=red>industrial</font>]<b>.</b> <b>For example: m_rank_1 fr47658, te54.84 m_rank_2:27.07 m_rank_0:26.65 e_rank_0:0.31 b_rank_0:0.21 e_rank_1:0.19 iter65 terr0.320290 serr0.717372 diff0.0559666295793355 fsize73762836(1.10 act) time span: 00:31:56.4866295, aver</b>[<font color=red>industrial</font>]<b>.</b> <b>Time span per iter: 00:00:29 the encoding information has two parts</b>[<font color=red>industrial</font>]<b>.</b> The first part is information about each tag, the second part is information in overview.for each tag, it has two lines information. The first line shows the number of this tag in total (fr) and current token error rate (te) about this tag. <b>The second line shows this tag's token error distribution</b>[<font color=red>industrial</font>]<b>.</b> <b>In above example, in no.65 iteration, m_rank_1 tag's token error rate is 54.84 in total</b>[<font color=red>industrial</font>]<b>.</b> <b>In these token error, 27.07 is m_rank_2, 26.65 is m_rank_0 and so on.for second part (information in overview), some global information is showed</b>[<font color=red>industrial</font>]<b>.</b> Iter : the number of iteration processed terr : tag's token error rate in all serr : record's error rate in all diff : different between current and previous iteration fsize( x act) : the number of feature set in total, x act means the number of non-zero value features. <b>In l1 regularization, with the increasement of iter, x is reduced</b>[<font color=red>industrial</font>]<b>.</b> <b>Time span : how long the encoding process has been taken aver</b>[<font color=red>industrial</font>]<b>.</b> <b>Time span per iter : the average time span for each iteration after encoding process is finished, the following files will be generated</b>[<font color=red>industrial</font>]<b>.</b> <b>File1: model file name this is model meta data file</b>[<font color=red>industrial</font>]<b>.</b> <b>It contains model's global parameters, feature templates, output tags and so on</b>[<font color=red>industrial</font>]<b>.</b> <b>File2: model file name.feature this is feature set lexical dictionary file</b>[<font color=red>industrial</font>]<b>.</b> <b>It contains all features's strings and corresponding ids</b>[<font color=red>industrial</font>]<b>.</b> <b>For high performance, it's built by double array tri-tree</b>[<font color=red>industrial</font>]<b>.</b> <b>In debug mode, model file name.feature.raw_text which saves lexical dictionary in raw text will be generated</b>[<font color=red>industrial</font>]<b>.</b> <b>File3: model file name.alpha this is feature set weight score file</b>[<font color=red>industrial</font>]<b>.</b> <b>Decode model this mode is used to decode and test encoded model</b>[<font color=red>industrial</font>]<b>.</b> <b>Besidesdecode parameter, there are some other required and optional parameters: crfsharpconsole.exedecode -modelfile : the model file used for decoding -inputfile : the input file to predict its content tags -outputfile : the output file to save predicted result -nbest : output n-best result, default value is 1 -prob : output probability, default is not output here is an example: crfsharpconsole.exedecodemodelfile ner.modelinputfile ner_test.txtoutputfile ner_test_result.txtnbest 5prob shrink model encoded model with l1 regularization is usually a sparse model</b>[<font color=red>industrial</font>]<b>.</b> <b>Shrink parameter is used to reduce model file size</b>[<font color=red>industrial</font>]<b>.</b> <b>Withshrink parameter, the command line as follows: crfsharpconsole.exeshrink encoded crf model file name shrinked crf model file name thread num an example as follows: crfsharpconsole.exeshrink ner.model ner_shrinked.model 16 this example is used to shrink ner.model files and the working thread is 16</b>[<font color=red>industrial</font>]<b>.</b> <b>Incremental training for some complex tasks, encoding model is timing-cost</b>[<font color=red>industrial</font>]<b>.</b> With retrainmodel option and updated training corpus (both old and new training corpus), crfsharp supports to train model incrementally and compared with full training, incremental training is able to save lots of time. <b>There is an example: crfsharpconsole.exeencodetemplate template.1trainfile ner_new.trainmodelfile ner_new.modelretrainmodel ner.modelmaxiter 100minfeafreq 1mindiff 0.0001thread 4 debug feature templates crfsharp template is totally compatible with crf++ and used to generate feature set from training and testing corpus</b>[<font color=red>industrial</font>]<b>.</b> <b>In template file, each line describes one template which consists of prefix, id and rule-string</b>[<font color=red>industrial</font>]<b>.</b> <b>There are two prefix, u for unigram template, and b for bigram template</b>[<font color=red>industrial</font>]<b>.</b> <b>And rule-string is used to guide crfsharp to generate features</b>[<font color=red>industrial</font>]<b>.</b> <b>The rule-string has two types of form, one is constant string, and the other is macro</b>[<font color=red>industrial</font>]<b>.</b> Row specifies the offset between current focusing token and generating feature token in row. <b>Col specifies the absolute column position in corpus</b>[<font color=red>industrial</font>]<b>.</b> <b>Moreover, combined macro is also supported, for example: c xrow1, col1 xrow2, col2d</b>[<font color=red>industrial</font>]<b>.</b> <b>When generating feature set, macro will be replaced as specific string</b>[<font color=red>industrial</font>]<b>.</b> <b>A template file example as follows: \\ unigram u01: x-1,0 u02: x0,0 u03: x1,0 u04: x-1,0 x0,0 u05: x0,0 x1,0 u06: x-1,0 x1,0 u07: x-1,1 u08: x0,1 u09: x1,1 u10: x-1,1 x0,1 u11: x0,1 x1,1 u12: x-1,1 x1,1 u13:c x-1,0 x-1,1 u14:c x0,0 x0,1 u15:c x1,0 x1,1 \\ bigram b in this template file, it contains both unigram and bigram templates</b>[<font color=red>industrial</font>]<b>.</b> Assuming current focusing token is cyork nnp e_locationd in the first record in training corpus above, the generated unigram feature set as follows: u01:new u02:york u03:are u04:new york u05:york are u06:new are u07:nnp u08:nnp u09:are u10:nnp nnp u11:nnp vbp u12:nnp vbp u13:cnew nnp u14:cyork nnp u15:care vbp although u07 and u08, u11 and u12s rule-string are the same, we can still distinguish them by id string. <b>In encoding process, according templates, encoder will generate feature set (like the example in above) from records in training corpus and save them into model file</b>[<font color=red>industrial</font>]<b>.</b> <b>In decoding process, for each test record, decoder will also generate features by template, and check every feature whether it exists in model</b>[<font color=red>industrial</font>]<b>.</b> <b>If it is yes, features alpha value will be applied while processing cost value</b>[<font color=red>industrial</font>]<b>.</b> <b>For each token, how many features will be generated from unigram templates? as the above said, if we have m unigram templates, each token will have m feature generated from the template set</b>[<font color=red>industrial</font>]<b>.</b> Moreover, assuming each token has n different output classes, in order to indicate all possible statuses by binary function, we need to have cmnd features for one token in total. <b>For a record which contains l tokens, the feature size of this record is cmnld</b>[<font color=red>industrial</font>]<b>.</b> <b>For bigram template, crfsharp will enumerate all possible combined output classes of two contiguous tokens, and generate features for each combined one</b>[<font color=red>industrial</font>]<b>.</b> So, if each token has n different output classes, and the number of features generated by templates is m, the total bigram feature set size is cnnmd. <b>For a record which contains l tokens, the feature size of this record is cmnn(l-1)d</b>[<font color=red>industrial</font>]<b>.</b> Run on linux mac with mono-project which is the third party.net framework on linux mac, crfsharp is able to run on some non-windows platforms, such as linux, mac and others. <b>With no_support_parallel_lib flag, crfsharp needn't to be re-compiled or modified to run on these operating systems</b>[<font color=red>industrial</font>]<b>.</b> However, if you want to disable no_support_parallel_lib for the highest encoding performance, please modify existed code by replacing parallel.for for long type with that for int type, since so far mono-project hasn't implemented parallel.for for long type yet. <b>Use crfsharp api in your project besides command line tool, crfsharp provides apis for developers to use it in their projects</b>[<font color=red>industrial</font>]<b>.</b> <b>Basically, crfsharp has two dll files: one is crfsharp.dll which contains core algorithm and provides many apis in low level</b>[<font color=red>industrial</font>]<b>.</b> <b>The other is crfsharpwrapper.dll which wraps above low level interfaces and provides interfaces in high level</b>[<font color=red>industrial</font>]<b>.</b> <b>Add following code snippet for detailed information, please visit source code: decode a crfsharp model in your project 1</b>[<font color=red>industrial</font>]<b>.</b> <b>Add following code snippet the decoder.segment is a wrapped decoder interface</b>[<font color=red>industrial</font>]<b>.</b> <b>It's defined as follows: crfsharp referenced by the following published papers 1</b>[<font color=red>industrial</font>]<b>.</b> Reconhecimento de entidades nomeadas em textos em portugus do brasil no domnio do e-commerce 2. Multimodal wearable sensing for fine-grained activity recognition in healthcare 3. <b>A crf-based method for automatic construction of chinese symptom lexicon 4</b>[<font color=red>industrial</font>]<b>.</b> Bilefik cmlelerde yan cmleciklerin otomatik etiketlenmesi 5. <b>A hybrid semi-supervised learning approach to identifying protected health information in electronic medical records 7</b>[<font color=red>industrial</font>]<b>.</b> Einf uhrung in conditional random fields zum taggen von sequentiellen daten tool: wapiti 9. <b>Nghin cu phng php trch chdn thng tin thdi tit t vn bn ting vit 10</b>[<font color=red>industrial</font>]<b>.</b> Unsupervised word and dependency path embeddings for aspect term extraction 11. <b>A hybrid intelligent system to improve data preprocessing 12</b>[<font color=red>industrial</font>]<b>.</b> Family matters: company relations extraction from wikipedia active learning for incremental poi extraction and pairing semantic role labeling with relative clauses automatic de-identification of medical records with a multilevel hybrid semi-supervised learning approach ccaaefa 17."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 90 ************************************\n",
      "Number of words: 486\n",
      "Number of sentences: 13\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 9\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/EducationalTestingService/skll/blob/master/README.md\n",
      "URL: https://github.com/EducationalTestingService/skll/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b> doi for citing skll 1.0.0 this python package provides command-line utilities to make it easier to run machine learning experiments with scikit-learn</b>[<font color=red>industrial</font>]<b>.</b> <b>One of the primary goals of our project is to make it so that you can run scikit-learn experiments without actually needing to write any code other than what you used to generate extract the features</b>[<font color=red>industrial</font>]<b>.</b> <b>Command-line interface the main utility we provide is called ``run_experiment`` and it can be used to easily run a series of learners on datasets specified in a configuration file like:</b>[<font color=red>industrial</font>]<b>.</b> Code:: ini general experiment_name titanic_evaluate_tuned valid tasks: cross_validate, evaluate, predict, train task evaluate input these directories could also be absolute paths (and must be if you're not running things in local mode) train_directory train test_directory dev can specify multiple sets of feature files that are merged together automatically (even across formats) featuresets family.ndj , misc.csv , socioeconomic.arff , vitals.csv list of scikit-learn learners to use learners randomforestclassifier , decisiontreeclassifier , svc , multinomialnb column in csv containing labels to predict label_col survived column in csv containing instance ids (if any) id_col passengerid tuning should we tune parameters of all learners by searching provided parameter grids? grid_search true function to maximize when performing grid search objectives 'accuracy' output also compute the area under the roc curve as an additional metric metrics 'roc_auc' the following can should be absolute paths log output results output predictions output models output for more information about getting started with ``run_experiment``, please check out `our tutorial `, or `our config file specs `  <b>We also provide utilities for: `converting between machine learning toolkit formats ` (e.g, arff, csv, megam) `filtering feature files ` `joining feature files ` `other common tasks ` python api if you just want to avoid writing a lot of boilerplate learning code, you can also use our simple python api which also supports pandas dataframes</b>[<font color=red>industrial</font>]<b>.</b> <b>The main way you'll want to use the api is through the ``learner`` and ``reader`` classes</b>[<font color=red>industrial</font>]<b>.</b> <b>For more details on our api, see `the documentation `</b>[<font color=red>industrial</font>]<b>.</b> While our api can be broadly useful, it should be noted that the command-line utilities are intended as the primary way of using skll. The api is just a nice side-effect of our developing the utilities. <b> doc spacer.png scikit-learn laboratory (skll) is pronounced skull : that's where the learning happens</b>[<font color=red>industrial</font>]<b>.</b> <b>Requirements python 2.7+ `scikit-learn ` `six ` `tabulate ` `beautifulsoup 4 ` `pandas ` `grid map ` (only required if you plan to run things in parallel on a drmaa-compatible cluster) `joblib ` `ruamel.yaml ` `configparser ` (only required for python 2.7) `logutils ` (only required for python 2.7) `mock ` (only required for python 2.7) the following packages can be optionally installed for additional features but are not required: `seaborn ` (optional) talks simpler machine learning with skll 1.0, dan blanchard, pydata nyc 2014 (`video `</b>[<font color=red>industrial</font>]<b>.</b> <b>`slides `) books skll is featured in `data science at the command line ` by `jeroen janssens `</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 91 ************************************\n",
      "Number of words: 591\n",
      "Number of sentences: 25\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 19\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/rieck/harry/blob/master/README.md\n",
      "URL: https://github.com/rieck/harry/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Harry(harry.png) harry a tool for measuring string similarity this software belongs to the publication konrad rieck and christian wressnegger</b>[<font color=red>industrial</font>]<b>.</b> <b> journal of machine learning research (jmlr), 17(9): 1-5, 2016 is a small tool for comparing strings</b>[<font color=red>industrial</font>]<b>.</b> <b>The tool supports several common distance and kernel functions for strings as well as some excotic similarity measures</b>[<font color=red>industrial</font>]<b>.</b> <b>The focus of harry lies on implicit similarity measures, that is, comparison functions that do not give rise to an explicit vector space</b>[<font color=red>industrial</font>]<b>.</b> Examples of such similarity measures are the levenshtein distance, the jaro-winkler distance or the spectrum kernel. <b>During operation harry loads a set of strings from input, computes the specified similarity measure and writes a matrix of similarity values to output</b>[<font color=red>industrial</font>]<b>.</b> <b>The similarity measure can be computed based on the granulartiy of bytes, bits and tokens (words) contained in the strings</b>[<font color=red>industrial</font>]<b>.</b> <b>The configuration of this process, such as the input format, the similarity measure and the output format, are specified in a configuration file and can be additionally refined using command-line options</b>[<font color=red>industrial</font>]<b>.</b> <b>Harry is implemented using openmp, such that the computation time for a set of strings scales linear with the number of available cpu cores</b>[<font color=red>industrial</font>]<b>.</b> <b>Moreover, efficient implementations of several similarity measures, effective caching of similarity values and low-overhead locking further speedup the computation</b>[<font color=red>industrial</font>]<b>.</b> Harry complements the tool sally(1) that embeds strings in a vector space and allows computing vectorial similarity measures, such as the cosine distance and the bag-of-words kernel. Similarity measures the following similarity measures are supported so by harry dist_bag bag distance dist_compression normalized compression distance (ncd) dist_damerau damerau-levenshtein distance dist_hamming hamming distance dist_jaro jaro distance dist_jarowinkler jaro-winkler distance dist_kernel kernel-based distance dist_lee lee distance dist_levenshtein levenshtein distance dist_osa optimal string alignment (osa) distance kern_distance distance substitution kernel (dsk) kern_spectrum spectrum kernel kern_subsequence subsequence kernel (ssk) kern_wdegree weighted-degree kernel (wdk) sim_braun braun-blanquet coefficient sim_dice soerensen-dice coefficient sim_jaccard jaccard coefficient sim_kulczynski second kulczynski coefficient sim_otsuka otsuka coefficient sim_simpson simpson coefficient sim_sokal sokal-sneath coefficient dependencies + openmp 2.5 (need to be supported by the c compiler) + zlib 1.2.1, + libconfig 1.3.2, + libarchive 3.1.2, debian ubuntu linux the following packages need to be installed for compiling harry on debian and ubuntu linux gcc libz-dev libconfig8-dev libarchive-dev for bootstrapping harry from the git repository or manipulating the automake autoconf configuration, the following additional packages are necessary. <b>Automake autoconf libtool mac os x for compiling harry on mac os x a working installation of xcode is needed</b>[<font color=red>industrial</font>]<b>.</b> <b>Moreover, a c compiler supporting openmp is required (`clang` from xcode currently does not support openmp)</b>[<font color=red>industrial</font>]<b>.</b> <b>The following packages need to be installed from homebrew</b>[<font color=red>industrial</font>]<b>.</b> Gcc43 (or download from ) libconfig libarchive (from homebrew-alt) openbsd due to the vague state of openbsd multi-threading, neither the default `gcc` nor the packaged `gcc` seem to correctly support openmp. <b>To get harry to run you can only try to build gcc from scratch :( compilation installation from git repository first run. bootstrap from tarball run. configure options make make check make install options for configure prefixpath set directory prefix for installation by default harry is installed into usr local</b>[<font color=red>industrial</font>]<b>.</b> <b>If you prefer a different location, use this option to select an installation directory</b>[<font color=red>industrial</font>]<b>.</b> <b>Enable-prwlock enable support for posix read-write locks this feature enables read-write locks (rwlocks) from the posix thread library</b>[<font color=red>industrial</font>]<b>.</b> <b>The locks can accelerate the run-time performance on multi-core systems</b>[<font color=red>industrial</font>]<b>.</b> However, these posix locks are not guaranteed to interplay with openmp and thus may not work on all platforms. <b>Enable-md5hash enable md5 as alternative hash harry uses a hash function for mapping tokens to symbols</b>[<font color=red>industrial</font>]<b>.</b> <b>By default the very efficient murmur hash is used for this task</b>[<font color=red>industrial</font>]<b>.</b> <b>In certain critical cases it may be useful to use a cryptographic hash as md5</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 92 ************************************\n",
      "Number of words: 396\n",
      "Number of sentences: 19\n",
      "Total civic: 2\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 10\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 1\n",
      "Total renown: 0\n",
      "URL: https://github.com/nikolamilosevic86/owasp-seraphimdroid/blob/master/README.md\n",
      "URL: https://github.com/nikolamilosevic86/owasp-seraphimdroid/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Owasp seraphimdroid mission: to create, as a community, an open platform for education and protection of android users against privacy and security threats</b>[<font color=blue>civic</font>]<b>.</b> <b>Project description: owasp seraphimdroid is a privacy and security protection app for android devices</b>[<font color=red>industrial</font>]<b>.</b> <b>It enables users to protect their devices against malicious software (viruses, trojans, worms, etc.), phishing sms, mms messages, execution of dangerous ussd codes, theft and loss</b>[<font color=red>industrial</font>]<b>.</b> <b>Also, it enables the user to protect their privacy and to control the usage of applications and services via various kinds of locks</b>[<font color=red>industrial</font>]<b>.</b> This project has two main aims: to protect user's privacy and secure the device against malicious features that may cost user money to educate user about threats and risks for their privacy, privacy of their data and security of their device. Owasp seraphimdroid is free and open source community driven project, supported by owasp (open web application security project) foundation. <b>If you are interested, join our mailing list free to contact the project lead if you want to participate or contribute to the project</b>[<font color=blue>civic</font>]<b>.</b> <b>More info available on the website app is available on google play store setting up development environment for seraphimdroid: requirements: 1</b>[<font color=red>industrial</font>]<b>.</b> <b>X86 emulator or a hardware device for testing (optional) additional sdk requirements: google repository android support repository android support library android sdk tools (included) android sdk build-tools (included) android sdk platform-tools (included) haxm (optional if using x86 emulator) all of these libraries can be installed using the integrated android sdk manager available in android studio detailed info on this wiki</b>[<font color=red>industrial</font>]<b>.</b> <b>Clone owasp-seraphimdroid from github to your desired location using : git clone 2</b>[<font color=red>industrial</font>]<b>.</b> <b>Click on open an existing android studio project or go to file open</b>[<font color=red>industrial</font>]<b>.</b> <b>Select 'seraphimdroid' folder from the cloned repository</b>[<font color=red>industrial</font>]<b>.</b> <b>As soon as the gradle build finishes, you are ready with your project</b>[<font color=orange>project</font>]<b>.</b> <b>You need to configure the maps api for gps functions to work</b>[<font color=red>industrial</font>]<b>.</b> Important note: project has been ported for android studio, so the instructions for old eclipse setup are available on this wiki referencing you may reference the following paper: milosevic, nikola, ali dehghantanha, and kim-kwang raymond choo. <b>Machine learning aided android malware classification</b>[<font color=red>industrial</font>]<b>.</b> Note: this project is under active development and stable build is available on the master build will be kept free of errors mostly, but may still face some minor glitches. The knowledge base is currently under development as a sister project and the results are fetched from the api deployed at openshift the information related to that is available on this wiki."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 93 ************************************\n",
      "Number of words: 22\n",
      "Number of sentences: 3\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 1\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/mgroncki/IPythonScripts/blob/master/README.md\n",
      "URL: https://github.com/mgroncki/IPythonScripts/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " Ipythonscripts here you find my ipython (jupyter) notebooks from my blog (ipythonquant.wordpress.org). <b>Feel free to download the samples and play around with them</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 94 ************************************\n",
      "Number of words: 501\n",
      "Number of sentences: 18\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 8\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/mlr-org/mlr/blob/master/README.md\n",
      "URL: https://github.com/mlr-org/mlr/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " Machine learning in r stable) cran release site online tutorial cheatsheet changelog are actively working on mlr3 as a successor of mlr. This implies that we have less time to reply to mlr issues. Stackoverflow: mlr slack blog installation release development citing _mlr_ in publications please cite our jmlr paper bibtex parts of the package were created as part of other publications. <b>If you use these parts, please cite the relevant work appropriately</b>[<font color=red>industrial</font>]<b>.</b> <b>An overview of all _mlr_ related publications can be found here introduction r does not define a standardized interface for its machine-learning algorithms</b>[<font color=red>industrial</font>]<b>.</b> <b>Therefore, for any non-trivial experiments, you need to write lengthy, tedious and error-prone wrappers to call the different algorithms and unify their respective output</b>[<font color=red>industrial</font>]<b>.</b> <b>Additionally you need to implement infrastructure to resample your models optimize hyperparameters select features cope with preand post-processing of data and compare models in a statistically meaningful way</b>[<font color=red>industrial</font>]<b>.</b> As this becomes computationally expensive, you might want to parallelize your experiments as well. This often forces users to make crummy trade-offs in their experiments due to time constraints or lacking expert programming skills. _mlr_ provides this infrastructure so that you can focus on your experiments the framework provides supervised methods like classification, regression and survival analysis along with their corresponding evaluation and optimization methods, as well as unsupervised methods like clustering. It is written in a way that you can extend it yourself or deviate from the implemented convenience methods and construct your own complex experiments or algorithms. <b>Furthermore, the package is nicely connected to the openml r package and its online platform which aims at supporting collaborative machine learning online and allows to easily share datasets as well as machine learning tasks, algorithms and experiments in order to support reproducible research</b>[<font color=red>industrial</font>]<b>.</b> <b>Features clear s3 interface to r classification, regression, clustering and survival analysis methods abstract description of learners and tasks by properties convenience methods and generic building blocks for your machine learning experiments resampling methods like bootstrapping, cross-validation and subsampling extensive visualizations (e.g roc curves, predictions and partial predictions) simplified benchmarking across data sets and learners easy hyperparameter tuning using different optimization strategies, including potent configurators like iterated f-racing (irace) sequential model-based optimization variable selection with filters and wrappers nested resampling of models with tuning and feature selection cost-sensitive learning, threshold tuning and imbalance correction wrapper mechanism to extend learner functionality in complex ways possibility to combine different processing steps to a complex data mining chain that can be jointly optimized openml connector for the open machine learning server built-in parallelization detailed tutorial miscellaneous simple usage questions are better suited at stackoverflow using the mlr tag</b>[<font color=red>industrial</font>]<b>.</b> Please note that all of us work in academia and put a lot of work into this project simply because we like it, not because we are paid for it. New development efforts should go into _mlr3_ have a developer guide and our own coding style which can easily applied by using the `mlr_style` from the styler package. <b>Mlr-tutorial please read here if you want to contribute to the online manual</b>[<font color=red>industrial</font>]<b>.</b> <b>Mlr-outreach holds all outreach activities related to _mlr_ and _mlr3_</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 95 ************************************\n",
      "Number of words: 352\n",
      "Number of sentences: 22\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 5\n",
      "Total inspired: 0\n",
      "Total market: 1\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/padreati/rapaio/blob/master/README.md\n",
      "URL: https://github.com/padreati/rapaio/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " Rapaio logo(. docs logo logo-medium.png) disambiguation 1. It is also a place where there is confusion, where tricks and sims are plotted. <b>(computer science) statistics, data mining and machine learning library written in java</b>[<font color=red>industrial</font>]<b>.</b> <b>What gap rapaio tries to fill? there are a lot of software stacks out there which provides plenty of nicely crafted tools for statistics, machine learning, data mining or pattern recognition</b>[<font color=red>industrial</font>]<b>.</b> Many of them are available as open source, quality is high and they are full of reach features. <b>It appears like a legitimate question to ask why another library for statistics and machine learning?</b>[<font color=red>industrial</font>]<b>.</b> Socrates said that understanding a question is half of an answer. In our case the question would be complete if we append it with context why another library for statistics and machine learning, when there are many available already?. My answer is: because none of them covers the taste and needs of everybody. Some reasons which provide motivation for this library are: i like r. I like functional paradigms, but the r language, like it is now, it's not something which i really want to program in for a long time. <b>This comes to a price: a plethora of things required</b>[<font color=violet>market</font>]<b>.</b> But more than that, i really hate that displaying a graph in a window of a given size can be done in too many ways, many of them undocumented and hackish. But i really do not like the standard java oop way of doing things. I want to have the freedom given by a programming language, like java is. In the end, i would really love to have an environment, a box full with plenty of tools, which can be extended, which allows me to experiment, study and learn. And i want to do all those things in an interactive way, where i would program my ideas. Java community deserves such kind of tool and this library aims to fill the gap. <b>Documentation rapaio manual is now published in this repository in the form of ipython notebooks format</b>[<font color=red>industrial</font>]<b>.</b> <b>Please inspect the notebooks folder of this repository</b>[<font color=red>industrial</font>]<b>.</b> The manual contains a tutorial on kaggle's titanic competition acknowledgements many thanks to jetbrains for providing an open source license for their java ide build status."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 96 ************************************\n",
      "Number of words: 182\n",
      "Number of sentences: 8\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 2\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/BVLC/caffe/blob/master/README.md\n",
      "URL: https://github.com/BVLC/caffe/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Caffe (license) caffe is a deep learning framework made with expression, speed, and modularity in mind</b>[<font color=red>industrial</font>]<b>.</b> It is developed by berkeley ai research the berkeley vision and learning center (bvlc) and community contributors. Check out the project site for all the details like diy deep learning for vision with caffe slideid.p) tutorial documentation bair reference models and the community model zoo installation instructions step-by-step examples. <b>Custom distributions intel caffe (optimized for cpu and support for multi-node), in particular intel xeon processors</b>[<font color=red>industrial</font>]<b>.</b> Windows caffe community join the caffe-users group forum caffe-users) or gitter chat to ask questions and talk about methods and models. Framework development discussions and thorough bug reports are collected on issues brewing license and citation caffe is released under the bsd 2-clause license bair bvlc reference models are released for unrestricted use. Please cite caffe in your publications if it helps your research: @articlejia2014caffe, author jia, yangqing and shelhamer, evan and donahue, jeff and karayev, sergey and long, jonathan and girshick, ross and guadarrama, sergio and darrell, trevor , journal arxiv preprint arxiv:1408.5093 , title caffe: convolutional architecture for fast feature embedding , year 2014 '."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 97 ************************************\n",
      "Number of words: 212\n",
      "Number of sentences: 8\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 4\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/openml/openml-r/blob/master/README.md\n",
      "URL: https://github.com/openml/openml-r/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " R interface to openml.org is an online machine learning platform where researchers can access open data, download and upload data sets, share their machine learning tasks and experiments and organize them online to work and collaborate with other researchers. <b>The r interface allows to query for data sets with specific properties, and allows the downloading and uploading of data sets, tasks, flows and runs</b>[<font color=red>industrial</font>]<b>.</b> For more information, have a look at our r cheatsheet r tutorial openml r package publication openml api guide how to cite to cite the openml r package in publications, please use our paper entitled `openml`: an `r` package to connect to the machine learning platform `openml` bibtex also here for further information on how to cite the openml project itself. <b>Installation of the package install the stable version from cran or install the development version from github (using `devtools`) furthermore, you need farff installed to process arff files: alternatively you can make use of the rweka r package to process arff files</b>[<font color=red>industrial</font>]<b>.</b> <b>However, in particular for larger arff files, farff is considerably faster than rweka</b>[<font color=red>industrial</font>]<b>.</b> <b>Contact found some nasty bugs? please use the issue tracker to report on bugs or missing features</b>[<font color=red>industrial</font>]<b>.</b> Pay attention to explain the problem as good as possible (in the best case with a `traceback` result and a `sessioninfo`)."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 98 ************************************\n",
      "Number of words: 174\n",
      "Number of sentences: 8\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 6\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/komahanb/kriging/blob/master/README.md\n",
      "URL: https://github.com/komahanb/kriging/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b> kriging code for multidimensional interpolation main author : wataru yamazaki modified version: komahan boopathy, \\t \\t university of dayton, oh \\t\\t komahan.cool@gmail.com date\\t\\t : 1 31 2014 article: k</b>[<font color=red>industrial</font>]<b>.</b> <b>Rumpfkeil, \\href ``unified framework for training point selection and error estimation for surrogate models'' , aiaa journal, vol</b>[<font color=red>industrial</font>]<b>.</b> Rumpfkeil , \\t\\t title unified framework for training point selection and error estimation for surrogate models , journal aiaa journal , \\t\\t volume 53, no. <b>1 , pages 215234 , doi , year 2015 \\t\\t main program can be executed within folders test or testdrag testlift as the case might be</b>[<font color=red>industrial</font>]<b>.</b> <b>The important.f90 files of interest will be: main.f90 input parameters and settings (driver program) monaco.f90 statistics and derivatives for uq ouu post.f90 post processing on the surrogate (rmse and other tecplot output coding) dynamicpointselection.f90 dynamic training point selection based on local surrogate models\\t functions.f90 contains test functions, the user can add more</b>[<font color=red>industrial</font>]<b>.</b> <b>Subroutine version can be evoked my running. makelibrary in the main folder that creates krigingestimate.a for optimization under uncertainty</b>[<font color=red>industrial</font>]<b>.</b> <b>Dependencies: tapenade for automatic differentiation mir for local interpolation '</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 99 ************************************\n",
      "Number of words: 1461\n",
      "Number of sentences: 53\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 28\n",
      "Total inspired: 0\n",
      "Total market: 2\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/pplu/aws-sdk-perl/blob/master/README.md\n",
      "URL: https://github.com/pplu/aws-sdk-perl/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>Aws-sdk-perl attempt to build a complete aws sdk in perl this project is attempting to build an entire aws sdk from the information that is stored in other aws sdks</b>[<font color=red>industrial</font>]<b>.</b> Other aws sdks have a data-driven approach, meaning that the definitions for the method calls are stored in a data structure describing input and output parameters. The project is actually generating all of it's classes from botocore project info: travis ci status: on cpan: you want to install and use paws then just install it via cpan, cpanm, carton or the likes. If you want to contribute code: read on development setup if you want to develop a feature, or contribute code in some way, you need a development setup. This is done by cloning the repo into a local directory. With carton you can install all the dependencies needed in a local environment, so you can play around with dependencies without affecting the system libraries. <b>The cpanfile is used to track the dependencies needed</b>[<font color=red>industrial</font>]<b>.</b> <b>It's possible that you needdev libraries for compiling some of these modules</b>[<font color=red>industrial</font>]<b>.</b> These packages are: in debian ubuntu: in red hat centos: in mac os x: if yo are using mac os x el capitan(10.11) you will probably need to force the link of the openssl header to usr local: and now tell carton to install the dependecies in a local lib now we'll pull the paws-maintained fork of boto (so we can generate the sdk) now we're ready to code away happy hacking. <b>Organization builder-lib: contains classes that convert the botocore definitions into perl classes</b>[<font color=red>industrial</font>]<b>.</b> <b>Changes to code in this directory will be overwritten, so only commit autogenerated code; never handwritten code (see generating api )</b>[<font color=red>industrial</font>]<b>.</b> Changes that fix something in auto-lib are welcome, but cannot be applied directly, since they will be overwritten. Usually, they indicate some kind of general problem with other apis, so the problem has to be fixed generically. <b>Lib: contains roles and classes that the auto-generated classes use to call the api, sign requests, handle responses, etc</b>[<font color=red>industrial</font>]<b>.</b> <b>Generating api note: this step is not necessary if you want to try out the sdk</b>[<font color=red>industrial</font>]<b>.</b> We commit in auto-lib the classes generated by the definitions to which the botocore submodule points to. If you're not developing the sdk, go directly to the trying it out step :) execute command make pull-other-sdks this will do a git pull of some official aws sdks that are data-driven, and used to generate the sdk. <b>If documentation-1.json is not present in botocore botocore data for each service, which it will not when first using this project or when botocore possibly updates, then you need to first run the command to generate links to the aws docs with: this will allow the following two commands to function</b>[<font color=red>industrial</font>]<b>.</b> <b>To generate the api for a given api call: this will generate file(s) in auto-lib</b>[<font color=red>industrial</font>]<b>.</b> <b>To generate all the apis: perl versions the sdk is targeted at modern perl versions</b>[<font color=red>industrial</font>]<b>.</b> Since a new perl gets released every year, distributions perl tend to lag behind, so support for perl versions on any modern, widespread distribution is our target. Very old versions may work, but no intention to support them is made. <b>You can always install a modern version of perl with perlbrew or plenv in a breeze</b>[<font color=red>industrial</font>]<b>.</b> <b>We're running the test cases on travis for all supported perl versions</b>[<font color=red>industrial</font>]<b>.</b> <b>If you want to support a lower version, you can contribute back</b>[<font color=violet>market</font>]<b>.</b> Acceptance of patches for older versions of perl won't mean that the compatibility will be maintained long-term, although it will be tried :). Dependencies dependencies are versioned in a cpanfile. <b>If you have carton, just execute 'carton install' in the sdk directory, and all dependencies will be pulled in automatically into a local library path</b>[<font color=red>industrial</font>]<b>.</b> <b>After that use 'carton exec.' to execute your scripts</b>[<font color=red>industrial</font>]<b>.</b> <b>If you add a dependency, just add it to the cpanfile file</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> There are three sections: the general section is for dependencies that are needed only in runtime the test section is for dependecies needed to run the test suite the develop section is for dependencies needed for developers carton install installs all dependencies in all sections (after all, we're in developer mode here) packaging packaging is managed with dist::zilla. <b>To install dist::zilla and the necessary plugins, run: after this, running dzil build will make a tar.gz suitable for uploading to cpan</b>[<font color=red>industrial</font>]<b>.</b> <b>Trying it out each class for each api can be constructed in the following way: create a perl script (myscript.pl) also take a look at the cli utility (see below) for fast testing) credentials see metacpan authentication or status don't consider the sdk as stable code</b>[<font color=red>industrial</font>]<b>.</b> That said, people are using it in production, so changes to the way you call apis, although not guaranteed, are not prone to change because they are autogenerated. <b>Expect changes around the way you obtain service classes, transmit credentials, etc</b>[<font color=red>industrial</font>]<b>.</b> As of 2015-02 i'm documenting breaking changes in the changes file. <b>Api changes that break stuff will be documented there</b>[<font color=red>industrial</font>]<b>.</b> <b>Please read the changes file before updating your git clone</b>[<font color=red>industrial</font>]<b>.</b> <b>Using the sdk in your code although the code isn't considered stable yet, it works, and more than one person is using it already</b>[<font color=red>industrial</font>]<b>.</b> I recommend you to using a cpanfile, and bundling paws with carton bundle supported aws services take a look at supported-services if a service is not supported, it will warn on construction with an explicit non supported api message. <b>Basically all query and json services are supported</b>[<font color=red>industrial</font>]<b>.</b> Documentation all services get auto-generated pod documentation. <b>Perldoc a file to take a look at the documentation</b>[<font color=red>industrial</font>]<b>.</b> <b>Cli utility paws comes with a command-line utility to exercise the sdk</b>[<font color=red>industrial</font>]<b>.</b> <b>Just like paws is the namespace for the sdk, paws (in script) is the cli utility</b>[<font color=red>industrial</font>]<b>.</b> It's quite rudimentary, but think of it as a quick way to try out calling services. If a service isn't supported yet, it will die explicitly advising you that paws doesn't support that service yet. <b>Parameters needed in for describeinstances can be passed as a list of parameters (see for complete documentation of how to pass parameters via command line</b>[<font color=red>industrial</font>]<b>.</b> License this code is distributed under the apache v2 license thanks capside for letting paws be contributed in an open source model and giving me time to build and maintain it regularly. <b>Lots of work from ziprecruiter has been done via shadowcat systems alberto gimenez (@agimenez) for the git-fu cleaning up the pull other sdks code credential providers code fixes for users that have no home env variable filecaller to fully mock responses srinvas (@kidambisrinivas) for testing, bug reporting and fixing juair10 for corrections and testing chorny for cpan and cpanfile packaging corrections iigo tejedor for service endpoint resolution based on rules codehead for helping fix sqs queue maps mbartold for helping fix sqs messagebatch functionality coreymayer for reporting bug in restxmlcaller arc (aaron crane) for documentation patches dtikhonov for lwp caller and bug reporting fixing vivus-ignis for dynamodb bug reporting and test scripts for dynamodb karenetheridge for bug reporting, pull requests and help ioanrogers for fixing unicode issues in tests ilmari for fixing issues with timestamps in date and x-amz-date headers, test fixes and 5.10 support fixes, documentation issue fixes for s3, cloudfront and route53, help with number stringification stevecaldwell77 for contributing support for temporary credentials in s3 fixing test suite failure scenarios ryan olson (beerbikesbbq) for contributing documentation fixes roger pettett for testing and contributing fixes for tests on macosx henri yandell for help with licensing issues oriol soriano (@ureesoriano) for contributions to api builders and better documentation generation h</b>[<font color=red>industrial</font>]<b>.</b> Daniel cesario (@maneta) for devel setup instructions on rh and macosx glen van ginkel for contributions to get s3 working javier arellano for discovering tagging bug ioan rogers for contributing assumerolewithsaml with adfs auth example miquel soriano for reporting a bug with describeautoscalinggroups albert bendicho (wiof) for contributing better retry logic brian hartsock for better handling of xmlresponse exceptions rpcme for reporting various bugs in the sdk glenveegee for lots of work sorting out the s3 implementation grinzz many bugs, suggestions and fixes installation speedup with module::builder::tiny dakkar for solving issues with parameter passing arthur axel frew schmidt for speeding up credential refreshing popefelix for solving issues around s3 and mojoasynccaller meis for (between others): contributing paws::credential::explicit enabling unstable warnings to be silenced sven-schubert for contributing fixes to restxml services, working on fixing s3 to work correctly. <b>Septamusnonovant for fixing paginators in non-callback mode gadgetjunkie for contributing the ecs credential provider mla for contributing a fix to correct dependencies castaway for contributing to fixing documentation problems properly providing backlinks between related pages making tocs render correctly on search.cpan.org generating helpful copy-paste ready scenarios in the synopsis of each method call autarch for correcting signature generation for a bunch of services piratefinn for linking calls to documentation aws urls slobo for fixing s3 behaviour bork1n for fixes to mojoasyncaller atoomic for: tweaking cpan packaging improving paws cli leonerd for (between others) documenting retry logic fixing retry sleep of mojoasynccaller campus-explorer for contributing to test suite byterock for testing and fixing pinpoint '</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 100 ************************************\n",
      "Number of words: 1409\n",
      "Number of sentences: 71\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 38\n",
      "Total inspired: 0\n",
      "Total market: 16\n",
      "Total project: 1\n",
      "Total renown: 0\n",
      "URL: https://github.com/imbrianj/switchBoard/blob/master/README.md\n",
      "URL: https://github.com/imbrianj/switchBoard/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " 23switchboard) is a node.js based application intended to run on a device within a local network preferably a dedicated server (such as a raspberry pi). <b>It allows all web capable devices within that same network to issue commands to any other configured device</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> <b>You may use your phone, tablet, desktop or laptop browser to interact with any controllable device or issue simple get commands programmatically</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> <b>It's able to (optionally) log actions for machine learning to determine usage patterns and automate some tasks</b>[<font color=red>industrial</font>]<b>.</b> <b>Follow along development in the switchboard blog demonstration: you may browse through a static version easy you'll need to install node.js</b>[<font color=red>industrial</font>]<b>.</b> <b>You can grab it from the node.js website `npm installg git+` add a config file anywhere on the device you want to run the app on</b>[<font color=red>industrial</font>]<b>.</b> <b>See the default(config config.js) file for examples run `switchboardc yournewconfigfile` open your favorite browser on any device within your wireless network, and point it to the ip and port of the device hosting switchboard (default is 8181)</b>[<font color=red>industrial</font>]<b>.</b> <b>E.g `` (remember to bookmark) profit advanced download the source, edit config config.js to reflect your node server ip, desired port to hit when you visit the remote and web mac address of the server (used for authenticating against samsung tvs)</b>[<font color=red>industrial</font>]<b>.</b> <b>If you don't have a specific device, just comment out or remove the configuration for it</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> If you do have a device you'd like to control, just populate the given fields they should all be pretty obvious in their use. Run a command from the remote and allow access on your tv. Probably works with other brands and models, but tested on mini select v2. Works with nest thermostat and protect smoke co detectors. <b>Uses gimx to emulate ps3 controller osx not supported</b>[<font color=red>industrial</font>]<b>.</b> <b>Launch apps directly, text input and basic controls</b>[<font color=red>industrial</font>]<b>.</b> <b>Pull sports scores from espn (please don't hammer this endpoint)</b>[<font color=red>industrial</font>]<b>.</b> <b>Uses the roku controller, but it's own template with additional controls</b>[<font color=red>industrial</font>]<b>.</b> <b>Present mentions of your twitter handle or subscribe to a user's feed</b>[<font color=red>industrial</font>]<b>.</b> <b>Simple controller to load an external site as an iframe</b>[<font color=red>industrial</font>]<b>.</b> <b>Basic controls work | how to contribute i'm always looking to add devices and services</b>[<font color=red>industrial</font>]<b>.</b> Even if you're not a developer, you can help by testing, doing documentation, translating or even just expressing interest in something to help guide the effort. Join switchboard on irc.freenode.net 23switchboard) if you'd like to reach out. <b>If you work for a device manufacturer let me know if i can beg, borrow or steal a device from you to integrate controlling your ps3 general instructions overview: you'll need to have your switchboard device (computer, raspberry pi, etc.) pretend to be a ps3 controller (aka sixaxis controller) that communicates with the ps3 via bluetooth</b>[<font color=red>industrial</font>]<b>.</b> <b>You'll need a supported bluetooth dongle that plugs into your device and communicates with the console (the csr bluecore4-rom is recommended): install gimx version 2.0x+ (earlier versions won't work)</b>[<font color=red>industrial</font>]<b>.</b> <b>Refer to the detailed instruction for spoofing your bluetooth dongle's mac address linux_.2b_bluetooth_.2b_ps3)</b>[<font color=red>industrial</font>]<b>.</b> <b>:point_up: tips: the above instructions assume you have plugged your sixaxis into your ps3, pressed the controller's ps button to pair it, unplugged the controller from the ps3, and pluged back into your switchboard device</b>[<font color=red>industrial, </font> <font color=orange>project</font>]<b>.</b> <b>Keep note of the bluetooth addresses of both your ps3 controller (aka current bluetooth device address, or sixaxis_bt_address) and ps3 console (aka current bluetooth master, or ps3_bt_address)</b>[<font color=red>industrial</font>]<b>.</b> <b>You'll need to add the ps3 bluetooth address in your config config.js file and you'll need the controller address to copy over to your dongle controlling your ps3 via a raspberry pi set up as a persistent server `cd switchboard npm update sudo apt-get update sudo apt-get yes dist-upgrade sudo apt-get clean all sudo updatedb reboot` make sure your raspberry pi is up to date you'll need a supported bluetooth dongle that plugs into your device and communicates with the console (the csr bluecore4-rom is recommended): `wget sudo dpkgi gimx_3.2-1_armhf.deb` install gimx plug your ps3 controller (aka sixaxis) into your ps3, press the controller's ps button to pair it</b>[<font color=red>industrial</font>]<b>.</b> Then, unplug the controller from the ps3 and plug into your raspberry pi. <b>`sixaddr` should result in: `current bluetooth master: 90:34:fc:f7:75:e3` your ps3's bluetooth address, set this to be mac address of your ps3 within config config.js, remember to enable the device as well</b>[<font color=red>industrial</font>]<b>.</b> And `current bluetooth device address: 04:98:f3:0c:fa:6b` save for later, you can disconnect the ps3 controller now `hciconfiga` with your dongle plugged in, this should reveal the active dongle. <b>Take note of a line that looks like this, with the rest of the metadata representing your dongle: `hci0: type: br edr bus: usb etc.` `bdaddrri hci0` use the integer (in our case 0) from the hci0 output above, in this command, to set the mac address of your dongle to that of the sixaxis you saved earlier</b>[<font color=red>industrial</font>]<b>.</b> <b>Your raspberry's dongle will now pretend to be the sixaxis controller</b>[<font color=violet>market</font>]<b>.</b> <b>`switchboardc config config.js` power up your controller, hit the raspberry pi via your browser you should now see the ps3 tab, click on it, click on the power button (equivalent to the ps3's start button), and profit</b>[<font color=red>industrial</font>]<b>.</b> <b>Reference: installing on a raspberry pi (retrieved on 9.10.2014) more device installation info for details about each device's specific requirements for installation, refer to the well commented config.js(config config.js) for any given device</b>[<font color=red>industrial</font>]<b>.</b> Credit thank you to matlo from gimx for his huge help in getting the ps3 control working. If you use the ps3 functionality and enjoy it, consider a donation to his project. Nearly every controller was inspired by hard work from others. Trolling forums and seeing people's proof of concept code made many of them possible. <b>For each controller file, a relevant link to the given forum blog post article page is available in a comment at the top</b>[<font color=red>industrial</font>]<b>.</b> <b>Mp3 sounds were taken from freesound.org specific attributions for each file are in the attribution.txt(mp3 attribution.txt)</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> Also thanks to purecss.io and fontawesome.io for their assets. Contact if you have questions, comments or want to complain, email me at brian@bevey.org if you require more immediate assistance, you can join switchboard on irc.freenode.net 23switchboard) faq q. <b>Why aren't you using a seed-based js library referencing css from a cdn? a</b>[<font color=red>industrial</font>]<b>.</b> <b>I want to make sure this works without any internet access</b>[<font color=violet>market</font>]<b>.</b> You need local lan access, but nothing critical should be over the internet. <b>Some services (stocks and weather) obviously require access, but they are not core to the functionality of the app</b>[<font color=violet>market</font>]<b>.</b> <b>If you see it, congrats you're able to grab real-time info from switchboard</b>[<font color=violet>market</font>]<b>.</b> <b>Your browser will attempt to connect via websockets for real-time updates</b>[<font color=red>industrial</font>]<b>.</b> <b>If your browser does not support websockets, it'll attempt to set up standard xhr polling</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> <b>If your browser doesn't support that, you can still issue commands, but will need to manually refresh your browser for updates</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> The colors indicate: red disconnected and attempting to reconnect or waiting to reconnect gray attempting to connect or waiting to connect blue connected no indicator either your browser doesn't support websockets, xhr or something bad happened. Gray and blue xhr polling leaves the icon as gray, then will briefly flashes blue when it's grabbed the latest data. It's assumed that any device that's on your network is deemed white-listed. <b>This probably shouldn't be used on a large network with people you don't trust to screw with your tv</b>[<font color=violet>market</font>]<b>.</b> My goal is to provide the most security by keeping external connections to a minimum. <b>Why aren't you using a seed-based js library referencing css from a cdn? q</b>[<font color=red>industrial</font>]<b>.</b> Why don't you use smartthings or another third party system to do all this integration? a. <b>This means that if you wanted to change your thermostat, you'd have to send a request to smartthings, then it'd send a request to nest</b>[<font color=violet>market</font>]<b>.</b> <b>I wanted to reduce that lag but i also wanted to gain more control</b>[<font color=violet>market</font>]<b>.</b> By having switchboard do the integration, we can keep things local and we can do actions those cloud-based solutions cannot. Want to poll every 5 seconds? you'd be a jerk but you can. <b>Additionally, i wanted to support hardware that isn't supported on other systems</b>[<font color=red>industrial</font>]<b>.</b> <b>The pi is capable of any tcp commands (rest or sockets) but can also interface with bluetooth, gpio and any native unix command</b>[<font color=red>industrial</font>]<b>.</b> <b>This ability allows the ps3, text-to-speech and mp3 capabilities</b>[<font color=red>industrial</font>]<b>.</b> Allowing a browser to simply hit a url has it's advantages but if there's interest in adhering to the true spirit of rest, i can change to the correct put, post, delete commands where appropriate. I would strongly advise you to not just punch a hole in your firewall. <b>If your router supports vpn connections, it's a very safe option to configure your phone to connect to that before using switchboard remotely</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> <b>If that's not possible or not convenient, you may use ngrok to easily access switchboard (at no cost) with no additional configuration</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> <b>If you choose ngrok, be sure to configure a password auth) '</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************EXAMPLE 101 ************************************\n",
      "Number of words: 214\n",
      "Number of sentences: 6\n",
      "Total civic: 0\n",
      "Total domestic: 0\n",
      "Total green: 0\n",
      "Total industrial: 5\n",
      "Total inspired: 0\n",
      "Total market: 0\n",
      "Total project: 0\n",
      "Total renown: 0\n",
      "URL: https://github.com/ikegami-yukino/oll-python/blob/master/README.md\n",
      "URL: https://github.com/ikegami-yukino/oll-python/blob/master/README.rst\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " <b>This is a python binding of the oll library for machine learning</b>[<font color=red>industrial</font>]<b>.</b> <b>Currently, oll 0.03 supports following binary classification algorithms: perceptron averaged perceptron passive agressive (pa, pa-i, pa-ii, kernelized) alma (modified slightly from original) confidence weighted linear-classification</b>[<font color=red>industrial</font>]<b>.</b> <b>For details of oll, see: pip install oll oll library is bundled, so you don't need to install it separately</b>[<font color=red>industrial</font>]<b>.</b> <b>Code:: python import oll you can choose algorithms in p perceptron, ap averaged perceptron, pa passive agressive, pa1 passive agressive-i, pa2 passive agressive-ii, pak kernelized passive agressive, cw confidence weighted linear-classification, al alma o oll.oll( cw , c1.0, bias0.0) o.add(0: 1.0, 1: 2.0, 2:1.0 , 1) train o.classify(0:1.0, 1:1.0 ) predict o.save('oll.model') o.load('oll.model') scikit-learn like fit predict interface import numpy as np array np.array 1,1 from scipy.sparse import csr_matrix matrix csr_matrix clf.fit(dataset.datatrain_idx, dataset.targettrain_idx) occ_predicts + list(clf.predict(dataset.datatest_idx)) expected + list(dataset.targettest_idx) print('elapsed time: s' (time.time start)) print('accuracy', metrics.accuracy_score(expected, occ_predicts)) p elapsed time: 109.82188701629639 accuracy 0.770172509738 ap elapsed time: 111.42936396598816 accuracy 0.760155815248 pa elapsed time: 110.95964503288269 accuracy 0.74735670562 pa1 elapsed time: 111.39844799041748 accuracy 0.806343906511 pa2 elapsed time: 115.12716913223267 accuracy 0.766277128548 pak elapsed time: 119.53838682174683 accuracy 0.77796327212 cw elapsed time: 121.20785689353943 accuracy 0.771285475793 al elapsed time: 116.52497220039368 accuracy 0.785754034502 note this module requires c++ compiler to build</b>[<font color=red>industrial</font>]<b>.</b> <b>Oll.cpp oll.hpp : copyright (c) 2011, daisuke okanohara oll_swig_wrap.cxx is generated based on 'oll_swig.i' in oll-ruby bsd license</b>[<font color=red>industrial</font>]<b>.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f1=open('./analysis.html', 'w+')\n",
    "\n",
    "i = 0\n",
    "\n",
    "for x in range(100):\n",
    "    i = x + 1\n",
    "    print(\"****************************EXAMPLE\",i+1, \"************************************\")\n",
    "    html = \"\"\n",
    "    print(\"Number of words:\", grouped_ai_repo_analysis_df.iloc[i][11])\n",
    "    print(\"Number of sentences:\", grouped_ai_repo_analysis_df.iloc[i][12])\n",
    "    print(\"Total civic:\", grouped_ai_repo_analysis_df.iloc[i][2])\n",
    "    print(\"Total domestic:\", grouped_ai_repo_analysis_df.iloc[i][3])\n",
    "    print(\"Total green:\", grouped_ai_repo_analysis_df.iloc[i][4])\n",
    "    print(\"Total industrial:\", grouped_ai_repo_analysis_df.iloc[i][5])\n",
    "    print(\"Total inspired:\", grouped_ai_repo_analysis_df.iloc[i][6])\n",
    "    print(\"Total market:\", grouped_ai_repo_analysis_df.iloc[i][7])\n",
    "    print(\"Total project:\", grouped_ai_repo_analysis_df.iloc[i][8])\n",
    "    print(\"Total renown:\", grouped_ai_repo_analysis_df.iloc[i][9])\n",
    "    print(\"URL:\", 'https://github.com/' + grouped_ai_repo_analysis_df.iloc[i][1] + '/blob/master/README.md')\n",
    "    print(\"URL:\", 'https://github.com/' + grouped_ai_repo_analysis_df.iloc[i][1] + '/blob/master/README.rst')\n",
    "    \n",
    "    f1.write(\"<br/>****************************EXAMPLE\" + str( i+1) + \"************************************\")\n",
    "    f1.write(\"<br/>Number of words:\"+ str( grouped_ai_repo_analysis_df.iloc[i][11]))\n",
    "    f1.write(\"<br/>Number of sentences:\"+ str( grouped_ai_repo_analysis_df.iloc[i][12]))\n",
    "    f1.write(\"<br/>Total civic:\"+ str( grouped_ai_repo_analysis_df.iloc[i][2]))\n",
    "    f1.write(\"<br/>Total domestic:\"+ str( grouped_ai_repo_analysis_df.iloc[i][3]))\n",
    "    f1.write(\"<br/>Total green:\"+ str( grouped_ai_repo_analysis_df.iloc[i][4]))\n",
    "    f1.write(\"<br/>Total industrial:\"+ str( grouped_ai_repo_analysis_df.iloc[i][5]))\n",
    "    f1.write(\"<br/>Total inspired:\"+ str( grouped_ai_repo_analysis_df.iloc[i][6]))\n",
    "    f1.write(\"<br/>Total market:\"+ str( grouped_ai_repo_analysis_df.iloc[i][7]))\n",
    "    f1.write(\"<br/>Total project:\"+ str( grouped_ai_repo_analysis_df.iloc[i][8]))\n",
    "    f1.write(\"<br/>Total renown:\"+ str( grouped_ai_repo_analysis_df.iloc[i][9]))\n",
    "    f1.write(\"<br/><a href='https://github.com/\" \n",
    "             + grouped_ai_repo_analysis_df.iloc[i][1] \n",
    "             + \"/blob/master/README.md'>https://github.com/\" \n",
    "             + grouped_ai_repo_analysis_df.iloc[i][1] \n",
    "             + \"/blob/master/README.md</a>\")\n",
    "    \n",
    "    f1.write(\"<br/><a href='https://github.com/\" \n",
    "             + grouped_ai_repo_analysis_df.iloc[i][1] \n",
    "             + \"/blob/master/README.rst'>https://github.com/\" \n",
    "             + grouped_ai_repo_analysis_df.iloc[i][1] \n",
    "             + \"/blob/master/README.rst</a>\")\n",
    "    sen_arr = grouped_ai_repo_analysis_df.iloc[i][10].split(\"].\")\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    for sen in sen_arr:\n",
    "        try:\n",
    "            parts = sen.split(\"[\")\n",
    "            tags = parts[1].split(\",\")\n",
    "            del tags[-1]\n",
    "            tags = list(map(int, tags))\n",
    "            sum_list = []\n",
    "            count = 0\n",
    "            conventions = \"\"\n",
    "            for conv, x in zip(_thresholds, range(len(tags))):\n",
    "                if conv == \"industrial\":\n",
    "                    color = 'red'\n",
    "                if conv == \"civic\":\n",
    "                    color = 'blue'\n",
    "                if conv == \"domestic\":\n",
    "                    color = 'purple'  \n",
    "                if conv == \"market\":\n",
    "                    color = 'violet'  \n",
    "                if conv == \"project\":\n",
    "                    color = 'orange' \n",
    "                if conv == \"green\":\n",
    "                    color = 'green'  \n",
    "                add_text = \"<font color=\" + color + \">\" + conv + ', ' + \"</font>\"\n",
    "                \n",
    "                if tags[x] == 1:\n",
    "                    conventions =  conventions + add_text  + \" \"\n",
    "                    count = count + 1\n",
    "            \n",
    "            \n",
    "            if (count > 0):\n",
    "                conventions = conventions[:-10]+\"</font>\"\n",
    "             \n",
    "                html = html + \" \" + \"<b>\" + parts[0][:-1] + \"</b>\" + \"[\" + conventions + \"]\" + '<b>.</b>'\n",
    "            else:\n",
    "                html = html +  \" \" + parts[0]\n",
    "\n",
    "        except:\n",
    "            print(\"\")\n",
    "    display(HTML(html))\n",
    "    f1.write(\"<br/>\")\n",
    "    f1.write(\"<br/>\")\n",
    "    f1.write( html)\n",
    "    f1.write(\"<br/>\")\n",
    "    f1.write(\"<br/>\")\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T05:46:27.881174Z",
     "start_time": "2019-12-02T05:46:11.670009Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "display(grouped_ai_repo_analysis_df.head())\n",
    "from pandas.plotting import scatter_matrix\n",
    "plt.style.use('ggplot')\n",
    "sm = scatter_matrix(grouped_ai_repo_analysis_df.rename(columns={\n",
    "\"civic_y_pred\": \"civic\",\n",
    "\"domestic_y_pred\": \"domestic\",\n",
    "\"green_y_pred\": \"green\",\n",
    "\"industrial_y_pred\": \"industry\",\n",
    "\"inspired_y_pred\": \"inspired\",\n",
    "\"market_y_pred\": \"market\",\n",
    "\"project_y_pred\": \"project\",\n",
    "\"renown_y_pred\": \"renown\",\n",
    "\"word_count\": \"words\",\n",
    "}).drop(\"text\", axis = 1),figsize=[20,20], alpha=0.4, diagonal='kde')\n",
    "#kernel density estimation(KDE)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T23:00:06.914941Z",
     "start_time": "2019-12-01T23:00:06.888956Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "url = 'https://github.com/keras-team/keras/blob/master/README.md'\n",
    "iframe = '<iframe src=' + url + ' width=900 height=700></iframe>'\n",
    "IPython.display.HTML(iframe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify Reddit Repo with latest Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:06:38.027546Z",
     "start_time": "2019-11-27T12:05:49.034148Z"
    }
   },
   "outputs": [],
   "source": [
    "ai_repo_analysis_df.groupby(['comment_num'])['id'].count().plot()\n",
    "print (\"total len\",to_analyze_df.shape[0])\n",
    "print (\"total unique\",len(to_analyze_df.id.unique()))\n",
    "to_analyze_df = to_analyze_df.loc[to_analyze_df['comment_num'] > 1]\n",
    "# print (total['comment_num'].unique())\n",
    "# total.groupby(['comment_num'])['id'].count().plot()\n",
    "to_analyze_df = to_analyze_df.loc[to_analyze_df['body'] != \"[deleted]\"]\n",
    "to_analyze_df = to_analyze_df.loc[to_analyze_df['body'] != \"[removed]\"]\n",
    "to_analyze_df = to_analyze_df.loc[to_analyze_df['body'].notnull()]\n",
    "print (\"zero comments with body\", toto_analyze_dftal.shape[0])\n",
    "# URL Format = https://www.reddit.com/r/artificial/comments/all1yw/\n",
    "display(to_analyze_df.head(10))\n",
    "display(to_analyze_df.iloc[1:1])\n",
    "display(to_analyze_df.iloc[1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T12:26:21.608739Z",
     "start_time": "2019-11-27T12:26:18.689446Z"
    }
   },
   "outputs": [],
   "source": [
    "ai_repo_analysis_df = retrieve_shelved_object('my_shelf', 'ai_repo_analysis_df')\n",
    "save_to_shelf('my_shelf', 'ai_repo_analysis_df', ai_repo_analysis_df)\n",
    "get_all_shelfed('my_shelf')\n",
    "print (global_final_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T13:24:49.294392Z",
     "start_time": "2019-11-27T13:22:29.386785Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, MeanShift, AgglomerativeClustering, DBSCAN, SpectralClustering\n",
    "from matplotlib import pyplot as plt, rcParams\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "\n",
    "class Take(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Pass through a single column without modification\"\"\"\n",
    "    def __init__(self, col):\n",
    "        self.col = col\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.col].to_frame(self.col)\n",
    "name = ai_repo_analysis_df.index\n",
    "print (name [0])\n",
    "\n",
    "\n",
    "features = [\n",
    "    ('civic_y_pred', Take('civic_y_pred')),\n",
    "    ('domestic_y_pred', Take('domestic_y_pred')),\n",
    "    ('green_y_pred', Take('green_y_pred')),\n",
    "    ('industrial_y_pred', Take('industrial_y_pred')),\n",
    "    ('inspired_y_pred', Take('inspired_y_pred')),\n",
    "    ('market_y_pred', Take('market_y_pred')),\n",
    "    ('project_y_pred', Take('project_y_pred')),\n",
    "    ('renown_y_pred', Take('renown_y_pred')),\n",
    "    ('word_count', Take('word_count')),\n",
    "]\n",
    "pipe = Pipeline([\n",
    "    ('feat', FeatureUnion(features)),\n",
    "    ('scale', StandardScaler())\n",
    "])\n",
    "\n",
    "trans = pipe.fit_transform(grouped_ai_repo_analysis_df)\n",
    "\n",
    "\n",
    "# Try to cluster using KMeans for colouring out plot\n",
    "cluster = KMeans(n_clusters=8)\n",
    "group_pred = cluster.fit_predict(trans)\n",
    "\n",
    "# Perform t-SNE to reduce the dimensionality down to 2 dimenions, for easier plotting.\n",
    "tsne = TSNE(n_components=2)\n",
    "tsne_fit = tsne.fit_transform(trans)\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "trace = go.Scatter(\n",
    "    x=tsne_fit.T[0], \n",
    "    y=tsne_fit.T[1],\n",
    "    mode='markers',\n",
    "    name='Lines, Markers and Text',\n",
    "    text=name,\n",
    "    textposition='top left',\n",
    "    marker=dict(\n",
    "        color = group_pred, #set color equal to a variable\n",
    "        colorscale='Portland',\n",
    "        showscale=True\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "layout = go.Layout(\n",
    "    showlegend=False\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View a sample repo and it's convention for manual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T13:30:41.865370Z",
     "start_time": "2019-11-27T13:30:41.735445Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num  = 180\n",
    "display(grouped_ai_repo_analysis_df[num:num+1])\n",
    "display(grouped_ai_repo_analysis_df.iloc[num][8] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for repos with > or < x words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T13:33:33.283808Z",
     "start_time": "2019-11-27T13:33:33.276065Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sum_list_new = sum_list\n",
    "sum_list_new.pop()\n",
    "repos = grouped_ai_repo_analysis_df.loc[grouped_ai_repo_analysis_df['word_count'] > 10000 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T13:33:48.247042Z",
     "start_time": "2019-11-27T13:33:48.217214Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(repos.head(3) )\n",
    "display(repos.shape[0] )\n",
    "#display(green_repos.iloc[2][8] )\n",
    "display (repos.iloc[0][8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python3.6 PyEnv",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "501.215px",
    "left": "1052.22px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
