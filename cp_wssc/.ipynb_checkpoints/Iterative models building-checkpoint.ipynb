{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative AIVC models building\n",
    "\n",
    "### Implemented pipeline:\n",
    "\n",
    "#### 1.- Data gathering\n",
    "#### 2.- Classify data (probabilistic classification)\n",
    "#### 3.- Sample data from each strate of classifications (high confidence, medium, low)\n",
    "#### 4.- Manually validate data\n",
    "#### 5.- Append data to training files\n",
    "#### 6.- Retrain new models\n",
    "#### 7.- Gather new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "\n",
    "## Deep Learning imports for the classifiers\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, Concatenate\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classificaiton NETWORKs Configuration parameters\n",
    "MAX_SEQUENCE_LENGTH = 32\n",
    "MAX_NB_WORDS = 10000\n",
    "EMBEDDING_DIM = 100 ## 100, 200 or 300\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook expected the following hiearchy of files:\n",
    "\n",
    "- BASE_DIR\n",
    "     - Gathered_data\n",
    "         - Conventions\n",
    "         - Software_characteristics\n",
    "     - Classification_results\n",
    "         - Conventions\n",
    "         - Software_characteristics\n",
    "     - Training_data\n",
    "         - Conventions\n",
    "         - Software_characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_DIR = \"./Data/VALIDATED_DATA/\"\n",
    "\n",
    "BASE_DIR = \"./Data/Iterative-models-building\"\n",
    "GATHERED_DATA_FOLDER = os.path.join(BASE_DIR, \"Gathered_data\")\n",
    "\n",
    "CONVS_DIR_NAME = \"Conventions\"\n",
    "SOFT_CHARS_DIR_NAME = \"Software_characteristics\"\n",
    "\n",
    "GATHERED_SOFTWARE_CHARS_DIR = os.path.join(GATHERED_DATA_FOLDER, SOFT_CHARS_DIR_NAME)\n",
    "GATHERED_CONVENTIONS_DIR = os.path.join(GATHERED_DATA_FOLDER, CONVS_DIR_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_SAMPLING_PERCENTAJE = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Data gathering\n",
    "\n",
    "To be done for different sources, to have high variability. It's important to keep tracking of where does each sentence come from (add a label of provenance).\n",
    "Identified data sources:\n",
    "    - Google\n",
    "    - Github\n",
    "    - Semantic Scholar\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions\n",
    "def rreplace(s, old, new, occurrence):\n",
    "    li = s.rsplit(old, occurrence)\n",
    "    return new.join(li)\n",
    "\n",
    "\n",
    "def clean_file_name(name, replacements2=[]):\n",
    "    \n",
    "    replacements=[\".txt\", \".csv\", \".tsv\"]\n",
    "    \n",
    "    for r in replacements:\n",
    "        name = name.replace(r, \"\")\n",
    "        \n",
    "    for r in replacements2:\n",
    "        name = name.replace(r, \"\")\n",
    "    \n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Read conventions data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "gathered_conventions_data = {}\n",
    "\n",
    "#For each file in gathered conventions folder\n",
    "gathered_conventions_files = [f for f in os.listdir(GATHERED_CONVENTIONS_DIR) if (os.path.isfile(os.path.join(GATHERED_CONVENTIONS_DIR, f)) and not f.startswith( '.' ) and not \"random\" in f and \"gathered_\" in f)]\n",
    "gathered_conventions_files\n",
    "\n",
    "for f in gathered_conventions_files:\n",
    "    \n",
    "    gathered_conventions_data[clean_file_name(f)] = pd.read_csv(os.path.join(GATHERED_CONVENTIONS_DIR, f), sep='\\t')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Read software characteristics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "gathered_software_characteristics_data = {}\n",
    "\n",
    "#For each file in gathered software characteristics folder\n",
    "gathered_characteristics_files = [f for f in os.listdir(GATHERED_SOFTWARE_CHARS_DIR) if (os.path.isfile(os.path.join(GATHERED_SOFTWARE_CHARS_DIR, f)) and not f.startswith( '.' ) and not \"random\" in f and \"gathered_\" in f)]\n",
    "gathered_characteristics_files\n",
    "\n",
    "for f in gathered_characteristics_files:\n",
    "    gathered_software_characteristics_data[clean_file_name(f)] = pd.read_csv(os.path.join(GATHERED_SOFTWARE_CHARS_DIR, f), sep='\\t')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.- Clasification\n",
    "\n",
    "Classify, using each of the classifiers, the gathered sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "## Read pickled classifiers\n",
    "## Load convention models from pickle file\n",
    "with open(os.path.join(MODELS_DIR, 'conv_models_items.pickle'), 'rb') as f:\n",
    "\n",
    "\n",
    "    convention_convnet_items = pickle.load(f)\n",
    "\n",
    "    _conventions_models = convention_convnet_items['model'] \n",
    "    _conventions_tokenizers = convention_convnet_items['tokenizer'] \n",
    "    _conventions_data_val_x = convention_convnet_items['_x_val'] \n",
    "    _conventions_data_val_y = convention_convnet_items['_y_val'] \n",
    "    _conventions_train_histories = convention_convnet_items['train_history'] \n",
    "    \n",
    "## Load convention models from pickle file\n",
    "with open(os.path.join(MODELS_DIR, 'charact_models_items.pickle'), 'rb') as f:\n",
    "\n",
    "\n",
    "    characteristics_convnet_items = pickle.load(f)\n",
    "\n",
    "    _characteristics_models = characteristics_convnet_items['model'] \n",
    "    _characteristics_tokenizers = characteristics_convnet_items['tokenizer'] \n",
    "    _characteristics_data_val_x = characteristics_convnet_items['_x_val'] \n",
    "    _characteristics_data_val_y = characteristics_convnet_items['_y_val'] \n",
    "    _characteristics_train_histories = characteristics_convnet_items['train_history'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_matches_proba(sequences, model):\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    preds = model.predict(data)\n",
    "\n",
    "    return preds[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_matches(sentences, _models, _tokenizers):\n",
    "    _repos_matches = {}\n",
    "\n",
    "    ## Getting classification confidence per model for each repo\n",
    "    for model_key in _models.keys():\n",
    "            \n",
    "        tokenized_sentences = _tokenizers[model_key].texts_to_sequences(sentences)\n",
    "        preds = get_model_matches_proba(tokenized_sentences, _models[model_key])\n",
    "\n",
    "        _repos_matches[clean_file_name(model_key)] = preds\n",
    "    return _repos_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Put here all sentences from gathered data\n",
    "data_sentences = np.array([\"This would be talking about efficiency in industrial terms\", \"just one random example\", \"please, classify this a green world\"])\n",
    "data_sentences_provenance = np.array([\"Google\", \"Github\", \"Semantic Scholar\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training_Domestic': array([0.42488375, 0.16314507, 0.14291137], dtype=float32),\n",
       " 'training_Civic': array([0.11745402, 0.50928205, 0.55491126], dtype=float32),\n",
       " 'training_Project': array([0.21544506, 0.06054401, 0.02405908], dtype=float32),\n",
       " 'training_Inspired': array([0.45563355, 0.5758446 , 0.57550967], dtype=float32),\n",
       " 'training_Market': array([0.7195272 , 0.05723041, 0.0197148 ], dtype=float32),\n",
       " 'training_Industrial': array([0.7223793 , 0.5822472 , 0.05261192], dtype=float32),\n",
       " 'training_Renown': array([0.27047116, 0.43848282, 0.43897298], dtype=float32)}"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conventions_classifications = calculate_matches( data_sentences, _conventions_models, _conventions_tokenizers)\n",
    "conventions_classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training_advantages': array([0.46067676, 0.34974173, 0.05037144], dtype=float32),\n",
       " 'training_usability': array([0.3161091 , 0.12540503, 0.27094385], dtype=float32),\n",
       " 'training_contributions': array([0.00117153, 0.66798824, 0.9999989 ], dtype=float32),\n",
       " 'training_efficiency': array([9.9969029e-01, 1.6380979e-01, 2.3044298e-04], dtype=float32),\n",
       " 'training_licensing': array([1.6415383e-08, 1.7161448e-05, 5.8574395e-05], dtype=float32),\n",
       " 'training_reliability:maintanability': array([9.5781392e-01, 8.8595971e-04, 9.3447906e-04], dtype=float32),\n",
       " 'training_functionalities': array([6.1490632e-06, 3.1083138e-03, 3.4248501e-03], dtype=float32),\n",
       " 'training_portability': array([3.6413821e-05, 8.5168773e-05, 1.5477547e-04], dtype=float32)}"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characteristics_classifications = calculate_matches( data_sentences, _characteristics_models, _characteristics_tokenizers)\n",
    "characteristics_classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.- Sample data from each strate of classifications (high confidence, medium, low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationResult:\n",
    "        \n",
    "    def __init__(self, text, value, level, provenance='Unknown'):\n",
    "        self.text = text[0]\n",
    "        self.confidence_value = value[0]\n",
    "        self.confidence_level = level\n",
    "        self.data_provenance = provenance[0]\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"{} -- {} -- {}\".format(self.text, self.confidence_value, self.confidence_level)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"{} -- {} -- {}\".format(self.text, self.confidence_value, self.confidence_level)\n",
    "        \n",
    "        \n",
    "def split_sentences_by_confidence (calculated_classifications, _sentences, _sentences_provenance):\n",
    "\n",
    "    stratified_classifications = {}\n",
    "\n",
    "    for k in calculated_classifications.keys():\n",
    "        classifications = calculated_classifications[k]\n",
    "\n",
    "        ## Low level percentile\n",
    "        low_percentile = np.percentile(classifications, 33)\n",
    "        classifications_low = np.where(classifications<=low_percentile)\n",
    "        #print(classifications_low)\n",
    "\n",
    "        ## Medium level percentile\n",
    "        medium_percentile = np.percentile(classifications, 66)\n",
    "        classifications_medium = np.where((classifications<=medium_percentile) & (classifications>low_percentile))\n",
    "        #print(classifications_medium)\n",
    "\n",
    "        ## High level percentile\n",
    "        top_percentile = np.percentile(classifications, 100)\n",
    "        classifications_top = np.where((classifications<=top_percentile) & (classifications>medium_percentile))\n",
    "        #print(classifications_top)\n",
    "\n",
    "        classified_sentences = []\n",
    "        for i1 in classifications_low:\n",
    "            c1 = ClassificationResult(_sentences[i1], classifications[i1], \"Low\", _sentences_provenance[i1])\n",
    "            classified_sentences.append(c1)\n",
    "\n",
    "        for i2 in classifications_medium:\n",
    "            c2 = ClassificationResult(_sentences[i2], classifications[i2], \"Medium\", _sentences_provenance[i2])\n",
    "            classified_sentences.append(c2)\n",
    "\n",
    "        for i3 in classifications_top:\n",
    "            c3 = ClassificationResult(_sentences[i3], classifications[i3], \"High\", _sentences_provenance[i3])\n",
    "            classified_sentences.append(c3)\n",
    "\n",
    "\n",
    "\n",
    "        stratified_classifications[clean_file_name(k)] = classified_sentences\n",
    "        \n",
    "    return stratified_classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Conventions results sampling\n",
    "conv_stratified_classifications = split_sentences_by_confidence(conventions_classifications, data_sentences, data_sentences_provenance)\n",
    "\n",
    "for k in conv_stratified_classifications.keys():\n",
    "    with open(os.path.join(BASE_DIR, \"Classification results\", CONVS_DIR_NAME, \"{}_stratified_classifications.tsv\".format(k)), \"w\")as f3:\n",
    "\n",
    "        f3.write(\"{}\\t{}\\t{}\\t{}\\n\".format(\"text\", \"confidence_value\", \"confidence_level\", \"data_provenance\"))\n",
    "    \n",
    "        for c in conv_stratified_classifications[k]:\n",
    "            \n",
    "            f3.write(\"{}\\t{}\\t{}\\t{}\\n\".format(c.text, c.confidence_value, c.confidence_level, c.data_provenance))\n",
    "\n",
    "        f3.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please, classify this a green world\n",
      "just one random example\n"
     ]
    }
   ],
   "source": [
    "print(conv_stratified_classifications['training_Domestic'][0].text)\n",
    "print(conv_stratified_classifications['training_Domestic'][1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Characteristics results sampling\n",
    "chars_stratified_classifications = split_sentences_by_confidence(characteristics_classifications, data_sentences, data_sentences_provenance)\n",
    "for k in conv_stratified_classifications.keys():\n",
    "    with open(os.path.join(BASE_DIR, \"Classification results\", SOFT_CHARS_DIR_NAME, \"{}_stratified_classifications.tsv\".format(k)), \"w\")as f3:\n",
    "\n",
    "        f3.write(\"{}\\t{}\\t{}\\n\".format(\"text\", \"confidence_value\", \"confidence_level\", \"data_provenance\"))\n",
    "    \n",
    "        for c in conv_stratified_classifications[k]:\n",
    "            \n",
    "            f3.write(\"{}\\t{}\\t{}\\t{}\\n\".format(c.text, c.confidence_value, c.confidence_level, c.data_provenance))\n",
    "\n",
    "        f3.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps to be done manually:\n",
    "\n",
    "### ~~4.- Manually validate data~~\n",
    "### ~~5.- Append data to training files ~~\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.- Retrain new models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
