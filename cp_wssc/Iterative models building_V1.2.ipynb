{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-requisits\n",
    "- \"Models training helpers.ipynb should be contained in the same directory as this file\n",
    "    in order to run command %run \"Models training helpers.ipynb\" sort=False and utilize the helpers\n",
    "- Glove should be downloaded to 'Data/Iterative-models-building/Training data/glove.6B.100d.t\n",
    "    this is where Models training helpers.ipynb will pick it up   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Iterative Model Building - Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Pre-requisits\" data-toc-modified-id=\"Pre-requisits-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Pre-requisits</a></span></li><li><span><a href=\"#Import-&quot;Models-training-helpers.ipynb-and-3-helper-classes.\" data-toc-modified-id=\"Import-&quot;Models-training-helpers.ipynb-and-3-helper-classes.-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Import \"Models training helpers.ipynb and 3 helper classes.</a></span></li><li><span><a href=\"#Setup-Github-credentials-and-repos-etc-to-be-used-for-reading-data-and-in-some-cases-pushing-data-directly-to-github\" data-toc-modified-id=\"Setup-Github-credentials-and-repos-etc-to-be-used-for-reading-data-and-in-some-cases-pushing-data-directly-to-github-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Setup Github credentials and repos etc to be used for reading data and in some cases pushing data directly to github</a></span></li><li><span><a href=\"#Definition-of-the-various-paths-both-locally-and-directly-in-github\" data-toc-modified-id=\"Definition-of-the-various-paths-both-locally-and-directly-in-github-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Definition of the various paths both locally and directly in github</a></span></li><li><span><a href=\"#Data-gathering\" data-toc-modified-id=\"Data-gathering-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Data gathering</a></span></li><li><span><a href=\"#Clasification---load-the-classifiers-from-pickle-files\" data-toc-modified-id=\"Clasification---load-the-classifiers-from-pickle-files-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Clasification - load the classifiers from pickle files</a></span></li><li><span><a href=\"#Pass-all-the-combined-convention-sentences-through-each-convention-classifier.\" data-toc-modified-id=\"Pass-all-the-combined-convention-sentences-through-each-convention-classifier.-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Pass all the combined convention sentences through each convention classifier.</a></span></li><li><span><a href=\"#Annotate-data-for-each-type-of-classifications-(high-confidence,-medium,-low-+-the-confidence-score)\" data-toc-modified-id=\"Annotate-data-for-each-type-of-classifications-(high-confidence,-medium,-low-+-the-confidence-score)-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Annotate data for each type of classifications (high confidence, medium, low + the confidence score)</a></span></li><li><span><a href=\"#Stort-Conventions-classification-results-at:\" data-toc-modified-id=\"Stort-Conventions-classification-results-at:-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Stort Conventions classification results at:</a></span></li><li><span><a href=\"#Next-steps-to-be-done-manually-via-the-audit-tool,-which-will-created-audit-files-at-location-AUDITED_DATA_DIR_GIT\" data-toc-modified-id=\"Next-steps-to-be-done-manually-via-the-audit-tool,-which-will-created-audit-files-at-location-AUDITED_DATA_DIR_GIT-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Next steps to be done manually via the audit tool, which will created audit files at location AUDITED_DATA_DIR_GIT</a></span></li><li><span><a href=\"#Collect-Audited-Data-for-purpose-of-retraining.\" data-toc-modified-id=\"Collect-Audited-Data-for-purpose-of-retraining.-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Collect Audited Data for purpose of retraining.</a></span></li><li><span><a href=\"#Retrain-the-models-with-the-new-audited-data-and-training-data\" data-toc-modified-id=\"Retrain-the-models-with-the-new-audited-data-and-training-data-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Retrain the models with the new audited data and training data</a></span></li><li><span><a href=\"#Get-Stopwords\" data-toc-modified-id=\"Get-Stopwords-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>Get Stopwords</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import \"Models training helpers.ipynb and 3 helper classes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T00:02:30.572964Z",
     "start_time": "2019-10-14T00:02:23.598923Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Necessary libraries imported.\n",
      "Initialised generic_parsing_helpers class and methods\n",
      "Initialised AIVM_helper class and methods\n",
      "Initialised model_helpers class and methods\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "You will find instantiate_model_helpers_for_project at the very TOP of All_helper_classes.ipynb, If you do not want to change the default config, used across all files you can instantiate your own model_helpers() instead."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "SEQUENCE_LENGTH 32"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "MAX_NB_WORDS 10000"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "EMBEDDING_DIM 100"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "VALIDATION_SPLIT 0.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "TFIDF_MAX_FEATURES 10000"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "NUM_EPOCHS 20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run \"All_helper_classes.ipynb\" sort=False\n",
    "\n",
    "#from \"Models training helpers.ipynb\" we will import 3 helper classes and the associated helper methods.\n",
    "parsing_helpers = generic_parsing_helpers()\n",
    "aivm_helper = AIVM_helper()\n",
    "model_helpers = default_model_helpers_for_project()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T00:02:30.930753Z",
     "start_time": "2019-10-14T00:02:30.922020Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "//To remove scroll from any output area and automatically extend the jupyter cell\n",
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "//To remove scroll from any output area and automatically extend the jupyter cell\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Github credentials and repos etc to be used for reading data and in some cases pushing data directly to github\n",
    "\n",
    "*****Rember to add your github user and passeord here*******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T00:03:43.257680Z",
     "start_time": "2019-10-14T00:03:43.250598Z"
    }
   },
   "outputs": [],
   "source": [
    "#To read all the data files directly from  github\n",
    "USE_GIT_HUB = True \n",
    "READ_PICKLE_FROM_CLOUD = False #we have no central location as of yet for pickel files\n",
    "\n",
    "###*******GIT Hub repo ********###\n",
    "GIT_REPO = 'AIVC'\n",
    "GIT_OWNER = 'aideenf'\n",
    "GIT_OWNER_REPO = GIT_OWNER + '/'+ GIT_REPO\n",
    "\n",
    "#########To push data files directly to  github - usr/pssword required######\n",
    "GIT_USER = 'aideenf' \n",
    "GIT_PSWD = 'aid99rk4'\n",
    "###*******END-POPULATE GIT_HUB Credentials ********#########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of the various paths both locally and directly in github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T00:03:46.409553Z",
     "start_time": "2019-10-14T00:03:46.351575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "*****PROJECT PATHS*******"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "paths.BASE_DIR_LOCAL: ./Data/Iterative-models-building"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "paths.GLOVE_DIR_LOCAL : Data/Iterative-models-building/Training data/"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "paths.GATHERED_DATA_CONV_DIR_LOCAL : Data/Iterative-models-building/Gathered_data/Conventions/"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "paths.GATHERED_DATA_CONV_DIR_GIT : cp_wssc/Data/Iterative-models-building/Gathered_data/Conventions/"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "paths.GITHUB_GATHERED_URL_PATH : https://raw.githubusercontent.com/aideenf/AIVC/master/cp_wssc/Data/Iterative-models-building/Gathered_data/Conventions/"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "paths.MODELS_DIR_LOCAL : ./Data/Iterative-models-building/Models/"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "paths.STOP_WORDS_PATH : Data/Iterative-models-building/Training data/resources/stopwords.txt"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "paths.STOP_WORDS_URL_GIT : https://raw.githubusercontent.com/aideenf/AIVC/master/cp_wssc/Data/Iterative-models-building/Training data/resources/stopwords.txt"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "paths.TRAINING_DATA_DIR_FILE_LOCAL : Data/Iterative-models-building/Training data/Conventions/training_aggregated_conventions.tsv"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "paths.TRAINING_DATA_DIR_FILE_GIT : Data/Iterative-models-building/Training data/Conventions/training_aggregated_conventions.tsv"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "paths.AUDITED_DATA_DIR_LOCAL : ./Data/Iterative-models-building/Classification results/Conventions/Audited/"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "paths.AUDITED_DATA_DIR_GIT : cp_wssc/Data/Iterative-models-building/Classification results/Conventions/Audited/"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "paths.GITHUB_AUDIT_URL_PATH : https://raw.githubusercontent.com/aideenf/AIVC/master/cp_wssc/Data/Iterative-models-building/Classification%20results/Conventions/Audited/"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "paths.GITHUB_AGGREGATED_AUDIT_URL_FILE : https://raw.githubusercontent.com/aideenf/AIVC/master/cp_wssc/Data/Iterative-models-building/Classification%20results/Conventions/Audited/audited_ALL.tsv"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "paths.CONV_MODEL_PCKL_URL : not_defined"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "paths.CHARACT_MODEL_PCKL_URL : not_defined"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "paths.CLASSIFIED_DATA_DIR_LOCAL : ./Data/Iterative-models-building/Classification results/Conventions/"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "paths.CLASSIFIED_DATA_DIR_GIT : cp_wssc/Data/Iterative-models-building/Classification results/Conventions/"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#The paths are now defined in a helper class, in order to use paths.DIR\n",
    "paths = project_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data gathering\n",
    "\n",
    "To be done for different sources, to have high variability. It's important to keep tracking of where does each sentence come from (add a label of provenance).\n",
    "Identified data sources:\n",
    "    - Google\n",
    "    - Github\n",
    "    - Semantic Scholar\n",
    "    - pdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T00:03:54.212322Z",
     "start_time": "2019-10-14T00:03:54.201784Z"
    }
   },
   "outputs": [],
   "source": [
    "def aggrigate_gathered_data_from_git (gathered_data_dir_git,\n",
    "                            github_gathered_url, \n",
    "                            git_owner,\n",
    "                            git_repo) :\n",
    "\n",
    "    gathered_data_dir_git = gathered_data_dir_git\n",
    "    github_gathered_url_path = github_gathered_url\n",
    "\n",
    "    git_owner = git_owner\n",
    "    git_repo = git_repo\n",
    "    \n",
    "    gathered_conventions_files = []\n",
    "    gathered_conventions_data = {}\n",
    "    \n",
    "    gathered_conventions_files = aivm_helper.list_files_from_github_dir (git_owner, git_repo, gathered_data_dir_git)\n",
    "    gathered_conventions_files = aivm_helper.remove_excluded_files(gathered_conventions_files)\n",
    "                   #aivm_helper.remove_excluded_files_except(my_file_list, \"audited_training_data\")\n",
    "    \n",
    "    display (gathered_conventions_files)\n",
    "    #Read conventions data file from github. \n",
    "    for f in gathered_conventions_files:\n",
    "         gathered_conventions_data[aivm_helper.clean_file_name(f)] = pd.read_csv(github_gathered_url_path+f, sep='\\t', error_bad_lines=False)\n",
    "        # NOTE: \"truncated\": false  we should check for truncated = true to do follow on call to get all files\n",
    "\n",
    "    \n",
    "    return gathered_conventions_files, gathered_conventions_data\n",
    "    gc.collect()\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T00:04:10.567500Z",
     "start_time": "2019-10-14T00:03:57.196265Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gathered_github_sentences.tsv',\n",
       " 'gathered_green_test.tsv',\n",
       " 'gathered_news_sentences.tsv',\n",
       " 'gathered_pdf_green_AIFORSUSTAINABLEINTELIGENTBUILDINGS.tsv',\n",
       " 'gathered_pdf_green_Harnessing_Artificial_Intelligence_for_the_Earth_report_2018.tsv',\n",
       " 'gathered_pdf_green_introduction_to_the_green_economy_approach.tsv',\n",
       " 'gathered_s2_17-19_ki_kw.tsv']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gathered_conventions_files = []\n",
    "gathered_conventions_data = {}\n",
    "\n",
    "if(USE_GIT_HUB == False):   \n",
    "    #For each file() in gathered conventions folder\n",
    "    gathered_conventions_files = [f for f in os.listdir(paths.GATHERED_DATA_CONV_DIR_LOCAL)]\n",
    "    gathered_conventions_files = aivm_helper.remove_excluded_files(gathered_conventions_files) \n",
    "    for f in gathered_conventions_files:\n",
    "        gathered_conventions_data[aivm_helper.clean_file_name(f)] = pd.read_csv(os.path.join(paths.GATHERED_DATA_CONV_DIR_LOCAL, f), sep='\\t')\n",
    "        \n",
    "\n",
    "if(USE_GIT_HUB == True): \n",
    "        \n",
    "    gathered_conventions_files, gathered_conventions_data = aggrigate_gathered_data_from_git (paths.GATHERED_DATA_CONV_DIR_GIT,\n",
    "                            paths.GITHUB_GATHERED_URL_PATH, \n",
    "                            GIT_OWNER,\n",
    "                            GIT_REPO)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T00:04:12.197531Z",
     "start_time": "2019-10-14T00:04:12.175109Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Now we have a dictionary(gathered_conventions_data) of dataframes one entry for each gathered sentence source:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['gathered_github_sentences.tsv',\n",
       " 'gathered_green_test.tsv',\n",
       " 'gathered_news_sentences.tsv',\n",
       " 'gathered_pdf_green_AIFORSUSTAINABLEINTELIGENTBUILDINGS.tsv',\n",
       " 'gathered_pdf_green_Harnessing_Artificial_Intelligence_for_the_Earth_report_2018.tsv',\n",
       " 'gathered_pdf_green_introduction_to_the_green_economy_approach.tsv',\n",
       " 'gathered_s2_17-19_ki_kw.tsv']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>provenance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>react can also render on the server using node...</td>\n",
       "      <td>Github</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence provenance\n",
       "0  react can also render on the server using node...     Github"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display (\"Now we have a dictionary(gathered_conventions_data) of dataframes one entry for each gathered sentence source:\" , gathered_conventions_files)\n",
    "\n",
    "display (gathered_conventions_data[list(gathered_conventions_data.keys())[0]].head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasification - load the classifiers from pickle files\n",
    "\n",
    "Classify, using each of the classifiers, the gathered sentences\n",
    "Right now the pickle files are always read from the local machine as they are not compressed and are too big to store on gitHub. it will be convenient if we either\n",
    "    - push to another centralized location automatically on creation\n",
    "    \n",
    "in order to avoid the need for manual synchronization. Note that the code to read from cloud is partially implemented. see below! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T00:04:49.807464Z",
     "start_time": "2019-10-14T00:04:40.378845Z"
    }
   },
   "outputs": [],
   "source": [
    "## Read pickled classifiers\n",
    "# Note: pickle is used for serializing and de-serializing a Python object structure. \n",
    "# Any object in python can be pickled so that it can be saved on disk. \n",
    "# What pickle does is that it “serialises” the object first before writing it to file. \n",
    "# Pickling is a way to convert a python object (list, dict, etc.) into a character stream\n",
    "\n",
    "## read convention models from pickle file from gitHub or local \n",
    "if(READ_PICKLE_FROM_CLOUD == True): \n",
    "     \n",
    "    #make sure updated pickle files are in github\n",
    "    convention_convnet_items = pickle.load(urllib.request.urlopen(paths.CONV_MODEL_PCKL_CLOUD_URL))\n",
    "    characteristics_convnet_items = pickle.load(urllib.request.urlopen(paths.CHARACT_MODEL_PCKL_CLOUD_URL))\n",
    "    \n",
    "    \n",
    "elif(READ_PICKLE_FROM_CLOUD == False):\n",
    "    ## Load the  convention models from the downloaded pickle file             \n",
    "    _conventions_models, _conventions_tokenizers, _conventions_data_val_x ,_conventions_data_val_y ,_conventions_train_histories = model_helpers.read_DL_models_from_pickle( 'conv_models_items.pickle', paths.MODELS_DIR_LOCAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T00:04:51.775061Z",
     "start_time": "2019-10-14T00:04:51.489437Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Example sentence/source from combination of all data sources:"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Sentence: </b>react can also render on the server using node and power mobile apps using react native.learn how to use react in your own project."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Source: </b>Github"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sentences = []\n",
    "data_sentences_provenance = []\n",
    "\n",
    "#for each data source in the dataframe dictionary\n",
    "for k in gathered_conventions_data.keys():\n",
    "    #retrieve the dataFrame for the convention\n",
    "    conv_data_df = gathered_conventions_data[k]\n",
    "    #aggregate the source\n",
    "    provenances = conv_data_df['provenance'].values\n",
    "    #aggregate the sentence\n",
    "    texts = conv_data_df['sentence'].values\n",
    "    for i in range(len(conv_data_df)):\n",
    "        data_sentences.append( texts[i])\n",
    "        data_sentences_provenance.append(provenances[i])\n",
    "\n",
    "#Now we have a list of sentences and sentence sources which we will \n",
    "#input to the trained model and later we will evaluate and audit \n",
    "display (HTML('Example sentence/source from combination of all data sources:'))\n",
    "display (HTML('<b>Sentence: </b>'+ data_sentences[0]))\n",
    "display (HTML('<b>Source: </b>' + data_sentences_provenance[0]))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pass all the combined convention sentences through each convention classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T00:06:36.157731Z",
     "start_time": "2019-10-14T00:04:55.561383Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training_Domestic.txt': array([0.49484226, 0.5286302 , 0.3985585 , ..., 0.01692784, 0.01085482,\n",
       "        0.21079467], dtype=float32),\n",
       " 'training_Civic.txt': array([0.04898009, 0.00690331, 0.0257176 , ..., 0.00725713, 0.00043306,\n",
       "        0.01036146], dtype=float32),\n",
       " 'training_Project.txt': array([8.4338385e-01, 2.3925945e-01, 7.5533372e-01, ..., 1.7265688e-01,\n",
       "        8.0423959e-04, 9.7571075e-01], dtype=float32),\n",
       " 'training_Inspired.txt': array([0.00118321, 0.32361186, 0.02998996, ..., 0.02082202, 0.2663042 ,\n",
       "        0.00473872], dtype=float32),\n",
       " 'training_Green.txt': array([0.00276766, 0.2844936 , 0.042117  , ..., 0.08290219, 0.01610001,\n",
       "        0.01512012], dtype=float32),\n",
       " 'training_Market.txt': array([1.6413704e-01, 1.4360935e-03, 2.6892603e-04, ..., 7.1194448e-04,\n",
       "        1.2051293e-02, 1.0864363e-05], dtype=float32),\n",
       " 'training_Industrial.txt': array([0.7737953 , 0.18235698, 0.05561984, ..., 0.37716824, 0.99817514,\n",
       "        0.9960849 ], dtype=float32),\n",
       " 'training_Renown.txt': array([0.00453601, 0.05858297, 0.0797587 , ..., 0.00978723, 0.10841773,\n",
       "        0.0046782 ], dtype=float32)}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conventions_classifications = model_helpers.calculate_matches_DL( data_sentences, _conventions_models, _conventions_tokenizers)\n",
    "display (conventions_classifications)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate data for each type of classifications (high confidence, medium, low + the confidence score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T00:06:45.546743Z",
     "start_time": "2019-10-14T00:06:45.526550Z"
    }
   },
   "outputs": [],
   "source": [
    "class ClassificationResult:\n",
    "        \n",
    "    def __init__(self, text, value, level, provenance='Unknown'):\n",
    "        self.text = text\n",
    "        self.confidence_value = value\n",
    "        self.confidence_level = level\n",
    "        self.data_provenance = provenance\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"{} -- {} -- {}\".format(self.text, self.confidence_value, self.confidence_level)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"{} -- {} -- {}\".format(self.text, self.confidence_value, self.confidence_level)\n",
    "        \n",
    "        \n",
    "def split_sentences_by_confidence (calculated_classifications, _sentences, _sentences_provenance):\n",
    "\n",
    "    stratified_classifications = {}\n",
    "\n",
    "    for k in calculated_classifications.keys():\n",
    "        classifications = calculated_classifications[k]\n",
    "\n",
    "        ## Low level percentile\n",
    "        low_percentile = np.percentile(classifications, 33)#0.2\n",
    "        classifications_low = np.where(classifications<=low_percentile)[0]\n",
    "        #print(classifications_low)\n",
    "\n",
    "        ## Medium level percentile\n",
    "        medium_percentile = np.percentile(classifications, 66)#0.8\n",
    "        classifications_medium = np.where((classifications<=medium_percentile) & (classifications>low_percentile))[0]\n",
    "        #print(classifications_medium)\n",
    "\n",
    "        ## High level percentile\n",
    "        top_percentile = np.percentile(classifications, 100) #1.0\n",
    "        classifications_top = np.where((classifications<=top_percentile) & (classifications>medium_percentile))[0]\n",
    "        #print(classifications_top)\n",
    "       \n",
    "        classified_sentences = []\n",
    "        #print(classifications_low)\n",
    "        for i1 in classifications_low:\n",
    "            c1 = ClassificationResult(_sentences[i1], classifications[i1], \"Low\", _sentences_provenance[i1])\n",
    "            classified_sentences.append(c1)\n",
    "\n",
    "        for i2 in classifications_medium:\n",
    "            c2 = ClassificationResult(_sentences[i2], classifications[i2], \"Medium\", _sentences_provenance[i2])\n",
    "            classified_sentences.append(c2)\n",
    "\n",
    "        for i3 in classifications_top:\n",
    "            c3 = ClassificationResult(_sentences[i3], classifications[i3], \"High\", _sentences_provenance[i3])\n",
    "            classified_sentences.append(c3)\n",
    "            \n",
    "        stratified_classifications[aivm_helper.clean_file_name(k)] = classified_sentences\n",
    "    return stratified_classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stort Conventions classification results at:\n",
    "paths.CLASSIFIED_DATA_DIR_LOCAL \n",
    "paths.CLASSIFIED_DATA_DIR_GIT directly to git if this option is activated.\n",
    "\n",
    "in format \"training_{convention}_stratified_classifications.tsv\"\n",
    "\n",
    "###Question: why is the naming convention \"training\" when these have to be audited first. Can we simply name them\n",
    "\"{convention}_stratified_classifications.tsv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T00:06:51.588852Z",
     "start_time": "2019-10-14T00:06:51.582101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paths.CLASSIFIED_DATA_DIR_LOCAL: ./Data/Iterative-models-building/Classification results/Conventions/\n",
      "paths.CLASSIFIED_DATA_DIR_GIT: cp_wssc/Data/Iterative-models-building/Classification results/Conventions/\n"
     ]
    }
   ],
   "source": [
    "print(\"paths.CLASSIFIED_DATA_DIR_LOCAL:\", paths.CLASSIFIED_DATA_DIR_LOCAL)\n",
    "print(\"paths.CLASSIFIED_DATA_DIR_GIT:\", paths.CLASSIFIED_DATA_DIR_GIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T00:36:11.279927Z",
     "start_time": "2019-10-14T00:36:09.089300Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conv_stratified_classifications = split_sentences_by_confidence(conventions_classifications, data_sentences, data_sentences_provenance)\n",
    "\n",
    "file_list = []\n",
    "file_list_git = []\n",
    "  \n",
    "for k in conv_stratified_classifications.keys():\n",
    "    with open(os.path.join(paths.CLASSIFIED_DATA_DIR_LOCAL, \"{}_stratified_classifications.tsv\".format(k)), \"w\")as f3:\n",
    "        \n",
    "        file_list.append(paths.CLASSIFIED_DATA_DIR_LOCAL + \"{}_stratified_classifications.tsv\".format(k))\n",
    "        file_list_git.append(paths.CLASSIFIED_DATA_DIR_GIT + \"{}_stratified_classifications.tsv\".format(k))\n",
    "        \n",
    "        f3.write(\"{}\\t{}\\t{}\\t{}\\t{}\\n\".format(\"text\", \"convention\", \"confidence_value\", \"confidence_level\", \"data_provenance\"))\n",
    "    \n",
    "        for c in conv_stratified_classifications[k]:\n",
    "            convention = k.replace(\"training_\", \"\").lower()     \n",
    "            f3.write(\"{}\\t{}\\t{}\\t{}\\t{}\\n\".format(c.text, convention, c.confidence_value, c.confidence_level, c.data_provenance))\n",
    "        f3.close()\n",
    "        \n",
    "gc.collect()\n",
    "if(USE_GIT_HUB == True):\n",
    "#push to github\n",
    "        my_file_list = file_list\n",
    "        push_to_git_as = file_list_git\n",
    "        commit, message = aivm_helper.save_to_github(GIT_USER, GIT_PSWD, GIT_OWNER_REPO, my_file_list, push_to_git_as, \"auto push classified conventions\")\n",
    "\n",
    "        if (commit != \"error\"):\n",
    "            print (\"File to commit:\", my_file_list)\n",
    "            print (\"Push to git as:\", push_to_git_as)\n",
    "            print (\"Commit: \", commit)\n",
    "            display(HTML(\"<font color='green'><b>Files Merged and Saved!!</b></font>\"))\n",
    "        \n",
    "            \n",
    "        if (commit == \"error\"):\n",
    "            print (\"File to commit: \", my_file_list)\n",
    "            print (\"Push to git as: \", push_to_git_as)\n",
    "            display(HTML(\"<font color='red'><b>Warning!!</b></font>\"))\n",
    "            print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-13T23:27:43.780391Z",
     "start_time": "2019-10-13T23:27:21.663Z"
    }
   },
   "outputs": [],
   "source": [
    "##############CODE ADDED BY AIDEEN#######################################################\n",
    "#Create an aggregated DataFrame and then a tsv/csv file of the full data for auditing\n",
    "#This code is to create a file and push it to git hub to be read by the audit tool only\n",
    "# This is independant piece of code, removing it will only remove this functionality\n",
    "# leaving it here means that when we retrain the model this file will be auto updated with\n",
    "# new labels. (to discuss with David - to revert this to the format that can be used by the \n",
    "# model training process)\n",
    "df_list = [pd.read_csv(file, sep='\\t') for file in file_list]\n",
    "#concatenate them \n",
    "print(\"Concatenate all files to one\")\n",
    "convention_sentences_df = pd.concat(df_list)\n",
    "## We will perform one hot encoding\n",
    "print(\"Applying one hot encoding to convention to have format suitable for audit tool (binary and for all)\")\n",
    "convention_sentences_df[\"convention\"] = pd.Categorical(convention_sentences_df[\"convention\"])\n",
    "dfDummies = pd.get_dummies(convention_sentences_df[\"convention\"], prefix=\"convention\")\n",
    "convention_sentences_df = pd.concat([convention_sentences_df, dfDummies], axis=1)\n",
    "convention_sentences_df = convention_sentences_df.drop([\"convention\"], axis=1)\n",
    "\n",
    "## Now we will save a file 'audit_training_data.tsv'  and zipped version locally which contains all of the results\n",
    "file_name = 'audit_training_data.tsv'\n",
    "file_name_zip = 'audit_training_data.gz'\n",
    "\n",
    "convention_sentences_df.to_csv(paths.CLASSIFIED_DATA_DIR_LOCAL + file_name)\n",
    "convention_sentences_df.to_csv(paths.CLASSIFIED_DATA_DIR_LOCAL + file_name_zip, compression='gzip')\n",
    "\n",
    "## We will also push this to git hub for use by audit tool\n",
    "if(USE_GIT_HUB == True):\n",
    "    #Call helper file to zip the file for push to github\n",
    "    my_file_list = [paths.CLASSIFIED_DATA_DIR_LOCAL + file_name_zip]\n",
    "    push_to_git_as = [paths.CLASSIFIED_DATA_DIR_GIT +file_name_zip]\n",
    "\n",
    "    commit, message = aivm_helper.save_to_github(GIT_USER, GIT_PSWD, GIT_OWNER_REPO, my_file_list, push_to_git_as, \"auto push for audit\")\n",
    "\n",
    "    if (commit != \"error\"):\n",
    "        print (\"File to commit:\", my_file_list[0])\n",
    "        print (\"Push to git as:\", push_to_git_as[0])\n",
    "        print (\"Commit: \", commit)\n",
    "        display(HTML(\"<font color='green'><b>Files Merged and Saved!!</b></font>\"))\n",
    "        \n",
    "            \n",
    "    if (commit == \"error\"):\n",
    "        print (\"File to commit: \", my_file_list[0])\n",
    "        print (\"Push to git as: \", push_to_git_as[0])\n",
    "        display(HTML(\"<font color='red'><b>Warning!!</b></font>\"))\n",
    "        print(message)\n",
    "\n",
    "display (convention_sentences_df.head(2))\n",
    "display (convention_sentences_df.tail(2))\n",
    "\n",
    "############################END CODE ADDED BY AIDEEN####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T20:16:25.030994Z",
     "start_time": "2019-10-04T20:16:25.023992Z"
    }
   },
   "outputs": [],
   "source": [
    "print(conv_stratified_classifications['training_Domestic'][0].text)\n",
    "print(conv_stratified_classifications['training_Domestic'][1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps to be done manually via the audit tool, which will created audit files at location AUDITED_DATA_DIR_GIT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Audit vai tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter via provrnance or convention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Audited Data for purpose of retraining. \n",
    "The files that have been produced by the audit tool are retrieved and consolodated into a data frame, this data frame is then converted to tsv file to produce one TSV file with all audited results.\n",
    "\n",
    "This file will be called... 'audited_ALL_{date}-{time}.tsv' \n",
    "\n",
    "Any other process that wishes to use this file should look for the most recent 'audited_ALL_{date}-{time}.tsv'  or retrieve new version with this method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T01:14:19.418659Z",
     "start_time": "2019-10-14T01:14:19.403732Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Method to aggregate the audited data and to return a pandas DB containing the aggregated content. \n",
    "def aggrigate_audited_data_from_git (audited_data_dir_git,\n",
    "                            github_audit_url, \n",
    "                            audited_data_dir_local,\n",
    "                            git_user, \n",
    "                            git_pswd, \n",
    "                            git_owner_repo,\n",
    "                            git_owner,\n",
    "                            git_repo) :\n",
    "\n",
    "    audited_data_dir_git = audited_data_dir_git\n",
    "    github_audit_url = github_audit_url\n",
    "    audited_data_dir_local = audited_data_dir_local\n",
    "    git_user = git_user\n",
    "    git_pswd = git_pswd\n",
    "    git_owner_repo = git_owner_repo\n",
    "    git_owner = git_owner\n",
    "    git_repo = git_repo\n",
    "\n",
    "    # list all files from 'Audited' folder at cp_wssc/Data/Iterative-models-building/Classification%20results/Conventions/Audited/\n",
    "    # Helper file call to Returns a list of files from the github owner, repo and dir_ref\n",
    "    my_file_list = aivm_helper.list_files_from_github_dir (git_owner, git_repo, audited_data_dir_git)\n",
    "    ## Helper file call to Remove our standard excluded files and all files that are not \"audited_training_data\"\n",
    "    my_file_list = aivm_helper.remove_excluded_files_except(my_file_list, \"audited_training_data\")\n",
    "    if (len(my_file_list) > 0):\n",
    "        display (my_file_list.sort())\n",
    "        #Returns a dataframe of concatinated files from github, of names from file_list from the github url specified.\n",
    "        audited_df = aivm_helper.concat_files_from_github_dir (github_audit_url, my_file_list)\n",
    "        date, time = aivm_helper.time_stamp()\n",
    "        # file_name = 'audited_ALL_' + date + '-' + time + '.gz' # If using compression\n",
    "        #df.to_csv(FILE_PATH + file_name, sep='\\t', compression='gzip') # If using compression\n",
    "\n",
    "        file_name = 'audited_ALL_' + date + '-' + time + '.tsv'\n",
    "        file_name_unique = 'audited_ALL.tsv'\n",
    "        \n",
    "        audited_df.to_csv(audited_data_dir_local + file_name, sep='\\t')\n",
    "\n",
    "        #push to github\n",
    "        my_file_list = [audited_data_dir_local + file_name, audited_data_dir_local + file_name]\n",
    "        push_to_git_as = [audited_data_dir_git + file_name_unique, audited_data_dir_git + file_name]\n",
    "\n",
    "        commit, message = aivm_helper.save_to_github(git_user, git_pswd, git_owner_repo, my_file_list, push_to_git_as, \"auto push aggregated audit\")\n",
    "\n",
    "        if (commit != \"error\"):\n",
    "            print (\"File to commit:\", my_file_list[0])\n",
    "            print (\"Push to git as:\", push_to_git_as[0])\n",
    "            print (\"Commit: \", commit)\n",
    "            display(HTML(\"<font color='green'><b>Files Merged and Saved!!</b></font>\"))\n",
    "        \n",
    "            \n",
    "        if (commit == \"error\"):\n",
    "            print (\"File to commit: \", my_file_list[0])\n",
    "            print (\"Push to git as: \", push_to_git_as[0])\n",
    "            display(HTML(\"<font color='red'><b>Warning!!</b></font>\"))\n",
    "            print(message)\n",
    "    \n",
    "        return audited_df, message\n",
    "    else:\n",
    "        print (\"*****NO FILES RETRIEVED******\")\n",
    "    gc.collect()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-13T15:15:09.344320Z",
     "start_time": "2019-10-13T15:14:24.997837Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Call the aggrigate_audited_data method to have the aggrigated data regenerated and a pandas data frame returned.\n",
    "audited_df, message = aggrigate_audited_data_from_git (paths.AUDITED_DATA_DIR_GIT,\n",
    "                            paths.GITHUB_AUDIT_URL_PATH, \n",
    "                            paths.AUDITED_DATA_DIR_LOCAL,\n",
    "                            GIT_USER, \n",
    "                            GIT_PSWD, \n",
    "                            GIT_OWNER_REPO,\n",
    "                            GIT_OWNER,\n",
    "                            GIT_REPO)\n",
    "\n",
    "display (audited_df.head(5))\n",
    "print (message)\n",
    "print (\"The number of audited samples is\", audited_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-13T15:17:17.731583Z",
     "start_time": "2019-10-13T15:17:17.726793Z"
    }
   },
   "outputs": [],
   "source": [
    "#Method to retrieve a pandas df created by a direct call to the audited data in github. \n",
    "def retrieve_aggrigate_audited_data_from_git (path) :\n",
    "    print (path)\n",
    "    df = pd.read_csv(path, sep='\\t', error_bad_lines=False)\n",
    "    gc.collect()\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-13T15:18:06.648538Z",
     "start_time": "2019-10-13T15:18:06.011596Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calling the last created aggregation of the audited data.\n",
    "#audited_df = retrieve_aggrigate_audited_data_from_git (GITHUB_AGGREGATED_AUDIT_URL_FILE) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain the models with the new audited data and training data - still to be reviewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T20:16:44.701509Z",
     "start_time": "2019-10-04T20:16:44.693320Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T20:17:00.273447Z",
     "start_time": "2019-10-04T20:17:00.261465Z"
    }
   },
   "outputs": [],
   "source": [
    "mypath = \"Data/Training data/Conventions/\"\n",
    "training_files = [f for f in os.listdir(mypath) if (os.path.isfile(os.path.join(mypath, f)) and \"training\" in f and not f.startswith( '.' )) and not \"random\" in f]#[:2]\n",
    "training_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_LENGTH = 3000\n",
    "VALIDATION_SPLIT = 0.25\n",
    "GENERATE_NEW_TRAINING_FILES = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Stopwords\n",
    "We should also read these directly from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-13T21:39:10.938466Z",
     "start_time": "2019-10-13T21:39:10.929063Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords= parsing_helpers.get_stop_words(paths.STOP_WORDS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_conv_ML_models = {}\n",
    "_test_data_x = {}\n",
    "_test_data_y = {}\n",
    "\n",
    "\n",
    "for f in training_files:\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(\"            {}                  \".format(f))\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "\n",
    "    \n",
    "    \n",
    "    ## USING licensing text from github\n",
    "    data_train = pd.read_csv(mypath+f, sep='\\t')\n",
    "\n",
    "    #data_train = pd.read_csv('Data/TRAINING_0/training_contributions.txt', sep='\\t')\n",
    "\n",
    "    print(data_train.shape)\n",
    "\n",
    "    _texts = []\n",
    "    _labels = []\n",
    "\n",
    "    for idx in range(data_train.text.shape[0]):\n",
    "        \n",
    "        text = [data_train.text[idx]] #BeautifulSoup(data_train.text[idx])\n",
    "        \n",
    "        _texts.append(clean_str(text))#.encode('ascii','ignore')))\n",
    "        _labels.append(int(data_train.category[idx]))\n",
    "        \n",
    "    #SHUFFLE and DATA SPLITTING \n",
    "    x_train, x_val, y_train, y_val = train_test_split(_texts, _labels, test_size=VALIDATION_SPLIT, random_state=42)\n",
    "    \n",
    "    _test_data_x[f] = np.array(x_val)[np.where(np.array(y_val)==1)]\n",
    "    _test_data_y[f] = np.array(y_val)[np.where(np.array(y_val)==1)]\n",
    "    \n",
    "    model1 = train_new_text_pipelineNB(x_train, y_train)\n",
    "    tmp_pred = model1.predict(x_val)\n",
    "    print(\"Accuracy: {}\".format(np.mean(tmp_pred == y_val)))\n",
    "\n",
    "    _conv_ML_models[f] = model1\n",
    "    \n",
    "    #print(\"  Naive-Bayes: \",mean1)\n",
    "    #print(\"  Random-Forest: \",mean2)\n",
    "    #print(metrics.classification_report(_labels, pred))\n",
    "        \n",
    "    print(\"\\n\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conv_ML_matches = calculate_matches_ML( data_sentences, _conv_ML_models)\n",
    "conv_ML_matches = calculate_matches_mixture( data_sentences, _conv_ML_models, _conventions_models, _conventions_tokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_conv_ML_models['training_Domestic.txt'].predict_proba([data_sentences[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confidence_value(C):\n",
    "    return C.confidence_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Conventions results sampling\n",
    "conv_stratified_classifications = split_sentences_by_confidence(conv_ML_matches, data_sentences, data_sentences_provenance)\n",
    "\n",
    "for k in conv_stratified_classifications.keys():\n",
    "    with open(os.path.join(BASE_DIR, \"Classification results\", CONVS_DIR_NAME, \"ML_{}_stratified_classifications.tsv\".format(k)), \"w\")as f3:\n",
    "\n",
    "        f3.write(\"{}\\t{}\\t{}\\t{}\\n\".format(\"text\", \"confidence_value\", \"confidence_level\", \"data_provenance\"))\n",
    "    \n",
    "        for c in sorted(conv_stratified_classifications[k],key=get_confidence_value, reverse=True):\n",
    "            \n",
    "            f3.write(\"{}\\t{}\\t{}\\t{}\\n\".format(c.text, c.confidence_value, c.confidence_level, c.data_provenance))\n",
    "\n",
    "        f3.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_stratified_classifications.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_stratified_classifications['training_Civic'],"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Iterative Model Building - Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
