{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the helpr class file 'All_helper_classes.ipynb' and instantiate the three helper classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-13T22:43:10.696488Z",
     "start_time": "2019-10-13T22:43:10.251413Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Necessary libraries imported.\n",
      "Initialised generic_parsing_helpers class and methods\n",
      "Initialised AIVM_helper class and methods\n",
      "Initialised model_helpers class and methods\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "You will find instantiate_model_helpers_for_project at the very TOP of All_helper_classes.ipynb, If you do not want to change the default config, used across all files you can instantiate your own model_helpers() instead."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "SEQUENCE_LENGTH 32"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "MAX_NB_WORDS 10000"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "EMBEDDING_DIM 100"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "VALIDATION_SPLIT 0.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "TFIDF_MAX_FEATURES 10000"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "NUM_EPOCHS 20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run \"All_helper_classes.ipynb\" sort=False\n",
    "\n",
    "#from \"Models training helpers.ipynb\" we will import 3 helper classes and the associated helper methods.\n",
    "parsing_helpers = generic_parsing_helpers()\n",
    "aivm_helper = AIVM_helper()\n",
    "model_helpers = default_model_helpers_for_project()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-13T22:43:15.834253Z",
     "start_time": "2019-10-13T22:43:15.817389Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "//To remove scroll from any output area and automatically extend the jupyter cell\n",
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "//To remove scroll from any output area and automatically extend the jupyter cell\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get project paths from helper classes\n",
    "access using paths.THE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-13T22:43:21.134087Z",
     "start_time": "2019-10-13T22:43:20.813582Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "*****PROJECT PATHS*******"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "paths.BASE_DIR_LOCAL: ./Data/Iterative-models-building"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "paths.GLOVE_DIR_LOCAL : Data/Iterative-models-building/Training data/"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "paths.GATHERED_DATA_CONV_DIR_LOCAL : Data/Iterative-models-building/Gathered_data/Conventions/"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "paths.GATHERED_DATA_CONV_DIR_GIT : cp_wssc/Data/Iterative-models-building/Gathered_data/Conventions/"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "paths.GITHUB_GATHERED_URL_PATH : https://raw.githubusercontent.com/aideenf/AIVC/master/cp_wssc/Data/Iterative-models-building/Gathered_data/Conventions/"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "paths.MODELS_DIR_LOCAL : ./Data/Iterative-models-building/Models/"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "paths.STOP_WORDS_PATH : Data/Iterative-models-building/Training data/resources/stopwords.txt"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "paths.STOP_WORDS_URL_GIT : https://raw.githubusercontent.com/aideenf/AIVC/master/cp_wssc/Data/Iterative-models-building/Training data/resources/stopwords.txt"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "paths.TRAINING_DATA_DIR_FILE_LOCAL : Data/Iterative-models-building/Training data/Conventions/training_aggregated_conventions.tsv"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "paths.TRAINING_DATA_DIR_FILE_GIT : Data/Iterative-models-building/Training data/Conventions/training_aggregated_conventions.tsv"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "paths.AUDITED_DATA_DIR_LOCAL : ./Data/Iterative-models-building/Classification results/Conventions/Audited/"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "paths.AUDITED_DATA_DIR_GIT : cp_wssc/Data/Iterative-models-building/Classification results/Conventions/Audited/"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "paths.GITHUB_AUDIT_URL_PATH : https://raw.githubusercontent.com/aideenf/AIVC/master/cp_wssc/Data/Iterative-models-building/Classification%20results/Conventions/Audited/"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "paths.GITHUB_AGGREGATED_AUDIT_URL_FILE : https://raw.githubusercontent.com/aideenf/AIVC/master/cp_wssc/Data/Iterative-models-building/Classification%20results/Conventions/Audited/audited_ALL.tsv"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "paths.CONV_MODEL_PCKL_URL : not_defined"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "paths.CHARACT_MODEL_PCKL_URL : not_defined"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "paths.CLASSIFIED_DATA_DIR_LOCAL : ./Data/Iterative-models-building/Classification results/Conventions/"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "paths.CLASSIFIED_DATA_DIR_GIT : cp_wssc/Data/Iterative-models-building/Classification results/Conventions/"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gc.collect()\n",
    "paths = project_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We are always reading the glove embeddings from the local GLOVE_DIR with specified dimensions. These embeddings are too large to store on Git_hub, stop words are also read locally for now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-13T22:45:29.265042Z",
     "start_time": "2019-10-13T22:45:29.253291Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paths.TRAINING_DATA_DIR_FILE_LOCAL:  Data/Iterative-models-building/Training data/Conventions/training_aggregated_conventions.tsv\n",
      "paths.STOP_WORDS_PATH:  Data/Iterative-models-building/Training data/resources/stopwords.txt\n",
      "paths.GLOVE_DIR_LOCAL:  Data/Iterative-models-building/Training data/\n",
      "model_helpers.EMBEDDING_DIM:  100\n"
     ]
    }
   ],
   "source": [
    "print (\"paths.TRAINING_DATA_DIR_FILE_LOCAL: \", paths.TRAINING_DATA_DIR_FILE_LOCAL)\n",
    "print (\"paths.STOP_WORDS_PATH: \", paths.STOP_WORDS_PATH)\n",
    "print (\"paths.GLOVE_DIR_LOCAL: \", paths.GLOVE_DIR_LOCAL)\n",
    "print (\"model_helpers.EMBEDDING_DIM: \", model_helpers.EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-12T16:58:06.348367Z",
     "start_time": "2019-10-12T16:15:47.607Z"
    }
   },
   "outputs": [],
   "source": [
    "#******************THIS CODE WAS MOVED BY AF, NEED TO ASK DAVID WHY IT EXISTS***************\n",
    "\n",
    "# Read aggregated training data from TRAINING_DATA_DIR_FILE_LOCAL or TRAINING_DATA_DIR_FILE_GIT \n",
    "# and stores the content in a dataframe variable named 'training_df'. \n",
    "# Files in GATHERED_DATA_FOLDER are combined with the content of 'training_df' \n",
    "# to create the vocabulary of the Tokenizer, as by design, ```\"tokenizer.tokenize()\"``` \n",
    "# will remove any words not seen on the ```\"tokenizer.fit()\"``` stage.\n",
    "\n",
    "#TODO we need to read some of this data directly from GIT_HUB not from local.\n",
    "\n",
    "gathered_dfs = []\n",
    "\n",
    "# We are always reading the glove embeddings from the local GLOVE_DIR with specified dimensions\n",
    "# These embeddings are too large to store on Git_hub\n",
    "embeddings_index = parsing_helpers.read_glove_embeddings(paths.GLOVE_DIR_LOCAL, model_helpers.EMBEDDING_DIM)\n",
    "stopwords= parsing_helpers.get_stop_words(paths.STOP_WORDS_PATH)\n",
    "                                                      \n",
    "column_to_clean = 'text'    \n",
    "training_df = parsing_helpers.get_clean_df_text_column(paths.TRAINING_DATA_DIR_FILE, column_to_clean)\n",
    "\n",
    "gathered_data_files = [f for f in os.listdir(paths.GATHERED_CONVENTIONS_DATA_FOLDER)]\n",
    "display (\"all files in dir\", gathered_data_files)\n",
    "gathered_data_files = aivm_helper.remove_excluded_files(gathered_data_files) \n",
    "display (\"only relevant files included\",gathered_data_files)\n",
    "\n",
    "for f_name in gathered_data_files:\n",
    "    tmp_df = pd.read_csv(os.path.join(paths.GATHERED_CONVENTIONS_DATA_FOLDER, f_name), sep=\"\\t\")\n",
    "    tmp_df = tmp_df.rename(columns={\"sentence\": \"text\"})\n",
    "    gathered_dfs.append(tmp_df)\n",
    "\n",
    "gathered_dfs.append(training_df)##Adding training data sentences\n",
    "\n",
    "column_to_tokenize = 'text'\n",
    "extended_tokenizer = parsing_helpers.create_tokenizer(pd.concat(gathered_dfs, sort=True), column_to_tokenize, max_words=MAX_NB_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-12T16:58:06.370433Z",
     "start_time": "2019-10-12T16:20:03.922Z"
    }
   },
   "outputs": [],
   "source": [
    "EXECUTE_EXAMPLE = True\n",
    "if EXECUTE_EXAMPLE: #### JUST TO AVOID EXECUTING THIS CELL EVERY TIME\n",
    "\n",
    "    NUM_SENTENCES_TO_SHOW=10\n",
    "    #################################################\n",
    "    ## Example of how to use the models helper files\n",
    "    #################################################\n",
    "    display (training_df.head(3))\n",
    "    ## This is done as example, test dataframe should be readed from a diferent file!\n",
    "    test_df = training_df\n",
    "\n",
    "    ## Tokenizer was already trained!\n",
    "    assert(extended_tokenizer is not None)\n",
    "\n",
    "\n",
    "    ## Train DL models\n",
    "\n",
    "    _DLModels, _DLTokenizers, _DLData_val_x, _DLData_val_y, _DLTrain_histories = model_helpers.train_DL_models(df_train,\n",
    "                    data_class_column=\"convention\", \n",
    "                    data_label_column=\"label\",\n",
    "                    tokenizer=extended_tokenizer,\n",
    "                    random_seed=0,\n",
    "                    use_validation=False) \n",
    "\n",
    "    ## Train ML models\n",
    "    _MLModels = model_helpers.train_ML_models(df_train) \n",
    "\n",
    "    ##Getting sentences from data\n",
    "    test_sentences = test_df['text'].values\n",
    "\n",
    "    ## Calculate probability scores for for text data combining DL predictions and ML predictions\n",
    "    mixture_preds = model_helpers.calculate_matches_mixture(test_sentences, _MLModels, _DLModels, _DLTokenizers)\n",
    "\n",
    "    ## Getting top sentences per convention\n",
    "    for conv in _MLModels.keys():\n",
    "        print(\"\\n\\n     {}    \\n\\n\".format(conv))\n",
    "        for s in test_sentences[mixture_preds[conv].argsort()[-NUM_SENTENCES_TO_SHOW:][::-1]]:\n",
    "            print(\"{}\\n\".format(test_sentences[s]))\n",
    "            \n",
    "            \n",
    "    ## Store DL models in a pickle file\n",
    "    model_helpers.store_DL_models_in_picke(\"test.pickle\", _DLModels, _DLTokenizers, _DLData_val_x, _DLData_val_y, _DLTrain_histories, paths.PICKLED_MODELS_DIR)\n",
    "    \n",
    "    ## Read DL models from a pickle file\n",
    "    _DLModels, _DLTokenizers, _DLData_val_x, _DLData_val_y, _DLTrain_histories = model_helpers.read_pickle(\"test.pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONVENTION = \"Green\"\n",
    "df = training_df[training_df['convention'] == CONVENTION]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking performance setting validation data on the model training\n",
    "## Validation data is created inside the models trainign function\n",
    "conv_models = model_helpers.train_DL_models(df,\n",
    "            data_class_column=\"convention\", \n",
    "            data_label_column=\"label\",\n",
    "            df_val=None,\n",
    "            tokenizer=extended_tokenizer,\n",
    "            random_seed=0, \n",
    "            use_validation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking performance setting validation data on the model training\n",
    "## Validation data is created outside the models trainign function\n",
    "## Training models\n",
    "texts = df['text'].values\n",
    "labels = df['label'].values\n",
    "\n",
    "indices = np.arange(len(texts))\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(indices)\n",
    "texts = texts[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * len(texts))\n",
    "\n",
    "x_train = texts[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = texts[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "df_train = pd.DataFrame(columns=['text', 'label', 'convention'])\n",
    "df_train['text'] = x_train\n",
    "df_train['label'] = y_train\n",
    "df_train['convention'] = CONVENTION\n",
    "\n",
    "\n",
    "df_val = pd.DataFrame(columns=['text', 'label', 'convention'])\n",
    "df_val['text'] = x_val\n",
    "df_val['label'] = y_val\n",
    "df_val['convention'] = CONVENTION\n",
    "\n",
    "print(\"\\n\\n===========================\")\n",
    "print(\"WITH VALIDATION!\")\n",
    "print(\"===========================\\n\\n\")\n",
    "models2 = model_helpers.train_DL_models(df_train,\n",
    "                data_class_column=\"convention\", \n",
    "                data_label_column=\"label\",\n",
    "                df_val=df_val,\n",
    "                tokenizer=extended_tokenizer,\n",
    "                random_seed=0, use_validation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "### Cross validation\n",
    "######################\n",
    "\n",
    "## 1. Split training, validation data outside the training function\n",
    "## 2. Train a model with training data\n",
    "## 3. Check performance (acc) of the model on training and validation\n",
    "## 4. Repeat num_tests times\n",
    "## Next cell plots results\n",
    "\n",
    "## Number of tests = 10\n",
    "num_tests = 10\n",
    "num_epochs = 20\n",
    "\n",
    "accs = []\n",
    "val_accs = []\n",
    "epochs = np.arange(0,num_epochs, 1)\n",
    "\n",
    "texts = df['text'].values\n",
    "labels = df['label'].values\n",
    "\n",
    "\n",
    "for n in range(num_tests):\n",
    "    print(\"\\n\\n=================================\")\n",
    "    print(\"               {}\".format(n))\n",
    "    print(\"=================================\")\n",
    "    indices = np.arange(len(texts))\n",
    "    np.random.shuffle(indices)\n",
    "    texts = texts[indices]\n",
    "    labels = labels[indices]\n",
    "    nb_validation_samples = int(VALIDATION_SPLIT * len(texts))\n",
    "\n",
    "    x_train = texts[:-nb_validation_samples]\n",
    "    y_train = labels[:-nb_validation_samples]\n",
    "    x_val = texts[-nb_validation_samples:]\n",
    "    y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "    df_train = pd.DataFrame(columns=['text', 'label', 'convention'])\n",
    "    df_train['text'] = x_train\n",
    "    df_train['label'] = y_train\n",
    "    df_train['convention'] = CONVENTION\n",
    "\n",
    "\n",
    "    df_val = pd.DataFrame(columns=['text', 'label', 'convention'])\n",
    "    df_val['text'] = x_val\n",
    "    df_val['label'] = y_val\n",
    "    df_val['convention'] = CONVENTION\n",
    "\n",
    "\n",
    "    tmp_training = model_helpers.train_DL_models(df_train,\n",
    "                data_class_column=\"convention\", \n",
    "                data_label_column=\"label\",\n",
    "                df_val=df_val,\n",
    "                tokenizer=extended_tokenizer,\n",
    "                random_seed=0,\n",
    "                use_validation=True, \n",
    "                num_epochs = num_epochs)\n",
    "\n",
    "    accs.append(tmp_training[4][CONVENTION].history['acc'])\n",
    "    val_accs.append(tmp_training[4][CONVENTION].history['val_acc'])       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[10,10])\n",
    "fig.suptitle(\"Checking performance on multiple random data partitions\\n       ({} Convention)\".format(CONVENTION), fontsize=16)\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "\n",
    "for i in range(len(accs)):\n",
    "    plt.plot(epochs,accs[i], label=\"Test {}\".format(i))\n",
    "plt.title(\"Training accuracies per epoch\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"acc score\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "for i in range(len(val_accs)):\n",
    "    plt.plot(epochs,val_accs[i], label=\"Test {}\".format(i))\n",
    "plt.title(\"Validation accuracies per epoch\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"acc score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models2_tokenizer = models2[1][CONVENTION]\n",
    "val_seq = models2_tokenizer.texts_to_sequences(x_val)\n",
    "val_seq = pad_sequences(val_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "preds2 = models2[0][CONVENTION].predict(val_seq)\n",
    "print(\"Real label for samples classified with value 0\")\n",
    "display(y_val[preds2[:,0]>preds2[:,1]])\n",
    "print(\"Real label for samples classified with value 1\")\n",
    "display(y_val[preds2[:,0]<preds2[:,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n\\n===========================\")    \n",
    "print(\"WITHOUT VALIDATION!\")\n",
    "print(\"===========================\\n\\n\")\n",
    "models3 = model_helpers.train_DL_models(df_train,\n",
    "                data_class_column=\"convention\", \n",
    "                data_label_column=\"label\",\n",
    "                df_val=None,\n",
    "                tokenizer = models2_tokenizer,\n",
    "                random_seed=0, use_validation=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_seq = models2_tokenizer.texts_to_sequences(x_val)\n",
    "val_seq = pad_sequences(val_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "preds2 = models2[0][CONVENTION].predict(val_seq)\n",
    "print(\"Real label for samples classified with value 0\")\n",
    "display(y_val[preds2[:,0]>preds2[:,1]])\n",
    "print(\"Real label for samples classified with value 1\")\n",
    "display(y_val[preds2[:,0]<preds2[:,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tok1 = parsing_helpers.create_tokenizer(training_df)\n",
    "word_index = tok1.word_index\n",
    "tok1_words = []\n",
    "for word, i in word_index.items():\n",
    "    tok1_words.append(word)\n",
    "\n",
    "for idx, _df in enumerate(gathered_dfs[:-1]):\n",
    "    tok2 = parsing_helpers.create_tokenizer(_df)\n",
    "    word_index2 = tok2.word_index\n",
    "\n",
    "    gathered_f = gathered_data_files[idx]\n",
    "\n",
    "    tok2_words = []\n",
    "    for word, i in word_index2.items():\n",
    "        tok2_words.append(word)\n",
    "\n",
    "    words_in_tok1 = [w for w in tok2_words if w in tok1_words]\n",
    "    tok1_words_in_tok2 = [w for w in tok1_words if w in tok2_words]\n",
    "    print(\"\\n > {}\".format(gathered_f))\n",
    "    print(\"Gathered source words in training data: {}\".format(len(words_in_tok1) / len(tok1_words)))\n",
    "    print(\"Training data words in gathered source: {}\".format(len(tok1_words_in_tok2) / len(tok2_words)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_SENTENCES_TO_SHOW=10\n",
    "\n",
    "df_test = pd.read_csv(os.path.join(GATHERED_DATA_FOLDER, \"gathered_github_sentences.tsv\"), sep=\"\\t\")\n",
    "\n",
    "test_sentences = df_test['sentence'].values\n",
    "\n",
    "test_seq = extended_tokenizer.texts_to_sequences(test_sentences)\n",
    "test_seq = pad_sequences(val_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "preds = conv_models[0][CONVENTION].predict(test_seq)[:,1]\n",
    "\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\" >> Top sentences with higher confidence for Github:\")\n",
    "print(\"            ({} convention)\".format(CONVENTION))\n",
    "print(\"-----------------------------------------------------\\n\")\n",
    "for s in test_sentences[preds.argsort()[-NUM_SENTENCES_TO_SHOW:][::-1]]:\n",
    "    print(\"{}\\n\".format(s))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ading more green examples\n",
    "green_audit_df = pd.read_csv(\"Data/Iterative-models-building/Classification results/Conventions/Audited/audited_ALL_2019-10-08-20:23:58.598102.tsv\", sep=\"\\t\")\n",
    "green_audit_df = green_audit_df[['text', 'provenance', 'convention', 'new']]\n",
    "green_audit_df = green_audit_df.rename(columns={\"new\": \"label\"})\n",
    "green_audit_df = green_audit_df[green_audit_df['convention']=='green']\n",
    "green_audit_df['convention'] = \"Green\"\n",
    "green_audit_df.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_df = pd.concat([df, green_audit_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "### Cross validation with new data\n",
    "######################\n",
    "\n",
    "## 1. Split training, validation data outside the training function\n",
    "## 2. Train a model with training data\n",
    "## 3. Check performance (acc) of the model on training and validation\n",
    "## 4. Repeat num_tests times\n",
    "## Next cell plots results\n",
    "\n",
    "## Number of tests = 10\n",
    "num_tests = 10\n",
    "num_epochs = 20\n",
    "\n",
    "accs2 = []\n",
    "val_accs2 = []\n",
    "epochs = np.arange(0,num_epochs, 1)\n",
    "\n",
    "texts = green_df['text'].values\n",
    "labels = green_df['label'].values\n",
    "np.random.seed(0)\n",
    "\n",
    "for n in range(num_tests):\n",
    "    print(\"\\n\\n=================================\")\n",
    "    print(\"               {}\".format(n))\n",
    "    print(\"=================================\")\n",
    "    indices = np.arange(len(texts))\n",
    "\n",
    "    np.random.shuffle(indices)\n",
    "    texts = texts[indices]\n",
    "    labels = labels[indices]\n",
    "    nb_validation_samples = int(VALIDATION_SPLIT * len(texts))\n",
    "\n",
    "    x_train = texts[:-nb_validation_samples]\n",
    "    y_train = labels[:-nb_validation_samples]\n",
    "    x_val = texts[-nb_validation_samples:]\n",
    "    y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "    df_train = pd.DataFrame(columns=['text', 'label', 'convention'])\n",
    "    df_train['text'] = x_train\n",
    "    df_train['label'] = y_train\n",
    "    df_train['convention'] = CONVENTION\n",
    "\n",
    "\n",
    "    df_val = pd.DataFrame(columns=['text', 'label', 'convention'])\n",
    "    df_val['text'] = x_val\n",
    "    df_val['label'] = y_val\n",
    "    df_val['convention'] = CONVENTION\n",
    "\n",
    "\n",
    "    tmp_training = model_helpers.train_DL_models(df_train,\n",
    "                data_class_column=\"convention\", \n",
    "                data_label_column=\"label\",\n",
    "                df_val=df_val,\n",
    "                tokenizer=extended_tokenizer,\n",
    "                random_seed=0,\n",
    "                use_validation=True, \n",
    "                num_epochs = num_epochs)\n",
    "\n",
    "    accs2.append(tmp_training[4][CONVENTION].history['acc'])\n",
    "    val_accs2.append(tmp_training[4][CONVENTION].history['val_acc'])       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[10,10])\n",
    "fig.suptitle(\"Checking performance on multiple random data partitions\\n       ({} Convention)\".format(CONVENTION), fontsize=16)\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "\n",
    "for i in range(len(accs)):\n",
    "    plt.plot(epochs,accs2[i], label=\"Test {}\".format(i))\n",
    "plt.title(\"Training accuracies per epoch\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"acc score\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "for i in range(len(val_accs)):\n",
    "    plt.plot(epochs,val_accs2[i], label=\"Test {}\".format(i))\n",
    "plt.title(\"Validation accuracies per epoch\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"acc score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Checking performance setting validation data on the model training\n",
    "## Validation data is created inside the models trainign function\n",
    "conv_models2 = model_helpers.train_DL_models(green_df,\n",
    "            data_class_column=\"convention\", \n",
    "            data_label_column=\"label\",\n",
    "            df_val=None,\n",
    "            tokenizer=extended_tokenizer,\n",
    "            random_seed=0, \n",
    "            use_validation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "NUM_SENTENCES_TO_SHOW=20\n",
    "\n",
    "df_test = pd.read_csv(os.path.join(GATHERED_DATA_FOLDER, \"gathered_github_sentences.tsv\"), sep=\"\\t\")\n",
    "\n",
    "test_sentences = df_test['sentence'].values\n",
    "test_sentences = np.array([sent for sent in test_sentences if len(sent.split(\" \"))<40])\n",
    "\n",
    "test_seq = extended_tokenizer.texts_to_sequences(test_sentences)\n",
    "test_seq = pad_sequences(test_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "preds = conv_models2[0][CONVENTION].predict(test_seq)[:,1]\n",
    "\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\" >> DL model results\")\n",
    "print(\" >> Top sentences with higher confidence for Github:\")\n",
    "print(\"            ({} convention)\".format(CONVENTION))\n",
    "print(\"-----------------------------------------------------\\n\")\n",
    "for s in test_sentences[preds.argsort()[-NUM_SENTENCES_TO_SHOW:][::-1]]:\n",
    "    print(\"{}\\n\".format(s))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_DLModels = conv_models2[0]\n",
    "_DLTokenizers = conv_models2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_MLModels = model_helpers.train_ML_models(green_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = df_test['sentence'].values\n",
    "\n",
    "mixture_preds = model_helpers.calculate_matches_mixture(test_sentences, _MLModels, _DLModels, _DLTokenizers)\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\" >> Mixture model results\")\n",
    "print(\" >> Top sentences with higher confidence for Github:\")\n",
    "print(\"            ({} convention)\".format(CONVENTION))\n",
    "print(\"-----------------------------------------------------\\n\")\n",
    "for s in test_sentences[mixture_preds[CONVENTION].argsort()[-NUM_SENTENCES_TO_SHOW:][::-1]]:\n",
    "    print(\"{}\\n\".format(s))\n",
    "    \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
