{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Required Python utilities\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "from langdetect import detect\n",
    "from bs4 import BeautifulSoup\n",
    "from markdown import markdown\n",
    "from lxml import etree\n",
    "import os\n",
    "import random\n",
    "import tqdm\n",
    "import itertools \n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "## Deep Learning imports for the classifiers\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, Concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "##Â ML required imports (for clustering)\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA, LatentDirichletAllocation\n",
    "from sklearn.preprocessing import scale, StandardScaler\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "\n",
    "\n",
    "# Topic modeling imports\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "##Â NLP related imports\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "# visualization imports\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import base64\n",
    "import io\n",
    "%matplotlib inline\n",
    "sns.set() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imported_notebook = True ## Set this flag to true if importing this code from another notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_DIR = \"Data/Iterative-models-building/Training data/\"\n",
    "GATHERED_DATA_FOLDER = \"Data/Iterative-models-building/Gathered_data/Conventions/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classificaiton NETWORKs Configuration parameters\n",
    "MAX_SEQUENCE_LENGTH = 40\n",
    "MAX_NB_WORDS = 10000\n",
    "EMBEDDING_DIM = 100 ## 100, 200 or 300\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(df, max_words=MAX_NB_WORDS):\n",
    "    texts = df['text'].values\n",
    "    \n",
    "    _tokenizer = Tokenizer(num_words=max_words, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "    _tokenizer.fit_on_texts(texts)\n",
    "    \n",
    "    return _tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_embeddings():\n",
    "    ## Reading GLOVE (precalculated word embeddings)\n",
    "    embeddings_index = {}\n",
    "    f = open(os.path.join(GLOVE_DIR, 'glove.6B.{}d.txt'.format(EMBEDDING_DIM)))\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    \n",
    "    return embeddings_index\n",
    "\n",
    "embeddings_index = read_glove_embeddings()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate AUC\n",
    "## As explained here: https://stackoverflow.com/questions/41032551/how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras\n",
    "class roc_callback(Callback):\n",
    "    def __init__(self,validation_data):#training_data,validation_data):\n",
    "        #self.x = training_data[0]\n",
    "        #self.y = training_data[1]\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        #y_pred = self.model.predict(self.x)\n",
    "        #roc = roc_auc_score(self.y, y_pred)\n",
    "        y_pred_val = self.model.predict(self.x_val)\n",
    "        roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
    "        #print('\\rroc-auc: %s - roc-auc_val: %s' % (str(round(roc,4)),str(round(roc_val,4))),end=100*' '+'\\n')\n",
    "        print(\"Roc-AUC Validation: {}\".format(str(round(roc_val,4))))\n",
    "        return\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_matches_proba(sequences, model):\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    preds = model.predict(data)\n",
    "\n",
    "    return preds[:,1]\n",
    "\n",
    "\n",
    "def calculate_matches(sentences, _models, _tokenizers):\n",
    "    _repos_matches = {}\n",
    "\n",
    "    ## Getting classification confidence per model for each repo\n",
    "    for model_key in _models.keys():\n",
    "            \n",
    "        tokenized_sentences = _tokenizers[model_key].texts_to_sequences(sentences)\n",
    "        preds1 = get_model_matches_proba(tokenized_sentences, _models[model_key])\n",
    "        \n",
    "        \"\"\"\n",
    "        preds2 = get_model_matches_proba(tokenized_sentences, _models[model_key])\n",
    "        \n",
    "        data = pad_sequences(tokenized_sentences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "        preds2 = _models[model_key].predict(data)\n",
    "\n",
    "        _repos_matches[model_key] = [preds1, preds2]\n",
    "        \"\"\"\n",
    "        _repos_matches[model_key] = preds1\n",
    "    return _repos_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_DL_model(x_train, y_train, x_val, y_val, tokenizer, num_epochs=NUM_EPOCHS):\n",
    "    \n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    print('\\nNumber of elements from each class in traing and validation set ')\n",
    "    print(y_train.sum(axis=0))\n",
    "    print(y_val.sum(axis=0))\n",
    "\n",
    "\n",
    "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)\n",
    "    \n",
    "\n",
    "    convs = []\n",
    "    filter_sizes = [3,4,5]\n",
    "\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    for fsz in filter_sizes:\n",
    "        l_conv = Conv1D(nb_filter=128,filter_length=fsz,activation='relu')(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(5)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Concatenate(axis=1)(convs)#\n",
    "\n",
    "    l_cov1= Conv1D(128, 5, activation='relu')(l_merge)\n",
    "    l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "\n",
    "    l_flat = Flatten()(l_pool1)\n",
    "    l_dense = Dense(128, activation='relu')(l_flat)\n",
    "    preds = Dense(2, activation='softmax')(l_dense)\n",
    "    \n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # train a 1D convnet with global maxpooling\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(128, 5, activation='relu')(x)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(128, 5, activation='relu')(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"model fitting - more complex convolutional neural network\")\n",
    "    model.summary()\n",
    "    train_history = model.fit(x_train, y_train, validation_data=(x_val, y_val),epochs=num_epochs, batch_size=50, callbacks=[roc_callback(validation_data=(x_val, y_val))])\n",
    "\n",
    "    return model,tokenizer,x_val,y_val, train_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_DL_model_not_validation(x_train, y_train, tokenizer, num_epochs=NUM_EPOCHS):\n",
    "    \n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    print('\\nNumber of elements from each class in traing and validation set ')\n",
    "    print(y_train.sum(axis=0))\n",
    "    print(y_val.sum(axis=0))\n",
    "\n",
    "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)\n",
    "\n",
    "\n",
    "    convs = []\n",
    "    filter_sizes = [3,4,5]\n",
    "\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    for fsz in filter_sizes:\n",
    "        l_conv = Conv1D(nb_filter=128,filter_length=fsz,activation='relu')(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(5)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Concatenate(axis=1)(convs)#\n",
    "\n",
    "    l_cov1= Conv1D(128, 5, activation='relu')(l_merge)\n",
    "    l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "\n",
    "    l_flat = Flatten()(l_pool1)\n",
    "    l_dense = Dense(128, activation='relu')(l_flat)\n",
    "    preds = Dense(2, activation='softmax')(l_dense)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    print(\"model fitting - more complex convolutional neural network\")\n",
    "    model.summary()\n",
    "    train_history = model.fit(x_train, y_train,epochs=num_epochs, batch_size=50)\n",
    "    return model,tokenizer,train_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_DL_models(df_train,\n",
    "                    data_class_column=\"convention\", \n",
    "                    data_label_column=\"label\",\n",
    "                   df_val=None,\n",
    "                    tokenizer=None,\n",
    "                   random_seed=None,\n",
    "                   use_validation=True):\n",
    "    \n",
    "    \n",
    "    _models = {}\n",
    "    _tokenizers = {}\n",
    "    _data_val_x = {}\n",
    "    _data_val_y = {}\n",
    "    _train_histories = {}\n",
    "    if tokenizer is None:        \n",
    "        if df_val is None:\n",
    "            tokenizer = create_tokenizer(df_train)\n",
    "        else:\n",
    "            tokenizer = create_tokenizer(pd.concat([df_train, df_val]))\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    \n",
    "    for convention in df[data_class_column].unique():\n",
    "        \n",
    "        print(\"----------------------------------------------------------------\")\n",
    "        print(\"            {}                  \".format(convention))\n",
    "        print(\"----------------------------------------------------------------\")\n",
    "        \n",
    "        tmp_df_train = df_train[df_train[data_class_column] == convention]\n",
    "        if use_validation:\n",
    "            if df_val is None:\n",
    "\n",
    "                texts = tmp_df_train['text'].values\n",
    "                labels = tmp_df_train[data_label_column].values\n",
    "\n",
    "                sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "\n",
    "                data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "                labels = to_categorical(np.asarray(labels))\n",
    "                print('Shape of data tensor:', data.shape)\n",
    "                print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "                indices = np.arange(data.shape[0])\n",
    "                if random_seed is not None:\n",
    "                    np.random.seed(random_seed)\n",
    "                np.random.shuffle(indices)\n",
    "                data = data[indices]\n",
    "                labels = labels[indices]\n",
    "                nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "                x_train = data[:-nb_validation_samples]\n",
    "                y_train = labels[:-nb_validation_samples]\n",
    "                x_val = data[-nb_validation_samples:]\n",
    "                y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "\n",
    "                _model, _tokenizer, _x_val, _y_val, _train_h = train_DL_model(x_train, y_train, x_val, y_val, tokenizer)\n",
    "            else:\n",
    "\n",
    "                tmp_df_val = df_val[df_val[data_class_column] == convention]\n",
    "\n",
    "                train_texts = tmp_df_train['text'].values\n",
    "                train_labels = tmp_df_train[data_label_column].values\n",
    "\n",
    "                val_texts = tmp_df_val['text'].values\n",
    "                val_labels = tmp_df_val[data_label_column].values\n",
    "\n",
    "                train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "                val_sequences = tokenizer.texts_to_sequences(val_texts)\n",
    "\n",
    "\n",
    "\n",
    "                x_train = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "                x_val = pad_sequences(val_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "                y_train = to_categorical(np.asarray(train_labels))\n",
    "                y_val = to_categorical(np.asarray(val_labels))\n",
    "\n",
    "                _model, _tokenizer, _x_val, _y_val, _train_h = train_DL_model(x_train, y_train, x_val, y_val, tokenizer)\n",
    "        \n",
    "            _models[convention] = _model\n",
    "            _tokenizers[convention] = _tokenizer\n",
    "            _data_val_x[convention] = _x_val\n",
    "            _data_val_y[convention] = _y_val\n",
    "            _train_histories[convention] = _train_h\n",
    "        else:\n",
    "            texts = tmp_df_train['text'].values\n",
    "            labels = tmp_df_train[data_label_column].values\n",
    "\n",
    "            sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "\n",
    "            data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "            labels = to_categorical(np.asarray(labels))\n",
    "            print('Shape of data tensor:', data.shape)\n",
    "            print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "            \n",
    "            x_train = data\n",
    "            y_train = labels\n",
    "        \n",
    "            _model, _tokenizer, _train_h = train_DL_model_not_validation(x_train, y_train, tokenizer)\n",
    "\n",
    "            _models[convention] = _model\n",
    "            _tokenizers[convention] = _tokenizer\n",
    "            _train_histories[convention] = _train_h\n",
    "        \n",
    "\n",
    "        print(\"\\n\\n\\n\")\n",
    "        \n",
    "    return (_models, _tokenizers, _data_val_x, _data_val_y, _train_histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not imported_notebook:\n",
    "    df = pd.read_csv(\"Data/Iterative-models-building/Training data/Conventions/training_aggregated_conventions.tsv\", sep=\"\\t\")\n",
    "    df = df[df['convention'] =='Industrial']\n",
    "    \n",
    "    def remove_line_breaks(text_str):\n",
    "        text_str = text_str.replace(\"\\\\n\", \" \")\n",
    "        return text_str\n",
    "    \n",
    "    df['text'] = df['text'].apply(remove_line_breaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>provenance</th>\n",
       "      <th>convention</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3224</td>\n",
       "      <td>1</td>\n",
       "      <td>also, you can install drivers for various vm p...</td>\n",
       "      <td>Manually_gathered</td>\n",
       "      <td>Industrial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3225</td>\n",
       "      <td>1</td>\n",
       "      <td>will return a bar plot comparing the models on...</td>\n",
       "      <td>Manually_gathered</td>\n",
       "      <td>Industrial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3226</td>\n",
       "      <td>1</td>\n",
       "      <td>* expected result: 2.63% test error rate with ...</td>\n",
       "      <td>Manually_gathered</td>\n",
       "      <td>Industrial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3227</td>\n",
       "      <td>1</td>\n",
       "      <td>if you know your terminal size, you can contro...</td>\n",
       "      <td>Manually_gathered</td>\n",
       "      <td>Industrial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3228</td>\n",
       "      <td>1</td>\n",
       "      <td>- to provide a range of non-functional feature...</td>\n",
       "      <td>Manually_gathered</td>\n",
       "      <td>Industrial</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text  \\\n",
       "3224      1  also, you can install drivers for various vm p...   \n",
       "3225      1  will return a bar plot comparing the models on...   \n",
       "3226      1  * expected result: 2.63% test error rate with ...   \n",
       "3227      1  if you know your terminal size, you can contro...   \n",
       "3228      1  - to provide a range of non-functional feature...   \n",
       "\n",
       "             provenance  convention  \n",
       "3224  Manually_gathered  Industrial  \n",
       "3225  Manually_gathered  Industrial  \n",
       "3226  Manually_gathered  Industrial  \n",
       "3227  Manually_gathered  Industrial  \n",
       "3228  Manually_gathered  Industrial  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gathered_green_test.tsv',\n",
       " 'gathered_s2_17-19_ki.tsv',\n",
       " 'gathered_news_sentences.tsv',\n",
       " '_gathered_github_sentences.tsv',\n",
       " 'gathered_github_sentences.tsv',\n",
       " 'gathered_s2_17-19_ki_kw.tsv']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gathered_data_files = [f for f in os.listdir(GATHERED_DATA_FOLDER) \n",
    "                              if (os.path.isfile(os.path.join(GATHERED_DATA_FOLDER, f)) \n",
    "                                  and not f.startswith( '.' ) \n",
    "                                  and \"gathered_\" in f)]\n",
    "gathered_data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gathered_dfs = []\n",
    "\n",
    "for f_name in gathered_data_files:\n",
    "    tmp_df = pd.read_csv(os.path.join(GATHERED_DATA_FOLDER, f_name), sep=\"\\t\")\n",
    "    tmp_df = tmp_df.rename(columns={\"sentence\": \"text\"})\n",
    "    \n",
    "    gathered_dfs.append(tmp_df)\n",
    "\n",
    "gathered_dfs.append(df)##Adding training data sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "extended_tokenizer = create_tokenizer(pd.concat(gathered_dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 85688 unique tokens.\n",
      "----------------------------------------------------------------\n",
      "            Industrial                  \n",
      "----------------------------------------------------------------\n",
      "Shape of data tensor: (1738, 40)\n",
      "Shape of label tensor: (1738, 2)\n",
      "\n",
      "Number of elements from each class in traing and validation set \n",
      "[910. 481.]\n",
      "[231. 116.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=128, kernel_size=3)`\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=128, kernel_size=4)`\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=128, kernel_size=5)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - more complex convolutional neural network\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 40, 100)      8568900     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 38, 128)      38528       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 37, 128)      51328       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 36, 128)      64128       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 7, 128)       0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 7, 128)       0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 7, 128)       0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 21, 128)      0           max_pooling1d_13[0][0]           \n",
      "                                                                 max_pooling1d_14[0][0]           \n",
      "                                                                 max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 17, 128)      82048       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 3, 128)       0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 384)          0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 128)          49280       flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 2)            258         dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,854,470\n",
      "Trainable params: 285,570\n",
      "Non-trainable params: 8,568,900\n",
      "__________________________________________________________________________________________________\n",
      "Train on 1391 samples, validate on 347 samples\n",
      "Epoch 1/20\n",
      "1391/1391 [==============================] - 1s 675us/step - loss: 0.8005 - acc: 0.6219 - val_loss: 0.5194 - val_acc: 0.7032\n",
      "Roc-AUC Validation: 0.8314\n",
      "Epoch 2/20\n",
      "1391/1391 [==============================] - 0s 311us/step - loss: 0.5304 - acc: 0.7383 - val_loss: 0.3840 - val_acc: 0.8357\n",
      "Roc-AUC Validation: 0.9062\n",
      "Epoch 3/20\n",
      "1391/1391 [==============================] - 0s 319us/step - loss: 0.4354 - acc: 0.7850 - val_loss: 0.3664 - val_acc: 0.8415\n",
      "Roc-AUC Validation: 0.9313\n",
      "Epoch 4/20\n",
      "1391/1391 [==============================] - 0s 319us/step - loss: 0.3022 - acc: 0.8670 - val_loss: 0.3057 - val_acc: 0.8617\n",
      "Roc-AUC Validation: 0.9434\n",
      "Epoch 5/20\n",
      "1391/1391 [==============================] - 0s 310us/step - loss: 0.2344 - acc: 0.9015 - val_loss: 0.4043 - val_acc: 0.8271\n",
      "Roc-AUC Validation: 0.9357\n",
      "Epoch 6/20\n",
      "1391/1391 [==============================] - 0s 303us/step - loss: 0.1716 - acc: 0.9324 - val_loss: 0.3273 - val_acc: 0.8732\n",
      "Roc-AUC Validation: 0.9517\n",
      "Epoch 7/20\n",
      "1391/1391 [==============================] - 0s 327us/step - loss: 0.1399 - acc: 0.9439 - val_loss: 0.2486 - val_acc: 0.9078\n",
      "Roc-AUC Validation: 0.9588\n",
      "Epoch 8/20\n",
      "1391/1391 [==============================] - 0s 314us/step - loss: 0.1029 - acc: 0.9619 - val_loss: 0.2960 - val_acc: 0.8934\n",
      "Roc-AUC Validation: 0.9613\n",
      "Epoch 9/20\n",
      "1391/1391 [==============================] - 0s 318us/step - loss: 0.0803 - acc: 0.9756 - val_loss: 0.3375 - val_acc: 0.8761\n",
      "Roc-AUC Validation: 0.9488\n",
      "Epoch 10/20\n",
      "1391/1391 [==============================] - 0s 305us/step - loss: 0.0587 - acc: 0.9799 - val_loss: 0.4896 - val_acc: 0.8501\n",
      "Roc-AUC Validation: 0.9524\n",
      "Epoch 11/20\n",
      "1391/1391 [==============================] - 0s 333us/step - loss: 0.0861 - acc: 0.9712 - val_loss: 0.2714 - val_acc: 0.8991\n",
      "Roc-AUC Validation: 0.9644\n",
      "Epoch 12/20\n",
      "1391/1391 [==============================] - 0s 333us/step - loss: 0.0392 - acc: 0.9856 - val_loss: 0.3825 - val_acc: 0.8876\n",
      "Roc-AUC Validation: 0.9632\n",
      "Epoch 13/20\n",
      "1391/1391 [==============================] - 0s 327us/step - loss: 0.0675 - acc: 0.9727 - val_loss: 0.2948 - val_acc: 0.8934\n",
      "Roc-AUC Validation: 0.9615\n",
      "Epoch 14/20\n",
      "1391/1391 [==============================] - 0s 322us/step - loss: 0.0298 - acc: 0.9907 - val_loss: 0.4221 - val_acc: 0.8761\n",
      "Roc-AUC Validation: 0.9158\n",
      "Epoch 15/20\n",
      "1391/1391 [==============================] - 0s 305us/step - loss: 0.0873 - acc: 0.9698 - val_loss: 0.3228 - val_acc: 0.9107\n",
      "Roc-AUC Validation: 0.9624\n",
      "Epoch 16/20\n",
      "1391/1391 [==============================] - 0s 316us/step - loss: 0.0192 - acc: 0.9871 - val_loss: 0.3656 - val_acc: 0.9135\n",
      "Roc-AUC Validation: 0.961\n",
      "Epoch 17/20\n",
      "1391/1391 [==============================] - 0s 310us/step - loss: 0.0821 - acc: 0.9734 - val_loss: 0.3697 - val_acc: 0.9078\n",
      "Roc-AUC Validation: 0.9564\n",
      "Epoch 18/20\n",
      "1391/1391 [==============================] - 0s 312us/step - loss: 0.0187 - acc: 0.9892 - val_loss: 0.3773 - val_acc: 0.9107\n",
      "Roc-AUC Validation: 0.9603\n",
      "Epoch 19/20\n",
      "1391/1391 [==============================] - 0s 318us/step - loss: 0.0679 - acc: 0.9835 - val_loss: 0.3348 - val_acc: 0.9107\n",
      "Roc-AUC Validation: 0.964\n",
      "Epoch 20/20\n",
      "1391/1391 [==============================] - 0s 332us/step - loss: 0.0179 - acc: 0.9935 - val_loss: 0.4072 - val_acc: 0.9020\n",
      "Roc-AUC Validation: 0.9622\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not imported_notebook:\n",
    "    conv_models = train_DL_models(df,\n",
    "                data_class_column=\"convention\", \n",
    "                data_label_column=\"label\",\n",
    "                df_val=None,\n",
    "                tokenizer=extended_tokenizer,\n",
    "                random_seed=0, \n",
    "                use_validation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===========================\n",
      "WITH VALIDATION!\n",
      "===========================\n",
      "\n",
      "\n",
      "Found 85688 unique tokens.\n",
      "----------------------------------------------------------------\n",
      "            Industrial                  \n",
      "----------------------------------------------------------------\n",
      "\n",
      "Number of elements from each class in traing and validation set \n",
      "[910. 481.]\n",
      "[231. 116.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=128, kernel_size=3)`\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=128, kernel_size=4)`\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=128, kernel_size=5)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - more complex convolutional neural network\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 40, 100)      8568900     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 38, 128)      38528       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 37, 128)      51328       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 36, 128)      64128       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 7, 128)       0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 7, 128)       0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 7, 128)       0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 21, 128)      0           max_pooling1d_5[0][0]            \n",
      "                                                                 max_pooling1d_6[0][0]            \n",
      "                                                                 max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 17, 128)      82048       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 3, 128)       0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 384)          0           max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          49280       flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 2)            258         dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,854,470\n",
      "Trainable params: 285,570\n",
      "Non-trainable params: 8,568,900\n",
      "__________________________________________________________________________________________________\n",
      "Train on 1391 samples, validate on 347 samples\n",
      "Epoch 1/20\n",
      "1391/1391 [==============================] - 1s 608us/step - loss: 0.8005 - acc: 0.6219 - val_loss: 0.5301 - val_acc: 0.6744\n",
      "Roc-AUC Validation: 0.8348\n",
      "Epoch 2/20\n",
      "1391/1391 [==============================] - 0s 301us/step - loss: 0.5330 - acc: 0.7290 - val_loss: 0.3835 - val_acc: 0.8386\n",
      "Roc-AUC Validation: 0.9082\n",
      "Epoch 3/20\n",
      "1391/1391 [==============================] - 0s 318us/step - loss: 0.4351 - acc: 0.7850 - val_loss: 0.3783 - val_acc: 0.8357\n",
      "Roc-AUC Validation: 0.928\n",
      "Epoch 4/20\n",
      "1391/1391 [==============================] - 0s 304us/step - loss: 0.3060 - acc: 0.8720 - val_loss: 0.3277 - val_acc: 0.8617\n",
      "Roc-AUC Validation: 0.947\n",
      "Epoch 5/20\n",
      "1391/1391 [==============================] - 0s 298us/step - loss: 0.2406 - acc: 0.9080 - val_loss: 0.2536 - val_acc: 0.8905\n",
      "Roc-AUC Validation: 0.958\n",
      "Epoch 6/20\n",
      "1391/1391 [==============================] - 0s 310us/step - loss: 0.1869 - acc: 0.9331 - val_loss: 0.2454 - val_acc: 0.9107\n",
      "Roc-AUC Validation: 0.9591\n",
      "Epoch 7/20\n",
      "1391/1391 [==============================] - 0s 304us/step - loss: 0.1541 - acc: 0.9461 - val_loss: 0.2567 - val_acc: 0.9135\n",
      "Roc-AUC Validation: 0.9578\n",
      "Epoch 8/20\n",
      "1391/1391 [==============================] - 0s 322us/step - loss: 0.1067 - acc: 0.9655 - val_loss: 0.2877 - val_acc: 0.8934\n",
      "Roc-AUC Validation: 0.9596\n",
      "Epoch 9/20\n",
      "1391/1391 [==============================] - 0s 318us/step - loss: 0.0654 - acc: 0.9784 - val_loss: 0.7694 - val_acc: 0.7896\n",
      "Roc-AUC Validation: 0.9374\n",
      "Epoch 10/20\n",
      "1391/1391 [==============================] - 0s 331us/step - loss: 0.0713 - acc: 0.9720 - val_loss: 0.3407 - val_acc: 0.9107\n",
      "Roc-AUC Validation: 0.9597\n",
      "Epoch 11/20\n",
      "1391/1391 [==============================] - 0s 331us/step - loss: 0.0649 - acc: 0.9792 - val_loss: 0.2908 - val_acc: 0.9164\n",
      "Roc-AUC Validation: 0.9631\n",
      "Epoch 12/20\n",
      "1391/1391 [==============================] - 0s 317us/step - loss: 0.0860 - acc: 0.9698 - val_loss: 0.2870 - val_acc: 0.9107\n",
      "Roc-AUC Validation: 0.9643\n",
      "Epoch 13/20\n",
      "1391/1391 [==============================] - 0s 305us/step - loss: 0.0316 - acc: 0.9892 - val_loss: 0.3565 - val_acc: 0.8847\n",
      "Roc-AUC Validation: 0.956\n",
      "Epoch 14/20\n",
      "1391/1391 [==============================] - 0s 301us/step - loss: 0.0597 - acc: 0.9784 - val_loss: 0.3147 - val_acc: 0.8934\n",
      "Roc-AUC Validation: 0.955\n",
      "Epoch 15/20\n",
      "1391/1391 [==============================] - 0s 307us/step - loss: 0.0302 - acc: 0.9863 - val_loss: 0.4117 - val_acc: 0.8963\n",
      "Roc-AUC Validation: 0.958\n",
      "Epoch 16/20\n",
      "1391/1391 [==============================] - 0s 317us/step - loss: 0.0340 - acc: 0.9863 - val_loss: 0.5003 - val_acc: 0.8646\n",
      "Roc-AUC Validation: 0.9597\n",
      "Epoch 17/20\n",
      "1391/1391 [==============================] - 0s 303us/step - loss: 0.0386 - acc: 0.9835 - val_loss: 0.3290 - val_acc: 0.9222\n",
      "Roc-AUC Validation: 0.9669\n",
      "Epoch 18/20\n",
      "1391/1391 [==============================] - 0s 314us/step - loss: 0.0529 - acc: 0.9784 - val_loss: 0.3144 - val_acc: 0.8934\n",
      "Roc-AUC Validation: 0.9513\n",
      "Epoch 19/20\n",
      "1391/1391 [==============================] - 0s 306us/step - loss: 0.0233 - acc: 0.9878 - val_loss: 0.2942 - val_acc: 0.9193\n",
      "Roc-AUC Validation: 0.9678\n",
      "Epoch 20/20\n",
      "1391/1391 [==============================] - 0s 327us/step - loss: 0.0231 - acc: 0.9871 - val_loss: 0.3514 - val_acc: 0.9107\n",
      "Roc-AUC Validation: 0.967\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not imported_notebook:\n",
    "    texts = df['text'].values\n",
    "    labels = df['label'].values\n",
    "\n",
    "    indices = np.arange(len(texts))\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(indices)\n",
    "    texts = texts[indices]\n",
    "    labels = labels[indices]\n",
    "    nb_validation_samples = int(VALIDATION_SPLIT * len(texts))\n",
    "\n",
    "    x_train = texts[:-nb_validation_samples]\n",
    "    y_train = labels[:-nb_validation_samples]\n",
    "    x_val = texts[-nb_validation_samples:]\n",
    "    y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "    df_train = pd.DataFrame(columns=['text', 'label', 'convention'])\n",
    "    df_train['text'] = x_train\n",
    "    df_train['label'] = y_train\n",
    "    df_train['convention'] = 'Industrial'\n",
    "\n",
    "\n",
    "    df_val = pd.DataFrame(columns=['text', 'label', 'convention'])\n",
    "    df_val['text'] = x_val\n",
    "    df_val['label'] = y_val\n",
    "    df_val['convention'] = 'Industrial'\n",
    "\n",
    "    print(\"\\n\\n===========================\")\n",
    "    print(\"WITH VALIDATION!\")\n",
    "    print(\"===========================\\n\\n\")\n",
    "    models2 = train_DL_models(df_train,\n",
    "                    data_class_column=\"convention\", \n",
    "                    data_label_column=\"label\",\n",
    "                    df_val=df_val,\n",
    "                    tokenizer=extended_tokenizer,\n",
    "                    random_seed=0, use_validation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real label for samples classified with value 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real label for samples classified with value 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not imported_notebook:\n",
    "    models2_tokenizer = models2[1]['Industrial']\n",
    "    val_seq = models2_tokenizer.texts_to_sequences(x_val)\n",
    "    val_seq = pad_sequences(val_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    preds2 = models2[0]['Industrial'].predict(val_seq)\n",
    "    print(\"Real label for samples classified with value 0\")\n",
    "    display(y_val[preds2[:,0]>preds2[:,1]])\n",
    "    print(\"Real label for samples classified with value 1\")\n",
    "    display(y_val[preds2[:,0]<preds2[:,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===========================\n",
      "WITHOUT VALIDATION!\n",
      "===========================\n",
      "\n",
      "\n",
      "Found 85688 unique tokens.\n",
      "----------------------------------------------------------------\n",
      "            Industrial                  \n",
      "----------------------------------------------------------------\n",
      "Shape of data tensor: (1391, 40)\n",
      "Shape of label tensor: (1391, 2)\n",
      "\n",
      "Number of elements from each class in traing and validation set \n",
      "[910. 481.]\n",
      "116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:30: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=128, kernel_size=3)`\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:30: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=128, kernel_size=4)`\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:30: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=128, kernel_size=5)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - more complex convolutional neural network\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 40, 100)      8568900     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 38, 128)      38528       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 37, 128)      51328       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 36, 128)      64128       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 7, 128)       0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 7, 128)       0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 7, 128)       0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 21, 128)      0           max_pooling1d_9[0][0]            \n",
      "                                                                 max_pooling1d_10[0][0]           \n",
      "                                                                 max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 17, 128)      82048       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 3, 128)       0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 384)          0           max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          49280       flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 2)            258         dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,854,470\n",
      "Trainable params: 285,570\n",
      "Non-trainable params: 8,568,900\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "1391/1391 [==============================] - 1s 567us/step - loss: 0.7884 - acc: 0.6290\n",
      "Epoch 2/20\n",
      "1391/1391 [==============================] - 0s 290us/step - loss: 0.5362 - acc: 0.7419\n",
      "Epoch 3/20\n",
      "1391/1391 [==============================] - 0s 299us/step - loss: 0.3821 - acc: 0.8239\n",
      "Epoch 4/20\n",
      "1391/1391 [==============================] - 0s 290us/step - loss: 0.3113 - acc: 0.8699\n",
      "Epoch 5/20\n",
      "1391/1391 [==============================] - 0s 294us/step - loss: 0.2510 - acc: 0.9015\n",
      "Epoch 6/20\n",
      "1391/1391 [==============================] - 0s 270us/step - loss: 0.1566 - acc: 0.9540\n",
      "Epoch 7/20\n",
      "1391/1391 [==============================] - 0s 269us/step - loss: 0.1372 - acc: 0.9454\n",
      "Epoch 8/20\n",
      "1391/1391 [==============================] - 0s 279us/step - loss: 0.0797 - acc: 0.9741\n",
      "Epoch 9/20\n",
      "1391/1391 [==============================] - 0s 282us/step - loss: 0.1023 - acc: 0.9626\n",
      "Epoch 10/20\n",
      "1391/1391 [==============================] - 0s 285us/step - loss: 0.0932 - acc: 0.9626\n",
      "Epoch 11/20\n",
      "1391/1391 [==============================] - 0s 295us/step - loss: 0.0378 - acc: 0.9899\n",
      "Epoch 12/20\n",
      "1391/1391 [==============================] - 0s 288us/step - loss: 0.0948 - acc: 0.9655\n",
      "Epoch 13/20\n",
      "1391/1391 [==============================] - 0s 292us/step - loss: 0.0395 - acc: 0.9849\n",
      "Epoch 14/20\n",
      "1391/1391 [==============================] - 0s 290us/step - loss: 0.0378 - acc: 0.9863\n",
      "Epoch 15/20\n",
      "1391/1391 [==============================] - 0s 287us/step - loss: 0.0478 - acc: 0.9842\n",
      "Epoch 16/20\n",
      "1391/1391 [==============================] - 0s 287us/step - loss: 0.0399 - acc: 0.9849\n",
      "Epoch 17/20\n",
      "1391/1391 [==============================] - 0s 282us/step - loss: 0.0380 - acc: 0.9849\n",
      "Epoch 18/20\n",
      "1391/1391 [==============================] - 0s 272us/step - loss: 0.0313 - acc: 0.9835\n",
      "Epoch 19/20\n",
      "1391/1391 [==============================] - 0s 293us/step - loss: 0.0315 - acc: 0.9842\n",
      "Epoch 20/20\n",
      "1391/1391 [==============================] - 0s 279us/step - loss: 0.0208 - acc: 0.9885\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not imported_notebook:\n",
    "    print(\"\\n\\n===========================\")    \n",
    "    print(\"WITHOUT VALIDATION!\")\n",
    "    print(\"===========================\\n\\n\")\n",
    "    models3 = train_DL_models(df_train,\n",
    "                    data_class_column=\"convention\", \n",
    "                    data_label_column=\"label\",\n",
    "                    df_val=None,\n",
    "                    tokenizer = models2_tokenizer,\n",
    "                    random_seed=0, use_validation=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real label for samples classified with value 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real label for samples classified with value 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not imported_notebook:\n",
    "    val_seq = models2_tokenizer.texts_to_sequences(x_val)\n",
    "    val_seq = pad_sequences(val_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    preds2 = models2[0]['Industrial'].predict(val_seq)\n",
    "    print(\"Real label for samples classified with value 0\")\n",
    "    display(y_val[preds2[:,0]>preds2[:,1]])\n",
    "    print(\"Real label for samples classified with value 1\")\n",
    "    display(y_val[preds2[:,0]<preds2[:,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real label for samples classified with value 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real label for samples classified with value 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not imported_notebook:\n",
    "    val_seq = models2_tokenizer.texts_to_sequences(x_val)\n",
    "    val_seq = pad_sequences(val_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    preds2 = models2[0]['Industrial'].predict(val_seq)\n",
    "    print(\"Real label for samples classified with value 0\")\n",
    "    display(y_val[preds2[:,0]>preds2[:,1]])\n",
    "    print(\"Real label for samples classified with value 1\")\n",
    "    display(y_val[preds2[:,0]<preds2[:,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > gathered_green_test.tsv\n",
      "Percentaje of words in common between training and gathered source: 0.21209844072891226\n",
      " > gathered_s2_17-19_ki.tsv\n",
      "Percentaje of words in common between training and gathered source: 0.6973511177907196\n",
      " > gathered_news_sentences.tsv\n",
      "Percentaje of words in common between training and gathered source: 0.5782453503663347\n",
      " > _gathered_github_sentences.tsv\n",
      "Percentaje of words in common between training and gathered source: 0.6545181288746947\n",
      " > gathered_github_sentences.tsv\n",
      "Percentaje of words in common between training and gathered source: 0.7832049596092429\n",
      " > gathered_s2_17-19_ki_kw.tsv\n",
      "Percentaje of words in common between training and gathered source: 0.6513244411046403\n"
     ]
    }
   ],
   "source": [
    "if not imported_notebook:\n",
    "    tok1 = create_tokenizer(df)\n",
    "    word_index = tok1.word_index\n",
    "    tok1_words = []\n",
    "    for word, i in word_index.items():\n",
    "        tok1_words.append(word)\n",
    "        \n",
    "    for idx, _df in enumerate(gathered_dfs[:-1]):\n",
    "        tok2 = create_tokenizer(_df)\n",
    "        word_index2 = tok2.word_index\n",
    "        \n",
    "        gathered_f = gathered_data_files[idx]\n",
    "        \n",
    "        tok2_words = []\n",
    "        for word, i in word_index2.items():\n",
    "            tok2_words.append(word)\n",
    "            \n",
    "        words_in_tok1 = [w for w in tok2_words if w in tok1_words]\n",
    "        print(\" > {}\".format(gathered_f))\n",
    "        print(\"Percentaje of words in common between training and gathered source: {}\".format(len(words_in_tok1) / len(tok1_words)))\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >> Top sentences with higher confidence for Github:\n",
      "\n",
      "malert - malert is a simple, easy and custom ios uialertview written in swift ralertview - alertview, ios popup window, a pop-up framework, can be simple and convenient to join your project.\n",
      "\n",
      "python-ios-template a template to create a python project that will run on ios devices.\n",
      "\n",
      "creditskudos to peka for his awesome work on the logo .traefik s logo is licensed under the creative commons 3.0 attributions license.traefik s logo was inspired by the gopher stickers made by takuya ueda https twitter.com tenntenn .the original go gopher was designed by renee french http reneefrench.blogspot.com .\n",
      "\n",
      "contextmenu - an ios context menu ui inspired by things 3. panels - panels is a framework to easily add sliding panels to your application.\n",
      "\n",
      "licensethe project name iris was inspired by the greek mythology.iris web framework is free and open-source software licensed under the 3-clause bsd license.\n",
      "\n",
      "misc actions callfunc, orbitcamera, follow, tween basic menus and buttons integrated with physics engines box2d 5 and chipmunk 6 particle system skeleton animations spine 7 and armature support fonts fast font rendering using fixed and variable width fonts support for .ttf fonts tile map support orthogonal, isometric and hexagonal parallax scrolling motion streak render to texture touch accelerometer on mobile devices touch mouse keyboard on desktop sound engine support cocosdenshion library based on openal integrated slow motion fast forward fast and compressed textures pvr compressed and uncompressed textures, etc1 compressed textures, and more resolution independent language c , with lua and javascript bindings open source commercial friendly mit compatible with open and closed source projects opengl es 2.0 mobile opengl 2.1 desktop basedbuild requirements mac os x 10.7 , xcode 8 or ubuntu 14.04 , cmake 3.1 or windows 7 , vs 2015 python 2.7.5 not python 3 ndk r16 is required to build android games android studio 3.0.0 to build android games tested with 3.0.0 jre or jdk 1.6 is required for web publishingruntime requirements ios 8.0 for iphone ipad games android 3.0.0 for android os x v10.9 for mac games windows 7 for win games modern browsers and ie 9 for web gamesrunning testsselect the test you want from xcode scheme chooser.\n",
      "\n",
      "bigboard - an elegant financial markets library written in swift that makes requests to yahoo finance api s under the hood.\n",
      "\n",
      "build and run a new project for ios cocos run -p ios build and run a new project for osx cocos run -p mac build and run a new project for linux if you never run cocos2d-x on linux, you need to install all dependencies by thescript in cocos2d build install-deps-linux.sh cd cocos2d-x build .\n",
      "\n",
      "replaceanimation - pull-to-refresh animation in uicollectionview with a sticky header flow layout, written in swift pulltomakesoup - custom animated pull-to-refresh that can be easily added to uiscrollview rainyrefreshcontrol - simple refresh control for ios inspired by concept.\n",
      "\n",
      "for example json full name audrey roy , email audreyr gmail.com , project name complexity , repo name complexity , project short description refreshingly simple static site generator.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not imported_notebook:\n",
    "    NUM_SENTENCES_TO_SHOW=10\n",
    "    \n",
    "    df_test = pd.read_csv(os.path.join(GATHERED_DATA_FOLDER, \"gathered_github_sentences.tsv\"), sep=\"\\t\")\n",
    "    \n",
    "    test_sentences = df_test['sentence'].values\n",
    "    \n",
    "    test_seq = extended_tokenizer.texts_to_sequences(test_sentences)\n",
    "    test_seq = pad_sequences(val_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    preds = conv_models[0]['Industrial'].predict(test_seq)[:,1]\n",
    "    \n",
    "    print(\" >> Top sentences with higher confidence for Github:\\n\")\n",
    "    for s in test_sentences[preds.argsort()[-NUM_SENTENCES_TO_SHOW:][::-1]]:\n",
    "        print(\"{}\\n\".format(s))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'react can also render on the server using node and power mobile apps using react native.learn how to use react in your own project.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
