{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Required Python utilities\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "from langdetect import detect\n",
    "from bs4 import BeautifulSoup\n",
    "from markdown import markdown\n",
    "from lxml import etree\n",
    "import os\n",
    "import random\n",
    "import tqdm\n",
    "import itertools \n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "## Deep Learning imports for the classifiers\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, Concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "##Â ML required imports (for clustering)\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA, LatentDirichletAllocation\n",
    "from sklearn.preprocessing import scale, StandardScaler\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "\n",
    "\n",
    "# Topic modeling imports\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "##Â NLP related imports\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "# visualization imports\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import base64\n",
    "import io\n",
    "%matplotlib inline\n",
    "sns.set() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imported_notebook = False ## Set this flag to true if importing this code from another notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_DIR = \"Data/Iterative-models-building/Training data/\"\n",
    "GATHERED_DATA_FOLDER = \"Data/Iterative-models-building/Gathered_data/Conventions/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classificaiton NETWORKs Configuration parameters\n",
    "MAX_SEQUENCE_LENGTH = 40\n",
    "MAX_NB_WORDS = 10000\n",
    "EMBEDDING_DIM = 100 ## 100, 200 or 300\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(df, max_words=MAX_NB_WORDS):\n",
    "    texts = df['text'].values\n",
    "    \n",
    "    _tokenizer = Tokenizer(num_words=max_words, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "    _tokenizer.fit_on_texts(texts)\n",
    "    \n",
    "    return _tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_embeddings():\n",
    "    ## Reading GLOVE (precalculated word embeddings)\n",
    "    embeddings_index = {}\n",
    "    f = open(os.path.join(GLOVE_DIR, 'glove.6B.{}d.txt'.format(EMBEDDING_DIM)))\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    \n",
    "    return embeddings_index\n",
    "\n",
    "embeddings_index = read_glove_embeddings()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate AUC\n",
    "## As explained here: https://stackoverflow.com/questions/41032551/how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras\n",
    "class roc_callback(Callback):\n",
    "    def __init__(self,validation_data):#training_data,validation_data):\n",
    "        #self.x = training_data[0]\n",
    "        #self.y = training_data[1]\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        #y_pred = self.model.predict(self.x)\n",
    "        #roc = roc_auc_score(self.y, y_pred)\n",
    "        y_pred_val = self.model.predict(self.x_val)\n",
    "        roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
    "        #print('\\rroc-auc: %s - roc-auc_val: %s' % (str(round(roc,4)),str(round(roc_val,4))),end=100*' '+'\\n')\n",
    "        print(\"Roc-AUC Validation: {}\".format(str(round(roc_val,4))))\n",
    "        return\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_matches_proba(sequences, model):\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    preds = model.predict(data)\n",
    "\n",
    "    return preds[:,1]\n",
    "\n",
    "\n",
    "def calculate_matches(sentences, _models, _tokenizers):\n",
    "    _repos_matches = {}\n",
    "\n",
    "    ## Getting classification confidence per model for each repo\n",
    "    for model_key in _models.keys():\n",
    "            \n",
    "        tokenized_sentences = _tokenizers[model_key].texts_to_sequences(sentences)\n",
    "        preds1 = get_model_matches_proba(tokenized_sentences, _models[model_key])\n",
    "        \n",
    "        \"\"\"\n",
    "        preds2 = get_model_matches_proba(tokenized_sentences, _models[model_key])\n",
    "        \n",
    "        data = pad_sequences(tokenized_sentences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "        preds2 = _models[model_key].predict(data)\n",
    "\n",
    "        _repos_matches[model_key] = [preds1, preds2]\n",
    "        \"\"\"\n",
    "        _repos_matches[model_key] = preds1\n",
    "    return _repos_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_DL_model(x_train, y_train, x_val, y_val, tokenizer, num_epochs=NUM_EPOCHS):\n",
    "    \n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    print('\\nNumber of elements from each class in traing and validation set ')\n",
    "    print(y_train.sum(axis=0))\n",
    "    print(y_val.sum(axis=0))\n",
    "\n",
    "\n",
    "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)\n",
    "    \n",
    "\n",
    "    convs = []\n",
    "    filter_sizes = [3,4,5]\n",
    "\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    for fsz in filter_sizes:\n",
    "        l_conv = Conv1D(nb_filter=128,filter_length=fsz,activation='relu')(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(5)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Concatenate(axis=1)(convs)#\n",
    "\n",
    "    l_cov1= Conv1D(128, 5, activation='relu')(l_merge)\n",
    "    l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "\n",
    "    l_flat = Flatten()(l_pool1)\n",
    "    l_dense = Dense(128, activation='relu')(l_flat)\n",
    "    preds = Dense(2, activation='softmax')(l_dense)\n",
    "    \n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # train a 1D convnet with global maxpooling\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(128, 5, activation='relu')(x)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(128, 5, activation='relu')(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"model fitting - more complex convolutional neural network\")\n",
    "    model.summary()\n",
    "    train_history = model.fit(x_train, y_train, validation_data=(x_val, y_val),epochs=num_epochs, batch_size=50, callbacks=[roc_callback(validation_data=(x_val, y_val))])\n",
    "\n",
    "    return model,tokenizer,x_val,y_val, train_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_DL_model_not_validation(x_train, y_train, tokenizer, num_epochs=NUM_EPOCHS):\n",
    "    \n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    print('\\nNumber of elements from each class in traing and validation set ')\n",
    "    print(y_train.sum(axis=0))\n",
    "    print(y_val.sum(axis=0))\n",
    "\n",
    "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)\n",
    "\n",
    "\n",
    "    convs = []\n",
    "    filter_sizes = [3,4,5]\n",
    "\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    for fsz in filter_sizes:\n",
    "        l_conv = Conv1D(nb_filter=128,filter_length=fsz,activation='relu')(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(5)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Concatenate(axis=1)(convs)#\n",
    "\n",
    "    l_cov1= Conv1D(128, 5, activation='relu')(l_merge)\n",
    "    l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "\n",
    "    l_flat = Flatten()(l_pool1)\n",
    "    l_dense = Dense(128, activation='relu')(l_flat)\n",
    "    preds = Dense(2, activation='softmax')(l_dense)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    print(\"model fitting - more complex convolutional neural network\")\n",
    "    model.summary()\n",
    "    train_history = model.fit(x_train, y_train,epochs=num_epochs, batch_size=50)\n",
    "    return model,tokenizer,train_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_DL_models(df_train,\n",
    "                    data_class_column=\"convention\", \n",
    "                    data_label_column=\"label\",\n",
    "                   df_val=None,\n",
    "                    tokenizer=None,\n",
    "                   random_seed=None,\n",
    "                   use_validation=True):\n",
    "    \n",
    "    \n",
    "    _models = {}\n",
    "    _tokenizers = {}\n",
    "    _data_val_x = {}\n",
    "    _data_val_y = {}\n",
    "    _train_histories = {}\n",
    "    if tokenizer is None:        \n",
    "        if df_val is None:\n",
    "            tokenizer = create_tokenizer(df_train)\n",
    "        else:\n",
    "            tokenizer = create_tokenizer(pd.concat([df_train, df_val]))\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    \n",
    "    for convention in df[data_class_column].unique():\n",
    "        \n",
    "        print(\"----------------------------------------------------------------\")\n",
    "        print(\"            {}                  \".format(convention))\n",
    "        print(\"----------------------------------------------------------------\")\n",
    "        \n",
    "        tmp_df_train = df_train[df_train[data_class_column] == convention]\n",
    "        if use_validation:\n",
    "            if df_val is None:\n",
    "\n",
    "                texts = tmp_df_train['text'].values\n",
    "                labels = tmp_df_train[data_label_column].values\n",
    "\n",
    "                sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "\n",
    "                data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "                labels = to_categorical(np.asarray(labels))\n",
    "                print('Shape of data tensor:', data.shape)\n",
    "                print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "                indices = np.arange(data.shape[0])\n",
    "                if random_seed is not None:\n",
    "                    np.random.seed(random_seed)\n",
    "                np.random.shuffle(indices)\n",
    "                data = data[indices]\n",
    "                labels = labels[indices]\n",
    "                nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "                x_train = data[:-nb_validation_samples]\n",
    "                y_train = labels[:-nb_validation_samples]\n",
    "                x_val = data[-nb_validation_samples:]\n",
    "                y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "\n",
    "                _model, _tokenizer, _x_val, _y_val, _train_h = train_DL_model(x_train, y_train, x_val, y_val, tokenizer)\n",
    "            else:\n",
    "\n",
    "                tmp_df_val = df_val[df_val[data_class_column] == convention]\n",
    "\n",
    "                train_texts = tmp_df_train['text'].values\n",
    "                train_labels = tmp_df_train[data_label_column].values\n",
    "\n",
    "                val_texts = tmp_df_val['text'].values\n",
    "                val_labels = tmp_df_val[data_label_column].values\n",
    "\n",
    "                train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "                val_sequences = tokenizer.texts_to_sequences(val_texts)\n",
    "\n",
    "\n",
    "\n",
    "                x_train = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "                x_val = pad_sequences(val_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "                y_train = to_categorical(np.asarray(train_labels))\n",
    "                y_val = to_categorical(np.asarray(val_labels))\n",
    "\n",
    "                _model, _tokenizer, _x_val, _y_val, _train_h = train_DL_model(x_train, y_train, x_val, y_val, tokenizer)\n",
    "        \n",
    "            _models[convention] = _model\n",
    "            _tokenizers[convention] = _tokenizer\n",
    "            _data_val_x[convention] = _x_val\n",
    "            _data_val_y[convention] = _y_val\n",
    "            _train_histories[convention] = _train_h\n",
    "        else:\n",
    "            texts = tmp_df_train['text'].values\n",
    "            labels = tmp_df_train[data_label_column].values\n",
    "\n",
    "            sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "\n",
    "            data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "            labels = to_categorical(np.asarray(labels))\n",
    "            print('Shape of data tensor:', data.shape)\n",
    "            print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "            \n",
    "            x_train = data\n",
    "            y_train = labels\n",
    "        \n",
    "            _model, _tokenizer, _train_h = train_DL_model_not_validation(x_train, y_train, tokenizer)\n",
    "\n",
    "            _models[convention] = _model\n",
    "            _tokenizers[convention] = _tokenizer\n",
    "            _train_histories[convention] = _train_h\n",
    "        \n",
    "\n",
    "        print(\"\\n\\n\\n\")\n",
    "        \n",
    "    return (_models, _tokenizers, _data_val_x, _data_val_y, _train_histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not imported_notebook:\n",
    "    df = pd.read_csv(\"Data/Iterative-models-building/Training data/Conventions/training_aggregated_conventions.tsv\", sep=\"\\t\")\n",
    "    df = df[df['convention'] =='Industrial']\n",
    "    \n",
    "    def remove_line_breaks(text_str):\n",
    "        text_str = text_str.replace(\"\\\\n\", \" \")\n",
    "        return text_str\n",
    "    \n",
    "    df['text'] = df['text'].apply(remove_line_breaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>provenance</th>\n",
       "      <th>convention</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3224</td>\n",
       "      <td>1</td>\n",
       "      <td>also, you can install drivers for various vm p...</td>\n",
       "      <td>Manually_gathered</td>\n",
       "      <td>Industrial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3225</td>\n",
       "      <td>1</td>\n",
       "      <td>will return a bar plot comparing the models on...</td>\n",
       "      <td>Manually_gathered</td>\n",
       "      <td>Industrial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3226</td>\n",
       "      <td>1</td>\n",
       "      <td>* expected result: 2.63% test error rate with ...</td>\n",
       "      <td>Manually_gathered</td>\n",
       "      <td>Industrial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3227</td>\n",
       "      <td>1</td>\n",
       "      <td>if you know your terminal size, you can contro...</td>\n",
       "      <td>Manually_gathered</td>\n",
       "      <td>Industrial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3228</td>\n",
       "      <td>1</td>\n",
       "      <td>- to provide a range of non-functional feature...</td>\n",
       "      <td>Manually_gathered</td>\n",
       "      <td>Industrial</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text  \\\n",
       "3224      1  also, you can install drivers for various vm p...   \n",
       "3225      1  will return a bar plot comparing the models on...   \n",
       "3226      1  * expected result: 2.63% test error rate with ...   \n",
       "3227      1  if you know your terminal size, you can contro...   \n",
       "3228      1  - to provide a range of non-functional feature...   \n",
       "\n",
       "             provenance  convention  \n",
       "3224  Manually_gathered  Industrial  \n",
       "3225  Manually_gathered  Industrial  \n",
       "3226  Manually_gathered  Industrial  \n",
       "3227  Manually_gathered  Industrial  \n",
       "3228  Manually_gathered  Industrial  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gathered_green_test.tsv',\n",
       " 'gathered_s2_17-19_ki.tsv',\n",
       " 'gathered_news_sentences.tsv',\n",
       " '_gathered_github_sentences.tsv',\n",
       " 'gathered_github_sentences.tsv',\n",
       " 'gathered_s2_17-19_ki_kw.tsv']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gathered_data_files = [f for f in os.listdir(GATHERED_DATA_FOLDER) \n",
    "                              if (os.path.isfile(os.path.join(GATHERED_DATA_FOLDER, f)) \n",
    "                                  and not f.startswith( '.' ) \n",
    "                                  and \"gathered_\" in f)]\n",
    "gathered_data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gathered_dfs = []\n",
    "\n",
    "for f_name in gathered_data_files:\n",
    "    tmp_df = pd.read_csv(os.path.join(GATHERED_DATA_FOLDER, f_name), sep=\"\\t\")\n",
    "    tmp_df = tmp_df.rename(columns={\"sentence\": \"text\"})\n",
    "    \n",
    "    gathered_dfs.append(tmp_df)\n",
    "\n",
    "gathered_dfs.append(df)##Adding training data sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "extended_tokenizer = create_tokenizer(pd.concat(gathered_dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 85688 unique tokens.\n",
      "----------------------------------------------------------------\n",
      "            Industrial                  \n",
      "----------------------------------------------------------------\n",
      "Shape of data tensor: (1738, 40)\n",
      "Shape of label tensor: (1738, 2)\n",
      "\n",
      "Number of elements from each class in traing and validation set \n",
      "[910. 481.]\n",
      "[231. 116.]\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "model fitting - more complex convolutional neural network\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 40, 100)      8568900     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 38, 128)      38528       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 37, 128)      51328       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 36, 128)      64128       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 7, 128)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 7, 128)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 7, 128)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 21, 128)      0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 17, 128)      82048       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 3, 128)       0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 384)          0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          49280       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            258         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,854,470\n",
      "Trainable params: 285,570\n",
      "Non-trainable params: 8,568,900\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=128, kernel_size=3)`\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=128, kernel_size=4)`\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=128, kernel_size=5)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1391 samples, validate on 347 samples\n",
      "Epoch 1/20\n",
      "1391/1391 [==============================] - 1s 538us/step - loss: 0.8005 - acc: 0.6219 - val_loss: 0.5194 - val_acc: 0.7032\n",
      "Roc-AUC Validation: 0.8314\n",
      "Epoch 2/20\n",
      "1391/1391 [==============================] - 0s 290us/step - loss: 0.5314 - acc: 0.7362 - val_loss: 0.3845 - val_acc: 0.8329\n",
      "Roc-AUC Validation: 0.9092\n",
      "Epoch 3/20\n",
      "1391/1391 [==============================] - 0s 290us/step - loss: 0.4274 - acc: 0.7951 - val_loss: 0.3702 - val_acc: 0.8329\n",
      "Roc-AUC Validation: 0.9266\n",
      "Epoch 4/20\n",
      "1391/1391 [==============================] - 0s 295us/step - loss: 0.3055 - acc: 0.8670 - val_loss: 0.3215 - val_acc: 0.8588\n",
      "Roc-AUC Validation: 0.9462\n",
      "Epoch 5/20\n",
      "1391/1391 [==============================] - 0s 304us/step - loss: 0.2345 - acc: 0.9015 - val_loss: 0.4198 - val_acc: 0.8184\n",
      "Roc-AUC Validation: 0.9335\n",
      "Epoch 6/20\n",
      "1391/1391 [==============================] - 0s 303us/step - loss: 0.1763 - acc: 0.9339 - val_loss: 0.2487 - val_acc: 0.9020\n",
      "Roc-AUC Validation: 0.9605\n",
      "Epoch 7/20\n",
      "1391/1391 [==============================] - 0s 294us/step - loss: 0.1296 - acc: 0.9504 - val_loss: 0.2533 - val_acc: 0.8963\n",
      "Roc-AUC Validation: 0.9596\n",
      "Epoch 8/20\n",
      "1391/1391 [==============================] - 0s 313us/step - loss: 0.1086 - acc: 0.9655 - val_loss: 0.2917 - val_acc: 0.8934\n",
      "Roc-AUC Validation: 0.9639\n",
      "Epoch 9/20\n",
      "1391/1391 [==============================] - 0s 306us/step - loss: 0.0851 - acc: 0.9684 - val_loss: 0.2483 - val_acc: 0.9107\n",
      "Roc-AUC Validation: 0.9643\n",
      "Epoch 10/20\n",
      "1391/1391 [==============================] - 0s 297us/step - loss: 0.0455 - acc: 0.9849 - val_loss: 0.3510 - val_acc: 0.9107\n",
      "Roc-AUC Validation: 0.9585\n",
      "Epoch 11/20\n",
      "1391/1391 [==============================] - 0s 315us/step - loss: 0.0765 - acc: 0.9741 - val_loss: 0.2896 - val_acc: 0.9049\n",
      "Roc-AUC Validation: 0.9658\n",
      "Epoch 12/20\n",
      "1391/1391 [==============================] - 0s 287us/step - loss: 0.0493 - acc: 0.9813 - val_loss: 0.3995 - val_acc: 0.8674\n",
      "Roc-AUC Validation: 0.9683\n",
      "Epoch 13/20\n",
      "1391/1391 [==============================] - 0s 284us/step - loss: 0.0974 - acc: 0.9712 - val_loss: 0.2995 - val_acc: 0.9049\n",
      "Roc-AUC Validation: 0.9614\n",
      "Epoch 14/20\n",
      "1391/1391 [==============================] - 0s 305us/step - loss: 0.0291 - acc: 0.9907 - val_loss: 0.3673 - val_acc: 0.8847\n",
      "Roc-AUC Validation: 0.9396\n",
      "Epoch 15/20\n",
      "1391/1391 [==============================] - 0s 319us/step - loss: 0.1031 - acc: 0.9669 - val_loss: 0.2519 - val_acc: 0.9020\n",
      "Roc-AUC Validation: 0.9629\n",
      "Epoch 16/20\n",
      "1391/1391 [==============================] - 0s 311us/step - loss: 0.0232 - acc: 0.9899 - val_loss: 0.2981 - val_acc: 0.9164\n",
      "Roc-AUC Validation: 0.9669\n",
      "Epoch 17/20\n",
      "1391/1391 [==============================] - 0s 329us/step - loss: 0.0572 - acc: 0.9784 - val_loss: 0.3236 - val_acc: 0.8991\n",
      "Roc-AUC Validation: 0.9667\n",
      "Epoch 18/20\n",
      "1391/1391 [==============================] - 0s 330us/step - loss: 0.0195 - acc: 0.9892 - val_loss: 0.3970 - val_acc: 0.9078\n",
      "Roc-AUC Validation: 0.9612\n",
      "Epoch 19/20\n",
      "1391/1391 [==============================] - 0s 321us/step - loss: 0.0633 - acc: 0.9698 - val_loss: 0.3086 - val_acc: 0.9078\n",
      "Roc-AUC Validation: 0.9663\n",
      "Epoch 20/20\n",
      "1391/1391 [==============================] - 0s 320us/step - loss: 0.0198 - acc: 0.9892 - val_loss: 0.3517 - val_acc: 0.8905\n",
      "Roc-AUC Validation: 0.9671\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not imported_notebook:\n",
    "    models = train_DL_models(df,\n",
    "                data_class_column=\"convention\", \n",
    "                data_label_column=\"label\",\n",
    "                df_val=None,\n",
    "                tokenizer=extended_tokenizer,\n",
    "                random_seed=0, \n",
    "                use_validation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===========================\n",
      "WITH VALIDATION!\n",
      "===========================\n",
      "\n",
      "\n",
      "Found 85688 unique tokens.\n",
      "----------------------------------------------------------------\n",
      "            Industrial                  \n",
      "----------------------------------------------------------------\n",
      "\n",
      "Number of elements from each class in traing and validation set \n",
      "[910. 481.]\n",
      "[231. 116.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=128, kernel_size=3)`\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=128, kernel_size=4)`\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=128, kernel_size=5)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - more complex convolutional neural network\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 40, 100)      8568900     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 38, 128)      38528       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 37, 128)      51328       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 36, 128)      64128       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 7, 128)       0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 7, 128)       0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 7, 128)       0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 21, 128)      0           max_pooling1d_5[0][0]            \n",
      "                                                                 max_pooling1d_6[0][0]            \n",
      "                                                                 max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 17, 128)      82048       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 3, 128)       0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 384)          0           max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          49280       flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 2)            258         dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,854,470\n",
      "Trainable params: 285,570\n",
      "Non-trainable params: 8,568,900\n",
      "__________________________________________________________________________________________________\n",
      "Train on 1391 samples, validate on 347 samples\n",
      "Epoch 1/20\n",
      "1391/1391 [==============================] - 1s 608us/step - loss: 0.8005 - acc: 0.6219 - val_loss: 0.5301 - val_acc: 0.6744\n",
      "Roc-AUC Validation: 0.8348\n",
      "Epoch 2/20\n",
      "1391/1391 [==============================] - 0s 301us/step - loss: 0.5330 - acc: 0.7290 - val_loss: 0.3835 - val_acc: 0.8386\n",
      "Roc-AUC Validation: 0.9082\n",
      "Epoch 3/20\n",
      "1391/1391 [==============================] - 0s 318us/step - loss: 0.4351 - acc: 0.7850 - val_loss: 0.3783 - val_acc: 0.8357\n",
      "Roc-AUC Validation: 0.928\n",
      "Epoch 4/20\n",
      "1391/1391 [==============================] - 0s 304us/step - loss: 0.3060 - acc: 0.8720 - val_loss: 0.3277 - val_acc: 0.8617\n",
      "Roc-AUC Validation: 0.947\n",
      "Epoch 5/20\n",
      "1391/1391 [==============================] - 0s 298us/step - loss: 0.2406 - acc: 0.9080 - val_loss: 0.2536 - val_acc: 0.8905\n",
      "Roc-AUC Validation: 0.958\n",
      "Epoch 6/20\n",
      "1391/1391 [==============================] - 0s 310us/step - loss: 0.1869 - acc: 0.9331 - val_loss: 0.2454 - val_acc: 0.9107\n",
      "Roc-AUC Validation: 0.9591\n",
      "Epoch 7/20\n",
      "1391/1391 [==============================] - 0s 304us/step - loss: 0.1541 - acc: 0.9461 - val_loss: 0.2567 - val_acc: 0.9135\n",
      "Roc-AUC Validation: 0.9578\n",
      "Epoch 8/20\n",
      "1391/1391 [==============================] - 0s 322us/step - loss: 0.1067 - acc: 0.9655 - val_loss: 0.2877 - val_acc: 0.8934\n",
      "Roc-AUC Validation: 0.9596\n",
      "Epoch 9/20\n",
      "1391/1391 [==============================] - 0s 318us/step - loss: 0.0654 - acc: 0.9784 - val_loss: 0.7694 - val_acc: 0.7896\n",
      "Roc-AUC Validation: 0.9374\n",
      "Epoch 10/20\n",
      "1391/1391 [==============================] - 0s 331us/step - loss: 0.0713 - acc: 0.9720 - val_loss: 0.3407 - val_acc: 0.9107\n",
      "Roc-AUC Validation: 0.9597\n",
      "Epoch 11/20\n",
      "1391/1391 [==============================] - 0s 331us/step - loss: 0.0649 - acc: 0.9792 - val_loss: 0.2908 - val_acc: 0.9164\n",
      "Roc-AUC Validation: 0.9631\n",
      "Epoch 12/20\n",
      "1391/1391 [==============================] - 0s 317us/step - loss: 0.0860 - acc: 0.9698 - val_loss: 0.2870 - val_acc: 0.9107\n",
      "Roc-AUC Validation: 0.9643\n",
      "Epoch 13/20\n",
      "1391/1391 [==============================] - 0s 305us/step - loss: 0.0316 - acc: 0.9892 - val_loss: 0.3565 - val_acc: 0.8847\n",
      "Roc-AUC Validation: 0.956\n",
      "Epoch 14/20\n",
      "1391/1391 [==============================] - 0s 301us/step - loss: 0.0597 - acc: 0.9784 - val_loss: 0.3147 - val_acc: 0.8934\n",
      "Roc-AUC Validation: 0.955\n",
      "Epoch 15/20\n",
      "1391/1391 [==============================] - 0s 307us/step - loss: 0.0302 - acc: 0.9863 - val_loss: 0.4117 - val_acc: 0.8963\n",
      "Roc-AUC Validation: 0.958\n",
      "Epoch 16/20\n",
      "1391/1391 [==============================] - 0s 317us/step - loss: 0.0340 - acc: 0.9863 - val_loss: 0.5003 - val_acc: 0.8646\n",
      "Roc-AUC Validation: 0.9597\n",
      "Epoch 17/20\n",
      "1391/1391 [==============================] - 0s 303us/step - loss: 0.0386 - acc: 0.9835 - val_loss: 0.3290 - val_acc: 0.9222\n",
      "Roc-AUC Validation: 0.9669\n",
      "Epoch 18/20\n",
      "1391/1391 [==============================] - 0s 314us/step - loss: 0.0529 - acc: 0.9784 - val_loss: 0.3144 - val_acc: 0.8934\n",
      "Roc-AUC Validation: 0.9513\n",
      "Epoch 19/20\n",
      "1391/1391 [==============================] - 0s 306us/step - loss: 0.0233 - acc: 0.9878 - val_loss: 0.2942 - val_acc: 0.9193\n",
      "Roc-AUC Validation: 0.9678\n",
      "Epoch 20/20\n",
      "1391/1391 [==============================] - 0s 327us/step - loss: 0.0231 - acc: 0.9871 - val_loss: 0.3514 - val_acc: 0.9107\n",
      "Roc-AUC Validation: 0.967\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not imported_notebook:\n",
    "    texts = df['text'].values\n",
    "    labels = df['label'].values\n",
    "\n",
    "    indices = np.arange(len(texts))\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(indices)\n",
    "    texts = texts[indices]\n",
    "    labels = labels[indices]\n",
    "    nb_validation_samples = int(VALIDATION_SPLIT * len(texts))\n",
    "\n",
    "    x_train = texts[:-nb_validation_samples]\n",
    "    y_train = labels[:-nb_validation_samples]\n",
    "    x_val = texts[-nb_validation_samples:]\n",
    "    y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "    df_train = pd.DataFrame(columns=['text', 'label', 'convention'])\n",
    "    df_train['text'] = x_train\n",
    "    df_train['label'] = y_train\n",
    "    df_train['convention'] = 'Industrial'\n",
    "\n",
    "\n",
    "    df_val = pd.DataFrame(columns=['text', 'label', 'convention'])\n",
    "    df_val['text'] = x_val\n",
    "    df_val['label'] = y_val\n",
    "    df_val['convention'] = 'Industrial'\n",
    "\n",
    "    print(\"\\n\\n===========================\")\n",
    "    print(\"WITH VALIDATION!\")\n",
    "    print(\"===========================\\n\\n\")\n",
    "    models2 = train_DL_models(df_train,\n",
    "                    data_class_column=\"convention\", \n",
    "                    data_label_column=\"label\",\n",
    "                    df_val=df_val,\n",
    "                    tokenizer=extended_tokenizer,\n",
    "                    random_seed=0, use_validation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real label for samples classified with value 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real label for samples classified with value 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not imported_notebook:\n",
    "    models2_tokenizer = models2[1]['Industrial']\n",
    "    val_seq = models2_tokenizer.texts_to_sequences(x_val)\n",
    "    val_seq = pad_sequences(val_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    preds2 = models2[0]['Industrial'].predict(val_seq)\n",
    "    print(\"Real label for samples classified with value 0\")\n",
    "    display(y_val[preds2[:,0]>preds2[:,1]])\n",
    "    print(\"Real label for samples classified with value 1\")\n",
    "    display(y_val[preds2[:,0]<preds2[:,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===========================\n",
      "WITHOUT VALIDATION!\n",
      "===========================\n",
      "\n",
      "\n",
      "Found 85688 unique tokens.\n",
      "----------------------------------------------------------------\n",
      "            Industrial                  \n",
      "----------------------------------------------------------------\n",
      "Shape of data tensor: (1391, 40)\n",
      "Shape of label tensor: (1391, 2)\n",
      "\n",
      "Number of elements from each class in traing and validation set \n",
      "[910. 481.]\n",
      "116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:30: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=128, kernel_size=3)`\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:30: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=128, kernel_size=4)`\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:30: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=128, kernel_size=5)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - more complex convolutional neural network\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 40, 100)      8568900     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 38, 128)      38528       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 37, 128)      51328       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 36, 128)      64128       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 7, 128)       0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 7, 128)       0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 7, 128)       0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 21, 128)      0           max_pooling1d_9[0][0]            \n",
      "                                                                 max_pooling1d_10[0][0]           \n",
      "                                                                 max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 17, 128)      82048       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 3, 128)       0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 384)          0           max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          49280       flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 2)            258         dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,854,470\n",
      "Trainable params: 285,570\n",
      "Non-trainable params: 8,568,900\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "1391/1391 [==============================] - 1s 567us/step - loss: 0.7884 - acc: 0.6290\n",
      "Epoch 2/20\n",
      "1391/1391 [==============================] - 0s 290us/step - loss: 0.5362 - acc: 0.7419\n",
      "Epoch 3/20\n",
      "1391/1391 [==============================] - 0s 299us/step - loss: 0.3821 - acc: 0.8239\n",
      "Epoch 4/20\n",
      "1391/1391 [==============================] - 0s 290us/step - loss: 0.3113 - acc: 0.8699\n",
      "Epoch 5/20\n",
      "1391/1391 [==============================] - 0s 294us/step - loss: 0.2510 - acc: 0.9015\n",
      "Epoch 6/20\n",
      "1391/1391 [==============================] - 0s 270us/step - loss: 0.1566 - acc: 0.9540\n",
      "Epoch 7/20\n",
      "1391/1391 [==============================] - 0s 269us/step - loss: 0.1372 - acc: 0.9454\n",
      "Epoch 8/20\n",
      "1391/1391 [==============================] - 0s 279us/step - loss: 0.0797 - acc: 0.9741\n",
      "Epoch 9/20\n",
      "1391/1391 [==============================] - 0s 282us/step - loss: 0.1023 - acc: 0.9626\n",
      "Epoch 10/20\n",
      "1391/1391 [==============================] - 0s 285us/step - loss: 0.0932 - acc: 0.9626\n",
      "Epoch 11/20\n",
      "1391/1391 [==============================] - 0s 295us/step - loss: 0.0378 - acc: 0.9899\n",
      "Epoch 12/20\n",
      "1391/1391 [==============================] - 0s 288us/step - loss: 0.0948 - acc: 0.9655\n",
      "Epoch 13/20\n",
      "1391/1391 [==============================] - 0s 292us/step - loss: 0.0395 - acc: 0.9849\n",
      "Epoch 14/20\n",
      "1391/1391 [==============================] - 0s 290us/step - loss: 0.0378 - acc: 0.9863\n",
      "Epoch 15/20\n",
      "1391/1391 [==============================] - 0s 287us/step - loss: 0.0478 - acc: 0.9842\n",
      "Epoch 16/20\n",
      "1391/1391 [==============================] - 0s 287us/step - loss: 0.0399 - acc: 0.9849\n",
      "Epoch 17/20\n",
      "1391/1391 [==============================] - 0s 282us/step - loss: 0.0380 - acc: 0.9849\n",
      "Epoch 18/20\n",
      "1391/1391 [==============================] - 0s 272us/step - loss: 0.0313 - acc: 0.9835\n",
      "Epoch 19/20\n",
      "1391/1391 [==============================] - 0s 293us/step - loss: 0.0315 - acc: 0.9842\n",
      "Epoch 20/20\n",
      "1391/1391 [==============================] - 0s 279us/step - loss: 0.0208 - acc: 0.9885\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not imported_notebook:\n",
    "    print(\"\\n\\n===========================\")    \n",
    "    print(\"WITHOUT VALIDATION!\")\n",
    "    print(\"===========================\\n\\n\")\n",
    "    models3 = train_DL_models(df_train,\n",
    "                    data_class_column=\"convention\", \n",
    "                    data_label_column=\"label\",\n",
    "                    df_val=None,\n",
    "                    tokenizer = models2_tokenizer,\n",
    "                    random_seed=0, use_validation=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real label for samples classified with value 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real label for samples classified with value 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not imported_notebook:\n",
    "    val_seq = models2_tokenizer.texts_to_sequences(x_val)\n",
    "    val_seq = pad_sequences(val_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    preds2 = models2[0]['Industrial'].predict(val_seq)\n",
    "    print(\"Real label for samples classified with value 0\")\n",
    "    display(y_val[preds2[:,0]>preds2[:,1]])\n",
    "    print(\"Real label for samples classified with value 1\")\n",
    "    display(y_val[preds2[:,0]<preds2[:,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real label for samples classified with value 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real label for samples classified with value 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not imported_notebook:\n",
    "    val_seq = models2_tokenizer.texts_to_sequences(x_val)\n",
    "    val_seq = pad_sequences(val_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    preds2 = models2[0]['Industrial'].predict(val_seq)\n",
    "    print(\"Real label for samples classified with value 0\")\n",
    "    display(y_val[preds2[:,0]>preds2[:,1]])\n",
    "    print(\"Real label for samples classified with value 1\")\n",
    "    display(y_val[preds2[:,0]<preds2[:,1]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
