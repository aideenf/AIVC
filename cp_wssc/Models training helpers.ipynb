{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models training helper functions\n",
    "\n",
    "This notebook contains the functions required to train new models\n",
    "\n",
    "As it is right now, it reads the file:\n",
    "```\n",
    "\"Data/Iterative-models-building/Training data/Conventions/training_aggregated_conventions.tsv\"\n",
    "```\n",
    "and stores it content in a dataframe variable named 'training_df'.\n",
    "\n",
    "Files in GATHERED_DATA_FOLDER are combined with the content of 'training_df' to create the vocabulary of the Tokenizer, as by design, ```\"tokenizer.tokenize()\"``` will remove any words not seen on the ```\"tokenizer.fit()\"``` stage. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Required Python utilities\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "from langdetect import detect\n",
    "from bs4 import BeautifulSoup\n",
    "from markdown import markdown\n",
    "from lxml import etree\n",
    "import os\n",
    "import random\n",
    "import tqdm\n",
    "import itertools \n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "## Deep Learning imports for the classifiers\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, Concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "##Â ML required imports (for clustering)\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA, LatentDirichletAllocation\n",
    "from sklearn.preprocessing import scale, StandardScaler\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "\n",
    "# Topic modeling imports\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "##Â NLP related imports\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "# visualization imports\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import base64\n",
    "import io\n",
    "%matplotlib inline\n",
    "sns.set() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imported_notebook = True ## Set this flag to true if importing this code from another notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_DIR = \"Data/Iterative-models-building/Training data/\"\n",
    "GATHERED_DATA_FOLDER = \"Data/Iterative-models-building/Gathered_data/Conventions/\"\n",
    "MODELS_DIR = \"./Data/Iterative-models-building/Models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classificaiton NETWORKs Configuration parameters\n",
    "MAX_SEQUENCE_LENGTH = 40\n",
    "MAX_NB_WORDS = 10000\n",
    "EMBEDDING_DIM = 100 ## 100, 200 or 300\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "TFIDF_MAX_FEATURES=10000\n",
    "\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vocabulary loading\n",
    "def read_glove_embeddings():\n",
    "    ## Reading GLOVE (precalculated word embeddings)\n",
    "    embeddings_index = {}\n",
    "    f = open(os.path.join(GLOVE_DIR, 'glove.6B.{}d.txt'.format(EMBEDDING_DIM)))\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    \n",
    "    return embeddings_index\n",
    "\n",
    "embeddings_index = read_glove_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stop words list \n",
    "def get_stop_words(stop_file_path):\n",
    "    \"\"\"load stop words \"\"\"\n",
    "    \n",
    "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        return frozenset(stop_set)\n",
    "#load a set of stop words\n",
    "stopwords=get_stop_words(\"Data/Iterative-models-building/Training data/resources/stopwords.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Text cleaning functions\n",
    "def clean_line(X):\n",
    "    \n",
    "    stemmer = WordNetLemmatizer()\n",
    "\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X))\n",
    "\n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "\n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document2 = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document2)\n",
    "\n",
    "    return document\n",
    "\n",
    "def clean_str(text):\n",
    "    documents = []\n",
    "    ret = documents\n",
    "    if type(text == \"list\"):\n",
    "        #print(\"list: \", text)\n",
    "        for X in text:\n",
    "            documents.append(clean_line(X))\n",
    "    \n",
    "        ret =\"\"\n",
    "        if(len(documents)>1):\n",
    "\n",
    "            for d in documents:\n",
    "                ret+=d+\"\\n\"\n",
    "        else:\n",
    "            ret = documents[0]\n",
    "        \n",
    "    elif type(text)==\"str\":\n",
    "        p#rint(\"str\")\n",
    "        ret = clear_line(text)\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper function to create a tokenizer given a data frame with a column \"text\"\n",
    "def create_tokenizer(df, max_words=MAX_NB_WORDS):\n",
    "    texts = df['text'].values\n",
    "    \n",
    "    _tokenizer = Tokenizer(num_words=max_words, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "    _tokenizer.fit_on_texts(texts)\n",
    "    \n",
    "    return _tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate AUC for validation when training the model\n",
    "## As explained here: https://stackoverflow.com/questions/41032551/how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras\n",
    "class roc_callback(Callback):\n",
    "    def __init__(self,validation_data):#training_data,validation_data):\n",
    "        #self.x = training_data[0]\n",
    "        #self.y = training_data[1]\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "       \n",
    "        y_pred_val = self.model.predict(self.x_val)\n",
    "        roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
    "\n",
    "        print(\"Roc-AUC on validation: {}\".format(str(round(roc_val,4))))\n",
    "        return\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training a deep learning model given training, validation data\n",
    "def train_DL_model(x_train, y_train, x_val, y_val, tokenizer, num_epochs=NUM_EPOCHS):\n",
    "    \n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    print('\\nNumber of elements from each class in traing and validation set ')\n",
    "    print(y_train.sum(axis=0))\n",
    "    print(y_val.sum(axis=0))\n",
    "\n",
    "\n",
    "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)\n",
    "    \n",
    "\n",
    "    convs = []\n",
    "    filter_sizes = [3,4,5]\n",
    "\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    for fsz in filter_sizes:\n",
    "        l_conv = Conv1D(nb_filter=128,filter_length=fsz,activation='relu')(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(5)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Concatenate(axis=1)(convs)#\n",
    "\n",
    "    l_cov1= Conv1D(128, 5, activation='relu')(l_merge)\n",
    "    l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "\n",
    "    l_flat = Flatten()(l_pool1)\n",
    "    l_dense = Dense(128, activation='relu')(l_flat)\n",
    "    preds = Dense(2, activation='softmax')(l_dense)\n",
    "    \n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    \n",
    "    print(\"model fitting - more complex convolutional neural network\")\n",
    "    model.summary()\n",
    "    train_history = model.fit(x_train, y_train, validation_data=(x_val, y_val),epochs=num_epochs, batch_size=50, callbacks=[roc_callback(validation_data=(x_val, y_val))])\n",
    "\n",
    "    return model,tokenizer,x_val,y_val, train_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trains a DL model without using validation dataset\n",
    "def train_DL_model_not_validation(x_train, y_train, tokenizer, num_epochs=NUM_EPOCHS):\n",
    "    \n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    print('\\nNumber of elements from each class in traing and validation set ')\n",
    "    print(y_train.sum(axis=0))\n",
    "    print(y_val.sum(axis=0))\n",
    "\n",
    "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)\n",
    "\n",
    "\n",
    "    convs = []\n",
    "    filter_sizes = [3,4,5]\n",
    "\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    for fsz in filter_sizes:\n",
    "        l_conv = Conv1D(nb_filter=128,filter_length=fsz,activation='relu')(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(5)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Concatenate(axis=1)(convs)#\n",
    "\n",
    "    l_cov1= Conv1D(128, 5, activation='relu')(l_merge)\n",
    "    l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "\n",
    "    l_flat = Flatten()(l_pool1)\n",
    "    l_dense = Dense(128, activation='relu')(l_flat)\n",
    "    preds = Dense(2, activation='softmax')(l_dense)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    print(\"model fitting - more complex convolutional neural network\")\n",
    "    model.summary()\n",
    "    train_history = model.fit(x_train, y_train,epochs=num_epochs, batch_size=50)\n",
    "    return model,tokenizer,train_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trains one DL model for each group of sentences (within each convention)\n",
    "def train_DL_models(df_train,\n",
    "                    data_class_column=\"convention\", \n",
    "                    data_label_column=\"label\",\n",
    "                   df_val=None,\n",
    "                    tokenizer=None,\n",
    "                   random_seed=None,\n",
    "                   use_validation=True, num_epochs = NUM_EPOCHS):\n",
    "    \n",
    "    \n",
    "    _models = {}\n",
    "    _tokenizers = {}\n",
    "    _data_val_x = {}\n",
    "    _data_val_y = {}\n",
    "    _train_histories = {}\n",
    "    if tokenizer is None:        \n",
    "        if df_val is None:\n",
    "            tokenizer = create_tokenizer(df_train)\n",
    "        else:\n",
    "            tokenizer = create_tokenizer(pd.concat([df_train, df_val]))\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    \n",
    "    for convention in df[data_class_column].unique():\n",
    "        \n",
    "        print(\"----------------------------------------------------------------\")\n",
    "        print(\"            {}                  \".format(convention))\n",
    "        print(\"----------------------------------------------------------------\")\n",
    "        \n",
    "        tmp_df_train = df_train[df_train[data_class_column] == convention]\n",
    "        if use_validation:\n",
    "            if df_val is None:\n",
    "\n",
    "                texts = tmp_df_train['text'].values\n",
    "                labels = tmp_df_train[data_label_column].values\n",
    "\n",
    "                sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "\n",
    "                data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "                labels = to_categorical(np.asarray(labels))\n",
    "                print('Shape of data tensor:', data.shape)\n",
    "                print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "                indices = np.arange(data.shape[0])\n",
    "                if random_seed is not None:\n",
    "                    np.random.seed(random_seed)\n",
    "                np.random.shuffle(indices)\n",
    "                data = data[indices]\n",
    "                labels = labels[indices]\n",
    "                nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "                x_train = data[:-nb_validation_samples]\n",
    "                y_train = labels[:-nb_validation_samples]\n",
    "                x_val = data[-nb_validation_samples:]\n",
    "                y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "\n",
    "                _model, _tokenizer, _x_val, _y_val, _train_h = train_DL_model(x_train, y_train, x_val, y_val, tokenizer, num_epochs=num_epochs)\n",
    "            else:\n",
    "\n",
    "                tmp_df_val = df_val[df_val[data_class_column] == convention]\n",
    "\n",
    "                train_texts = tmp_df_train['text'].values\n",
    "                train_labels = tmp_df_train[data_label_column].values\n",
    "\n",
    "                val_texts = tmp_df_val['text'].values\n",
    "                val_labels = tmp_df_val[data_label_column].values\n",
    "\n",
    "                train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "                val_sequences = tokenizer.texts_to_sequences(val_texts)\n",
    "\n",
    "\n",
    "\n",
    "                x_train = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "                x_val = pad_sequences(val_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "                y_train = to_categorical(np.asarray(train_labels))\n",
    "                y_val = to_categorical(np.asarray(val_labels))\n",
    "\n",
    "                _model, _tokenizer, _x_val, _y_val, _train_h = train_DL_model(x_train, y_train, x_val, y_val, tokenizer, num_epochs=num_epochs)\n",
    "        \n",
    "            _models[convention] = _model\n",
    "            _tokenizers[convention] = _tokenizer\n",
    "            _data_val_x[convention] = _x_val\n",
    "            _data_val_y[convention] = _y_val\n",
    "            _train_histories[convention] = _train_h\n",
    "        else:\n",
    "            texts = tmp_df_train['text'].values\n",
    "            labels = tmp_df_train[data_label_column].values\n",
    "\n",
    "            sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "\n",
    "            data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "            labels = to_categorical(np.asarray(labels))\n",
    "            print('Shape of data tensor:', data.shape)\n",
    "            print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "            \n",
    "            x_train = data\n",
    "            y_train = labels\n",
    "        \n",
    "            _model, _tokenizer, _train_h = train_DL_model_not_validation(x_train, y_train, tokenizer, num_epochs=num_epochs)\n",
    "\n",
    "            _models[convention] = _model\n",
    "            _tokenizers[convention] = _tokenizer\n",
    "            _train_histories[convention] = _train_h\n",
    "        \n",
    "\n",
    "        print(\"\\n\\n\\n\")\n",
    "        \n",
    "    return (_models, _tokenizers, _data_val_x, _data_val_y, _train_histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_DL_models_in_picke(pickle_f_name, _models, _tokenizers, _val_x, _val_y, _train_histories):\n",
    "    _convnet_items = {}\n",
    "    \n",
    "    _convnet_items['model'] = _conventions_models\n",
    "    _convnet_items['tokenizer'] = _conventions_tokenizers\n",
    "    _convnet_items['_x_val'] = _conventions_data_val_x\n",
    "    _convnet_items['_y_val'] = _conventions_data_val_y\n",
    "    _convnet_items['train_history'] = _conventions_train_histories\n",
    "    \n",
    "    with open(os.path.join(MODELS_DIR, ), 'wb') as f:\n",
    "        # Pickle the 'data' dictionary using the highest protocol available.\n",
    "        pickle.dump(_convnet_items, f, pickle.HIGHEST_PROTOCOL)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_DL_models_from_pickle(pickle_f_name):\n",
    "    f = open( os.path.join(MODELS_DIR, pickle_f_name), 'rb') \n",
    "    _convnet_items = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    ## Load the models from the downloaded pickle file             \n",
    "    _models = _convnet_items['model'] \n",
    "    _tokenizers = _convnet_items['tokenizer'] \n",
    "    _val_x = _convnet_items['_x_val'] \n",
    "    _val_y = _convnet_items['_y_val'] \n",
    "    _train_histories = _convnet_items['train_history'] \n",
    "    \n",
    "    return _models, _tokenizers, _val_x, _val_y, _train_histories\n",
    "\n",
    "#_models, _tokenizers, _val_x, _val_y, _train_histories = read_DL_models_from_pickle('conv_models_items.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trains a TF-IDF, Naive-Bayes based classifier\n",
    "def train_new_text_pipelineNB(texts, labels1):\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', CountVectorizer(max_df=0.85, stop_words=stopwords, max_features=TFIDF_MAX_FEATURES)),\n",
    "        ('tfidf', TfidfTransformer(smooth_idf=True,use_idf=True)),\n",
    "        ('clf', MultinomialNB()),\n",
    "    ])\n",
    "    \n",
    "    text_clf.fit(texts, labels1)  \n",
    "    \n",
    "    return text_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ML_models(df,\n",
    "                    data_class_column=\"convention\", \n",
    "                    data_label_column=\"label\"):\n",
    "    _models = {}  \n",
    "    \n",
    "    for c in df[data_class_column].unique():\n",
    "        sentences = df[df[data_class_column] == c]['text'].values\n",
    "        labels = df[df[data_class_column] == c][data_label_column].values\n",
    "        \n",
    "        m = train_new_text_pipelineNB(sentences, labels)\n",
    "        \n",
    "        _models[c] = m\n",
    "    return _models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helpers for calculating ML model probability score of class 1 for a set of sentences\n",
    "def calculate_matches_DL(sentences, _models, _tokenizers):\n",
    "    _models_matches = {}\n",
    "\n",
    "    ## Getting classification confidence per model for each repo\n",
    "    for model_key in _models.keys():\n",
    "            \n",
    "        tokenized_sentences = _tokenizers[model_key].texts_to_sequences(sentences)\n",
    "        \n",
    "        tokenized_sentences = pad_sequences(tokenized_sentences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "        preds = _models[model_key].predict(tokenized_sentences)\n",
    "    \n",
    "        _models_matches[model_key] = preds[:,1]\n",
    "        \n",
    "    return _models_matches\n",
    "\n",
    "def calculate_matches_ML(_sentences, _models): \n",
    "    \n",
    "    _models_matches = {}\n",
    "\n",
    "    ## Getting classification confidence per model for each repo\n",
    "    for model_key in _models.keys():\n",
    "        \n",
    "        preds = _models[model_key].predict_proba(_sentences)[:,1]\n",
    "        _models_matches[model_key] = preds\n",
    "\n",
    "    return _models_matches\n",
    "\n",
    "\n",
    "## Calculates probability of sentence combining DL models and ML models predictions\n",
    "def calculate_matches_mixture(_sentences, _modelsML, _modelsDL, _tokenizersDL):\n",
    "    _repos_matches = {}\n",
    "    preds_ML = calculate_matches_ML(_sentences, _modelsML)\n",
    "    preds_DL = calculate_matches_DL(_sentences, _modelsDL, _tokenizersDL)\n",
    "\n",
    "    ## Getting classification confidence per model for each repo\n",
    "    for model_key in _modelsML.keys():\n",
    "            \n",
    "        preds1 = preds_ML[model_key]\n",
    "        preds2 = preds_DL[model_key]\n",
    "\n",
    "        _repos_matches[model_key] = preds1+preds2\n",
    "    return _repos_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_line_breaks(text_str):\n",
    "        text_str = text_str.replace(\"\\\\n\", \" \")\n",
    "        return text_str\n",
    "    \n",
    "def read_training_data():\n",
    "    training_df = pd.read_csv(\"Data/Iterative-models-building/Training data/Conventions/training_aggregated_conventions.tsv\", sep=\"\\t\")\n",
    "\n",
    "    training_df['text'] = training_df['text'].apply(remove_line_breaks)\n",
    "    return training_df\n",
    "\n",
    "training_df = read_training_data()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:16: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "gathered_data_files = [f for f in os.listdir(GATHERED_DATA_FOLDER) \n",
    "                              if (os.path.isfile(os.path.join(GATHERED_DATA_FOLDER, f)) \n",
    "                                  and not f.startswith( '.' ) \n",
    "                                  and \"gathered_\" in f)]\n",
    "gathered_dfs = []\n",
    "\n",
    "for f_name in gathered_data_files:\n",
    "    tmp_df = pd.read_csv(os.path.join(GATHERED_DATA_FOLDER, f_name), sep=\"\\t\")\n",
    "    tmp_df = tmp_df.rename(columns={\"sentence\": \"text\"})\n",
    "    \n",
    "    gathered_dfs.append(tmp_df)\n",
    "\n",
    "\n",
    "gathered_dfs.append(training_df)##Adding training data sentences\n",
    "\n",
    "extended_tokenizer = create_tokenizer(pd.concat(gathered_dfs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
