<br/>****************************EXAMPLE 2************************************<br/>Number of words:77<br/>Number of sentences:6<br/>Total civic:0<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:2<br/>Total inspired:0<br/>Total market:0<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/10kdup'>https://www.reddit.com/r/artificial/comments/10kdup</a><br/><br/> <b>Computers match humans in classifying painting styles FTFY</b>[<font color=red>industrial</font>]<b>.</b> <b>Anybody have the paper? I wonder how much domain specific knowledge was involved, or if the software came up with the 4000 metrics automatically</b>[<font color=red>industrial</font>]<b>.</b> Oh look everybody An AI agent which can pick out the number of animals present in a TV cartoon, because it matches humans in understanding art. We will have full blown AGI in 7 days, woohoo uncomfortable pause nevermind. The program only sort of categorizes paintings by historical style.<br/><br/><br/>****************************EXAMPLE 3************************************<br/>Number of words:902<br/>Number of sentences:42<br/>Total civic:0<br/>Total domestic:1<br/>Total green:0<br/>Total industrial:18<br/>Total inspired:1<br/>Total market:2<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/10qlr2'>https://www.reddit.com/r/artificial/comments/10qlr2</a><br/><br/> Hi, well i now that lisp has been in use for A.I since ever, is it still the main language used to implement A.I ? What tools are used today?, if i were to enter on an A.I project what i am more likely to find? Thanks everyone, i didn't know this sub reddit would be so awesome what programming lang is used to implement A.I. It depends on which part of AI you want to work on. AI is a broad field covering numerous sub disciplines. <b>There is no standardized programming language as such, but most projects make use of C C++ Python Java</b>[<font color=red>industrial</font>]<b>.</b> I do think that I've observed some patterns, though: Robots use C or C++. Even a bad call to malloc can make your robot fall over; using a garbage collected language would be a disaster. <b>When the problem is to do something quickly, people tend to use C++, Java, or Matlab</b>[<font color=red>industrial</font>]<b>.</b> <b>These give you enough low level speed for your new algorithm to demonstrate itself, but don't make things so hard you can't get results in the first place</b>[<font color=red>industrial</font>]<b>.</b> <b>Lots of machine learning, computer vision, and motion planning work is in this category</b>[<font color=red>industrial</font>]<b>.</b> <b>When the problem is to get an answer at all and there is no existing work to compare your speed against, you start seeing higher level languages</b>[<font color=red>industrial</font>]<b>.</b> Java, Python, and Lisp are common here, mostly because they make it easy to implement your algorithm without fiddling with data types, error checking, extensibility, reliability, cross platform operation, or in fact anything other than being able to publish 'We can solve unsolved problem X'. Note that are also piles of domain specific languages that people came up with to make particular tasks easier for themselves. <b>For example, PDDL and STRIPS are languages used to define classical planning problems like Sudoku, logistics, or sokoban</b>[<font color=red>industrial</font>]<b>.</b> <b>These days, languages that do numerical calculations are most useful</b>[<font color=red>industrial</font>]<b>.</b> <b>Matlab, R, Numpy (matrix oriented) and also java and c c++ (and other compiled languages such as ocaml, haskell, golang)</b>[<font color=red>industrial</font>]<b>.</b> I think python is best overall (has matrices plus lots of libraries). Another thing to consider is that AI is mostly about big data nowadays and managing large files possibly on multiple computers is important, so learn about sql, mapreduce, iterating splitting large files. OP needs to be more specific about the field he wants to program in. <b>i didn't knew golang would be used for this, is that good?, i remember seen an article saying that Rob Pike stoped using C after golang</b>[<font color=red>industrial</font>]<b>.</b> <b>I learned lisp so I could read the literature but every robot I have ever worked on or even heard about uses c c++</b>[<font color=purple>domestic</font>]<b>.</b> mm i see, but then do you use some sort of tool to exersise your planner AI? what tools are used today?, i mean you just don't go and start writing code. what is the scenario you are going to find is you would join an A.I devel team? I once simulated a neural net in Microsoft Excel. <b>In the 1950s, the LISP creators worked at the MIT AI lab</b>[<font color=red>industrial</font>]<b>.</b> I think a good foundation in math (i.e statistics, linear algebra, etc) is more useful than knowing any one particular programming language. <b>Weka is an excellent tool, very easy to use API as well</b>[<font color=red>industrial</font>]<b>.</b> Implementing of AI algorithms such as search, satisfiability, optimisation, reasoning, the many kinds of planning, constraint satisfaction, KR, various language and perception tasks, etc.) I see Python, C++, and Java. I am sure there are many other procedural languages in use and some people probably use various functional languages. I am only deeply familiar with a few sub areas of AI, but most research areas likely have a number of competing languages (for problem representation and solving). <b>There are a large number of declarative languages out there</b>[<font color=red>industrial</font>]<b>.</b> <b>There are various logic programming languages that extend Prolog with a fully declarative semantics</b>[<font color=red>industrial</font>]<b>.</b> <b>At the moment I really enjoy using Answer Set Programming to encode and solve a wide range of reasoning tasks</b>[<font color=red>industrial</font>]<b>.</b> <b>It can be comparable to SAT in terms of performance, but comes with really powerful first order like input language</b>[<font color=red>industrial</font>]<b>.</b> <b>Graph colouring in two lines plus the graph, travelling salesman in six lines plus the locations, roads, and costs</b>[<font color=violet>market</font>]<b>.</b> A very good solver is ( When I have time I would also really like to try out Z3 the SAT Modulo Theory solver. <b>TLDR: Try Answer Set Programming (clasp and gringo) ( is very commonly used in planning competitions</b>[<font color=red>industrial</font>]<b>.</b> Depending on which sub field of AI you are interested in going into they could be using anything from python to C, and from prolog to java, matlab to lisp. Thanks everyone for helping out with yours comments, it was pretty fast Now i now about several tools languages and fundamentals topics i need to read about. Please if you can let us know something about how is your A.I team environment it would be fun to know it u johnwalkr recently ( in r robotics, perl is good to learn because it's very good to use to tie various things together, meaning you can (for example) download an arduino sketch for turning colored LEDs on and download someone else's weather forecast fetching software and then use perl to interface the two things without having to change much code. <b>It will probably help you get a higher grade in your C course because the basic principles are always the same</b>[<font color=red>industrial</font>]<b>.</b> <b>I'm using Java and LISP for an ANN I've been developing</b>[<font color=red>industrial</font>]<b>.</b> <b>So it pretty much depend on what you are familiar with and what kind of AI you want to do</b>[<font color=red>inspired, </font> <font color=violet>market</font>]<b>.</b><br/><br/><br/>****************************EXAMPLE 4************************************<br/>Number of words:315<br/>Number of sentences:17<br/>Total civic:0<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:1<br/>Total inspired:2<br/>Total market:0<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/13cfrx'>https://www.reddit.com/r/artificial/comments/13cfrx</a><br/><br/> <b>If artificial intelligence can exist in The Cloud , doesn't that suggest that consciousness can exist independently of the body? Cloud Consciousness</b>[<font color=red>inspired</font>]<b>.</b> Assuming that consciousness emerges from brain activity which is the consensus among a vast majority of researchers then it should be possible to create consciousness using other platforms than just the biological brain. This would imply that the brain is not special, it is just one possible platform inside of which consciousness can live. Other platforms include traditional computers and distributed computing platforms (which are nothing else but traditional computers that are 'spread out' across a larger distance). I'm going to assume that your question is about spiritual beliefs. Perhaps you didn't mean it like that, but that's the feeling I'm getting that you want to ask: 'Does Artificial Intelligence plus Cloud Computing imply the existence of an immortal soul?' No, it does not. Death of the body would effectively terminate the platform inside of which consciousness operates. There is (apparently) nothing outside of the body that could 'receive' the consciousness and host it after the body dies. On the other hand, it should be possible to transfer consciousness from the brain to another platform, but that other platform needs to exist. And here's something else to consider: Given how the brain and neural networks in general work, you could already think of consciousness being inside a 'cloud' of sorts. <b>Our brain can be compared to a distributed platform where the basic units ('computers') are neurons</b>[<font color=red>industrial</font>]<b>.</b> At this point I'm fairly sure I've misunderstood your question, but at least I tried. I WAS asking if this implies the existence of an immortal soul. For me, it is enough to admit that consciousness can exist outside of the brain. <b>The fact that it can certainly doesn't PROVE the existence of the soul, but it makes the idea a little more plausible, I feel</b>[<font color=red>inspired</font>]<b>.</b> For now, I accept that there is no known platform to receive a dying person's consciousness.<br/><br/><br/>****************************EXAMPLE 5************************************<br/>Number of words:5566<br/>Number of sentences:268<br/>Total civic:11<br/>Total domestic:9<br/>Total green:0<br/>Total industrial:20<br/>Total inspired:3<br/>Total market:32<br/>Total project:11<br/>Total renown:3<br/><a href='https://www.reddit.com/r/artificial/comments/13ep3e'>https://www.reddit.com/r/artificial/comments/13ep3e</a><br/><br/> I should have called it Debate Database + Algorithm AI HOW AN ALGORITHM AND A DISCUSSION GROUPS CAN BECOME ARTIFICIAL INTELLIGENCE: If we combine (1) an algorithm similar to Google's page ranking algorithm with, (2) a forum in which every aspect of an idea can be organized, analyzed, and voted on we could create the world's first artificial intelligence. Let me first explain how it would work, and then I would like to explain how it actually qualifies as an artificial intelligence. This algorithm's goal would be to rate ideas similar to how Google rates web pages. <b>If accomplished it could create the best website on the internet, because the website that used this algorithm would not just be sending users to other websites, the way Google does, it would actually become the destination, instead of just a way to get to the destination</b>[<font color=violet>market</font>]<b>.</b> REASONS TO AGREE AND DISAGREE: An algorithm can not read two ideas and figure out which idea is better. <b>So the first thing we need to do is organize information, so that an algorithm can analyze it</b>[<font color=red>industrial</font>]<b>.</b> I would start with an idea at the top of a page, with reasons to agree in one column beneath the idea, and reasons to agree in another column. The goal of the algorithm would be to promote the best ideas to the top of a page, or the front page of a website, and the best reasons to agree or disagree with an idea, to the top of their columns beneath the idea. This simple step would do a lot to allow an algorithm to analyze the idea, but we will have to do a lot more organization before we turn this algorithm loose. CLASSIFY ORGANIZE EACH POST BOOKS The second step would be to allow users to further classify their posts beyond simple reasons to agree or reasons to disagree. <b>People could suggest a book as a reason to disagree with an idea</b>[<font color=violet>market</font>]<b>.</b> <b>Now start thinking ahead what an algorithm could do, if someone says that a book agrees with their beliefs</b>[<font color=violet>market</font>]<b>.</b> <b>Data is readily available from Amazon or E-bay or the New York times best selling list of how well a book has sold</b>[<font color=violet>market</font>]<b>.</b> <b>One place where you submit the item that agrees or disagrees with the original idea</b>[<font color=violet>market</font>]<b>.</b> The second field would let you classify the object. Is it a book, a website, or simply a logical argument. The third field would be a place where the user explains why he thinks the book supports the conclusion that he she has come to. Of course, people would be allowed to vote weather or not the book actually does support the side that the original user said that it would. <b>This is where the algorithm could get very sophisticated</b>[<font color=red>industrial</font>]<b>.</b> <b>Would you want to give more credibility to those who said they had actually read the book? Would you want to give even more credibility to those who had bought the book, as more proof that they actually read the book</b>[<font color=purple>domestic, </font> <font color=violet>market, </font> <font color=orange>project</font>]<b>.</b> <b>What about people who wrote an essay on the book on the website</b>[<font color=orange>renown</font>]<b>.</b> <b>If Google was doing this, they could provide a place for users to write essays on books, similar to how Amazon lets users write essays</b>[<font color=violet>market</font>]<b>.</b> Perhaps they could not let people copy and paste essays into the form. It would only allow people to type their essays directly, to prevent stealing of essays. Perhaps people could vote on weather the book-essays were good or not similar to how Amazon lets users rate reviews, as to weather the review was helpful or not. So, as an example, you could submit a best selling book as a reason to agree with an idea, and then right a thoroughly convincing explanation of why this book agrees with the idea, and an essay that proves that you understand the main points of the book. If Google really wants to organize the worlds information, they must do this. <b>We have plenty of books, we have plenty of content on the internet</b>[<font color=orange>renown</font>]<b>.</b> We need ways of organizing this information into what it all means, and how all this information should affect us. <b>The only good way information can affect us, is for it to help us make better decisions</b>[<font color=violet>market</font>]<b>.</b> <b>In order for us to make better decisions, we must know all of the reasons to agree or disagree with a particular course of action</b>[<font color=blue>civic</font>]<b>.</b> In order to do this, we should not start at ground zero, with only our own thoughts in our head. We should bring together all of the great thinkers from the ages from every corner of the planet, and organize all of their great thoughts, so that we can make the right decisions. As you can see, this algorithm could be very simple, but it could also offer programmers hundreds of years of challenges to make it more sophisticated. I believe this is a strength of the idea, because it allows for continual improvement. WEB PAGES In addition to books, people should also be allowed to submit web pages as reasons to agree or disagree with an idea. Similar to how books work, the user would have to explain exactly why he or she believes the web page supports or contradicts the original idea. <b>And like the books section, the online community will be able to vote weather or not the web page actually supports the side that the person said it did</b>[<font color=orange>project</font>]<b>.</b> They already have billions of web pages classified, in descending order. <b>If you download the Google toolbar, you can see a score given to each page, based on a scale from 1 to 10</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> To help you understand what I envision, I can see an idea, with the following classifications. <b>Sites that agree: 0 10 star websites 1 9 star websites 2 8 star</b>[<font color=violet>market</font>]<b>.</b> <b>MARKERS, LEAVING BREAD CRUMBS, AND ORDER OUT OF CHAOS In order to better classify each idea, users should be able to add XML tags to each idea</b>[<font color=red>industrial</font>]<b>.</b> Because my system would keep people from changing the subject (the nature of posting reasons to agree or disagree keeps the conversation topic from changing) people could suggest, and vote on, where the topic would fit into the Dewey Decimal System, the Library of Congress, Yahoo, MSN, or Google Directories. COMING TOGETHER: If Google created an algorithm and a forum that would allow it to rate ideas and promote the best reasons to agree or disagree with a website, it could become a top content provider instead of just a content finder. I would much rather use a website that has both sides. They only discuss the logical arguments, books, and websites that support their side. The book, Bowling Alone tells us that our society is becoming more and more isolated. Technology, which was supposed to bring us together, is largely to blame. Kids that used to play together in the street, now sit along playing video games. The internet was supposed to bring us together, but their are red state websites and blue state websites, which allow people from each side to come together and pat each other on the back saying how brilliant they are. This may make each side feel better, and more justified in acting in extreme manners, but this does little to help people find the best reasons to agree or disagree with an idea, and it is a terrible way to make a decision. PROMOTING BETTER IDEAS BY PROMOTING BETTER BEHAVIOR Much of this idea comes from a dispute resolution. In order to resolve conflicts, apposing sides must come together to a common table and sit face to face. <b>One way of forcing apposing sides to interact on my website, would be to have people evaluate the top reasons to DISAGREE with an idea, before they are allowed to post a reason to AGREE with it, and visa versa</b>[<font color=orange>project</font>]<b>.</b> <b>A user could evaluate any of hundreds of possible characteristics of an idea</b>[<font color=red>industrial</font>]<b>.</b> On a scale from one to 10 did the explanation make logical sense. <b>Their are other behaviors besides actually interacting with each other, and thinking about the arguments that your opponent is making that will lead to successful dispute resolution</b>[<font color=violet>market</font>]<b>.</b> One of these behaviors is the practice of examining interests, as apposed to just positions. This will often allow win win solution where both sides are able to come up with solutions that accommodate all interest, instead of just focusing on one particular solution. This would be easy to facilitate in a discussion forum. You would just create a specific place to submit probable INTEREST OF THOSE WHO AGREE and probable INTEREST OF THOSE WHO DISAGREE. These can further be broken down to COMMON INTEREST and APPOSING INTEREST. Perhaps people who work as professional negotiators in the conflict resolution industry could be hired to suggest other ways of implementing best practices into an online forum. A TOP DESTINATION? I would visit a web page all the time, if it really had the best reasons to agree or disagree with an idea. This could assist people with every aspect of their lives. <b>Imagine a website with all of the reasons to agree or disagree with the following statements</b>[<font color=blue>civic</font>]<b>.</b> The Chicago Bears are going to go all the way this year. you had Google adds where you could buy a Ford, Chevy, Bears Tickets, Microsoft or Star Office, or contribute to the Hilary Clinton or Condi Rice campaign fund? MORE ABOUT THE ALGORITHM: The goal of the algorithm is to put the best ideas to the top. I don't know which one of these would carry more weight. <b>Here is a list of just a few of the characteristics of an idea, that an algorithm could exame</b>[<font color=red>industrial</font>]<b>.</b> Quantity of reasons that agree or disagree with the idea: The side with more reasons (to agree or disagree) would get more points than the other side. For example this idea has more reasons to agree than disagree. <b>Just like when you fill out a list of reasons to or not to do certain activities, you tend to choose the side with more reasons to agree</b>[<font color=orange>project</font>]<b>.</b> This is the very simple explanation, but it would become much more nuanced. People would be evaluating each of the reasons to agree or disagree. <b>Statisticians know samples it takes to obtain certain confidence intervals</b>[<font color=red>industrial</font>]<b>.</b> The more people rate an idea, with an average score, and the smaller the standard deviating, the more confident you are of that score. <b>I believe an early algorithm would be to multiple the confidence interval (CI) by the average score</b>[<font color=red>industrial</font>]<b>.</b> <b>Number of people who agree or disagree with the idea: The side with more people who agree should get more points</b>[<font color=blue>civic</font>]<b>.</b> Perhaps, in the beginning you would simply be voting for or against an idea. But in the future, you could be evaluating hundreds of characteristics of an idea on a scale from 1 to 10. In the future you may be able to chart an idea's performance on any of these characteristics over time. <b>Perhaps, over time the average score for an idea's logical presentation will fluctuate with a downward trend</b>[<font color=red>industrial</font>]<b>.</b> <b>There would be a field where you could enter link that agree or disagree with the idea</b>[<font color=violet>market</font>]<b>.</b> Results of peer evaluations: Their would be forms that people would fill out that asked pointed questions about each idea. You could respond to each question on a scale from 1 to 10. These results would affect the total score for each idea. <b>People could donate money to this website if they believe in it</b>[<font color=violet>market</font>]<b>.</b> <b>But a better way of doing it would be to let people donate money towards a specific idea</b>[<font color=violet>market</font>]<b>.</b> If you don't like the way this sounds you should read Atlas Shrugged by Ayn Rand. I'll just briefly say that money is the only way of measuring someone's blood, sweat, and tears. <b>Money is the only way that someone can pay someone else for their work</b>[<font color=violet>market</font>]<b>.</b> Also, it could be used on this website as tug of war analogue. Each idea would get more points if it was submitted from the e-mail address of a professor with a degree in the subject mater that is being discussed. For instance if someone said that Abraham Lincoln was an idiot. If the person that disagreed had a degree in history and the idea was submitted to the history section, then the person who disagreed (the professor) would win. <b>Prestigious would be ranked by the US News report, or some other un-biased judge</b>[<font color=blue>civic</font>]<b>.</b> <b>I don't care about you people that say, The smartest people don't always make the best decisions</b>[<font color=violet>market</font>]<b>.</b> Of course the smartest people don't always make the best decisions, but they would tend to make better decisions that stupid or uneducated people. People would be able to submit books that they think are important to read to make an educated decision about a certain topic. For instance The communist manifesto by Carl Marx and Atlas Shrugged by Ian Rand may be considered to be the most important books to read regarding weather or not we should raise taxes. <b>Those that had read those books, should have more say on this idea than those who have not, because this website desided that those books are very important to understand to make a decision about this issue</b>[<font color=orange>project</font>]<b>.</b> We are only just beginning to enter the rabit hole. <b>We could let people who have read these books submit essays on them (like book reports in school)</b>[<font color=violet>market</font>]<b>.</b> The people with a higher grade on their essays would get more say in those issues that people have said that those books are important. I tried understanding but there's a few flaws from the outset it cannot be an AI because you're advocating humans in the loop the idea you're suggesting seems like reddit, or google moderator you have not framed it in the context of anything inside of the AI field, what AI idea does it build off of edit: I get the gist of what you're saying and it seems that it is in fact what a search engine is. It's not a general AI though because it cannot generalize, it can only give you what it thinks is the most authoritative answers that have already been spoken. All AI requires humans to build it, right? At this point computers are not building themselves. <b>I am not doing a good job of explaining it, if it sounds similar to redit</b>[<font color=purple>domestic</font>]<b>.</b> I have been advocating this sense 2001: This link proves I suck at explaining this, that I don't know how to code, and that I have been explaining this idea sense 2001. Above I explained some of the differences between redit and what I propose. Here are some more features: Users tag their arguments as either supporting or opposing a particular thesis (because computers can not identify, from chronological argument threads, which beliefs are intended to support or oppose a belief, we should have users tag their arguments as reasons to agree or disagree with a conclusion). This will create a relationship database between beliefs. <b>Users are encouraged to evaluate aspects of an argument (logic, verifiable, and accuracy) on a scale of 1 10</b>[<font color=red>industrial</font>]<b>.</b> Users are able to 'invest' play money (and eventually real money) in the belief that a belief's score will go up in the future. One belief per page A list of reasons that agree with the thesis. <b>The algorithm will determine which arguments are the best, but it will factor a number of things</b>[<font color=violet>market</font>]<b>.</b> List for probable motives (or interest) of those who agree and another list of those who disagree Common interest shared between those who agree with the thesis and those who disagree Links to 'better ways of saying the same things Books that agree or disagree with the thesis Webpages that agree or disagree with the thesis The ability to evaluate theses and reasons to agree or disagree with them on the following criteria The idea has been placed in the correct category Brevity. Things should be as simple as possible, but not more so. Is the information portrait verifiable correct Arrogance. <b>Is the poster letting his data do the talking, or are they trying to win by threatening or name calling</b>[<font color=violet>market</font>]<b>.</b> Promotion of good ideas and arguments to places that will be given more visibility and more scrutiny The ranking of users on how well they follow the rules, so that those who follow good dispute resolution techniques are rewarded. The promotion of good dispute resolution technique Rewarding those who support ideas before they are popular, once consensus has been achieved Allow people to post photos, or videos that agree. I suck at explaining the whole thing in an organized way. <b>I don't want this post to get any longer, but I want to show the differences between Redit and the idea</b>[<font color=blue>civic, </font> <font color=violet>market, </font> <font color=orange>project</font>]<b>.</b> This is so similar to reddit I feel like this has to be a joke. I see your point, and it holds truth to some extent. But as someone earlier stated, this human AI would hardly be artificial. By your idea, this 'world's first AI' woudl already exist, under the name of Reddit (and similar sites I guess, although the upvote system is pretty much what you mean). More interesting might be using the concept of Reddit up and downvotes to make a karma based AI. I see a lot of differences between my idea and reddit. <b>At first they may seem like very small differences, but I think they make a difference, once I explain them</b>[<font color=purple>domestic</font>]<b>.</b> <b>On redit you can talk about Politics and Britney spears</b>[<font color=violet>market</font>]<b>.</b> I would put reasons to agree and disagree in separate columns. This may not sound like a big deal, but putting them into separate columns tags the argument as a reason to agree or disagree with a statement. <b>This is the definition of a relationship database, which Redit does not generate</b>[<font color=violet>market</font>]<b>.</b> Then if you were to click on one of the reasons to agree with a belief, it would get its own page, with reasons to agree or disagree with it. So lets take the belief that we should legalize THC as a belief. <b>Someone would post a reason to agree as the belief that 'THC users only hurt themselves'</b>[<font color=violet>market</font>]<b>.</b> If you use separate columns, and the person puts this statement in the 'reasons to agree' column, then it tags these two sentences as a logical relationship to each other. <b>Then there are reason to agree or disagree with the belief that 'THC users only hurt themselves'</b>[<font color=violet>market</font>]<b>.</b> Posting reasons to agree or disagree with this belief, will affect the overall belief. Using this forum creates the relationship database between ideas that AI needs. You could fill most of this database out by scraping Wikipedia. How does this help a computer find and interpret new patterns in data? I think it starts as crowd sourcing, but becomes AI. Do you believe the brain is 'magic'? In order for a computer to have AI it needs to make decisions. If we want to talk to a computer, we are going to have to explain ourselves in logical forms. <b>If we want to tell AI that human life is valuable, we have to outline those reason now, before it is too late</b>[<font color=orange>project</font>]<b>.</b> I wouldn't call this AI, but rather collective intelligence. This is becoming a fairly active academic field, with a ( at MIT which just organized a ( In the right circumstances, a group of people can be more intelligent than any individual in the group. In Vernor Vinge's original ( about the Singularity, he mentioned collective intelligence as a potential path to superhuman intelligence. Having users post their comment as either a reason to agree or disagree with a conclusion, can help create a relational database that begins to understand human logic. I believe the algorithm is more than just collective intelligence. The terms collective intelligence is not what I a advocate. <b>I advocate a computer algorithm that takes in more and more factors and uses AI software to 'decide' what actions it should do, and then does them</b>[<font color=red>industrial</font>]<b>.</b> What I am interested in, is what decision AI thinks we should do, as humans. <b>I think we have to get this out of the way, because we keep dancing around the issue</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> I don't think a magical spirit will enter a computer chip and it will start talking to us. A think massive computers will have to build relationship databases. <b>It will be given two different options, and it will build an algorithm to help it choose between the outcomes</b>[<font color=red>industrial</font>]<b>.</b> I believe when asked a question, it will build a database that supports each possible solution, then give an answer, based on the information that it has. It will always use logic, and a weighted formal process for making decisions that it makes. Its never going to say that it 'feels like going for a walk' only on a high level. It will have databases that will propose that it stays inside or goes outside, it will weigh the options, and the winner will determine what it feels like doing. This happens with us, but we don't understand the calculations that are going on in the background. Because we don't 'understand' the calculations we call it a magical spirit. <b>I am just saying we need to build the databases, and algorithms for a computer to use to make decisions, then we can start talking to the computer</b>[<font color=red>industrial</font>]<b>.</b> I don't believe our self awareness is a magical spirit. In the same way I don't think this database, and algorithm will be self aware in a magical way. If we build the databases, and algorithms that are designed to answer the question: should I go for a walk, it will come up with an answer, yes or no. <b>Then if you have a computer that is capable of going for a walk, then you have AI</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> SO stop saying that all I have is an algorithm and a database, because that is all AI is. <b>Just because you don't feel like an algorithm and a database, does not mean you are not one</b>[<font color=violet>market</font>]<b>.</b> Putting reasons to agree and disagree in separate columns will help computers help us analyze issues. <b>People give hundreds or thousands of reasons to agree or disagree with each conclusion</b>[<font color=blue>civic, </font> <font color=violet>market</font>]<b>.</b> <b>Each of these arguments have hundreds of reasons to agree or disagree with them</b>[<font color=blue>civic</font>]<b>.</b> Issues have too many factors for one person to evaluate the correct path. Groups of people have never systematically weighed all the options, giving scores to each argument in favor or opposing a particular path, and come up with a solution that is unbiased. We usually make a decision when we get tired of thinking, and stick bye it. Whenever someone gives us data that supports their side, we give them data that supports our side, and we assume our side is better, dispute the fuzzy stuff that we have not fully investigated. <b>When you debate an issue you can only debate one very small issue at a time, but you go back and forth on this small issue, and feel like you aren't making any progress, because you know there are hundreds of other related issue, and so even if you did a real good job of making your point, you haven't won the war, because that would take weeks</b>[<font color=violet>market</font>]<b>.</b> So we continue to have frustrating exchanges with people, on minute details, and we never make progress, because the issues are too big, and we don't share assumptions, data sets, and world views. Only a very large database can aggregate all the points made in these small skirmishes, and evaluate the soundness of a proposed solution taking everything into account. 'thread based' chronological forums are stupid Because thread based conversations require history to make sense, going back and improving them does not make sense. Over the history of thread based chronological arguments, millions of people will say the same thing on separate websites, and their conversations will be burred and lost, because they are old. <b>It would be better for these people to get together in one place and organize their arguments by topic and category</b>[<font color=blue>civic</font>]<b>.</b> Much of the time spent in thread based conversations is spent off topic. Much of the time spent in thread based conversations is spent making and responding to personal accusations and criticism. Once we have the best reasons to agree and disagree with each belief in separate columns we can use crowdsourcing in creative ways to help evaluate each argument. In addition to the up vote and down vote that websites use, because we let one argument support a number of different conclusions, the argument will get a score, and each relationship will get a score. Crowdsourcing, obviously is not the same thing as AI, but it will be how we as a species can talk with, train, and teach AI. <b>Now all we need to do is give decision making algorithms information, so that it can choose</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> AI's already been solved, just needs a bit of lucre to make it real. Debates are typically frustrating pointless activities. Humans rarely take the simple steps that are required to ensure that their conclusions are sound. <b>As long as we keep doing things the wrong way, we are going to continue to be frustrated</b>[<font color=purple>domestic</font>]<b>.</b> Humans can, in a natural conversation, only investigate one small part of an issue at a time. Humans leave the limitations of a natural conversation when trying to debate issues. <b>valuating each of the arguments, one at a time, is the only valid way to come to conclusions</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> Humans rarely systematically give scores to every reason to agree or disagree with a belief, coming up with a thoughtful score reflecting the total summation of each of the arguments. Humans rarely write out all their assumptions, share them with the world in a transparent manner, to ensure that they are accurate. It seems weird that people carry around beliefs and assumptions in their head, and have no desire to ensure they are accurate. People believe their conclusions for hundreds of different reasons. <b>And each of these reasons have many reasons to agree and disagree with them</b>[<font color=blue>civic</font>]<b>.</b> For thousands of years it would have been very difficult for mankind to outline all these beliefs, link them together, organize them, and investigate their validity. For thousands of years, people have weighed all of this data with complex logic they don't even understand. Now that we have the capability of organizing our arguments, and evaluating them in a straightforward way, it seems weird that we don't. It seems weird that we are still making decisions in a disorganized emotional manner. <b>It seems weird that we have sequenced our DNA, but we haven't even organized our arguments and beliefs yet</b>[<font color=purple>domestic, </font> <font color=red>inspired</font>]<b>.</b> You can't address our beliefs in a linear fashion, but we have had complex relational databases for a number of years now. We should investigate our beliefs, taking each assumption, and argument one at a time. <b>Our problems are too complex for people to organize without relational databases</b>[<font color=blue>civic</font>]<b>.</b> People's brains can only consider a few items at a time, this makes it difficult to process complex issues. However, people believe their conclusions for hundreds of different reasons. <b>Each of these reasons have many reasons to agree and disagree with them</b>[<font color=blue>civic</font>]<b>.</b> <b>Simultaneously evaluating the relative validity of each of these arguments is too much non linear data for people to process</b>[<font color=red>industrial</font>]<b>.</b> <b>One person can not usually possesses even a small portion of data that is relevant to a disputed belief</b>[<font color=violet>market</font>]<b>.</b> We try to come to rational conclusions, but we usually just pick a side when we get tired of thinking. <b>Books are usually one sided, not updated when new data is available, and can not be considered comprehensive</b>[<font color=red>industrial</font>]<b>.</b> Webpages are usually one sided and can not be considered comprehensive. No website currently tries to comprehensibly gather reasons to agree and disagree, and compare the validity of each argument, or come up with a total beliefs score. A relational database is the best way of outlining our beliefs. Our beliefs are are all all tied together in nonlinear ways similar to a relationship database. Assumptions are just beliefs that support other beliefs. If you change one assumption, it will change the strength of each conclusion that builds on that assumption. In a relational database you can say 5 people live together, then when you change one person's address, it can change all of their addresses. In a similar way, if we strengthen or weaken any assumption in a relational database, it will strengthen or weaken all of the conclusions that are based on these assumptions. Defining all these relationships is the only way we can ever make any progress at weighing all the data that we have. We can build a relational database that outlines our beliefs relatively cheaply Politicians spend billions of dollars trying to tell us that they can know what we should do. It would be much more cost efficient to spend a few thousand dollars, and build a database that allows us to outline our beliefs, and build an algorithm that promotes rational thinking. The algorithm could be very simple, and could generate information about the strength of the belief, just by counting the interconnected assumptions and arguments, with some very simple good idea promoting procedures. If we target people that already give money to political parties, and branch out to those who might give money to a political party that doesn't suck, that promotes good ideas that can be backed up with a clear and easy to understand relational database, than we could probably make bring in enough money to continually improve our algorithm. <b>I don't think you get a notification unless I talk to you directly, so I don't think I am spanning anyone</b>[<font color=orange>project</font>]<b>.</b> I didn't fully explain the idea, and so as I have a better explanation, I am adding it. IS Idea Score n of times that a belief is removed from another belief in a relational database that allows one conclusion to be used as a reason to support another A D Agree Disagree Reasons RA D Reasons to agree or disagree This formula assigns: 1 point per reasons to agree or disagree, a point for reasons to agree or disagree with reasons to agree, 1 3 a point for reasons to agree or disagree with reasons to agree with reasons to agree We will have to have a way of tagging two beliefs as ways of saying the same thing. Therefore if 50 of people believe two arguments are saying the same thing, then the two arguments will contribute 1.5x instead of 2x their scores. Money M Money invested in a belief TM Total Money invested in Forum B number of beliefs The average amount of money invested in an idea TM B. The goal of this idea is to assign 1 point for the average belief, and 2 points for a belief that has twice the average amount of money invested. Logic Professors NP Number of times a certified logic instructor has verified discounted the logic of a reason to disagree Summing ns would mean that if a logic professor disagreed with a Books B Books that have been said to support or oppose the given conclusion BS Books Score. <b>Books scores can take into account number of books that are sold, as well as the score given from book reviewers, etc BLS Book link score</b>[<font color=orange>renown</font>]<b>.</b> You can have a good book, that doesn t actually support the proposed belief. Each argument that a book supports a belief, becomes its own argument that that its own book linkage score that is given points according to the above formula Up Down Votes UV DV Up or Down Vote U Number of Users We will have overall up or down votes. <b>We will also have votes on specific attributes like: logic, clarity, originality, verifiability, accuracy, etc</b>[<font color=red>industrial</font>]<b>.</b> Other Stuff, like movies, songs, experts, etc Similar to how I say books can support or oppose different conclusions, movies (often documentaries) can support or oppose different conclusions. All of this data could be imported, as well as the formal logical arguments that a movie actually attempts to support or oppose a belief. <b>When we submit beliefs as reasons to support other beliefs, and give higher scores to conclusions that have more reasons to agree with them, people will try to submit beliefs that don t really support the conclusion</b>[<font color=orange>project</font>]<b>.</b> For instance someone might post the belief that the grass is green as a reason to believe the NY Giants will win the super bowl. <b>The beliefs that the grass is green will receive a high score, but the Link Score as will be close to zero</b>[<font color=red>industrial</font>]<b>.</b> As we work this out we may have to apply multiplication factors to not give too much or too little weight to a factor. Who has a.edu e mail address from the philosophy department of an accredited university I like that people are thinking about AI. <b>If we want to talk to computers, or if we want computers to care about the same things we care about, we need to enter our beliefs into structured databases</b>[<font color=orange>project</font>]<b>.</b> In addition to helping computers learn how to think about issues the way that we do, it will help us organize our thoughts. There are many ways of ranking and promoting higher quality debate. <b>Using these algorithms will teach computers how to evaluate our beliefs</b>[<font color=red>industrial</font>]<b>.</b> Just as Google uses links from human web designers, algorithms (AI) will need our help learning how to think in a way that we would call intelligent. <b>We need to teach AI formal logic, so that it can understand how logical arguments work</b>[<font color=red>inspired</font>]<b>.</b> And we should advance that aspect of AI as far as possible. <b>One way to generate this data is with a structured forum that feeds a database</b>[<font color=red>industrial</font>]<b>.</b> We will want a structured forum for explaining to AI what we value. One to set up a system where people evaluate the different aspects of a debate, and feed this system to a database. In addition to the up or down vote that many forums use, users should be able to evaluate specific aspects of the debate such as logic soundness, accuracy, originality, and proper use of speech in explaining the idea. <b>If you want to accuse someone of being unoriginal, the form would have a place for submitting when the idea came up earlier</b>[<font color=purple>domestic, </font> <font color=orange>project</font>]<b>.</b> For example, there are 19 easy ways to evaluate the logical soundness of an argument. There are 9 common material fallacies, and 10 common Verbal fallacies. An intelligent discussion forum would have an easy to use form that tags an argument as one of these fallacies. <b>If you fed this database to Watson, AI could learn what type of arguments will be called unoriginal, or straw man arguments</b>[<font color=red>inspired</font>]<b>.</b><br/><br/><br/>****************************EXAMPLE 6************************************<br/>Number of words:914<br/>Number of sentences:36<br/>Total civic:0<br/>Total domestic:1<br/>Total green:0<br/>Total industrial:8<br/>Total inspired:0<br/>Total market:4<br/>Total project:2<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/147s9v'>https://www.reddit.com/r/artificial/comments/147s9v</a><br/><br/> Anyone cares to share some links about this subject? Software that writes software how much progress has been made in this field?. <b>I will provide you citations to state of the art in software that writes software</b>[<font color=red>industrial</font>]<b>.</b> You can review this material and decide for yourself how much progress has been made. + ( + ( juergen goedelmachine.html) + (ftp: ftp.idsia.ch pub juergen gmAGI.pdf) + ( + ( Do you understand Goedel machines well enough to explain them clearly? I've discussed them a few times in my lab meetings and we're still confused, so I'd really appreciate it if you could. Let's say we're using the Goedel machine to control a wheeled robot in an environment, and its goal is to forage as much 'food' as possible. <b>How does the machine control its movement? How does the machine improve itself? Goedel machines: juergen goedelmachine.html Now that is some fascinating shit right there What do you mean by 'writes software'? There are plenty of techniques for automatically generating code to solve a certain problem: genetic programming, evolutionary computation, machine learning</b>[<font color=red>industrial</font>]<b>.</b> For an overview of the subject and some classic books articles, check out I would also love to know more about this. Every since I started in programming, this has been in the back of my mind. Why is that we can't yet write a program from common language. like 'Computer, I need a program that spellchecks this particular document and sends an email to XYZ with a list of changes every morning between 8 and 9am.' Obviously just a very simple example, but I would love to see this in action. <b>Apple tried to do something similar with Hypercard and then AppleScript, but there were times the script would fail and you would have no idea why</b>[<font color=orange>project</font>]<b>.</b> <b>The syntax was actually more confusing because it was English like</b>[<font color=red>industrial</font>]<b>.</b> <b>One trouble is that what you are doing is not communicating with the computer</b>[<font color=violet>market</font>]<b>.</b> <b>Any steps you can omit are only because someone else has already told the computer how to do it (i.e compiling C to machine code, or interpreting a Ruby script)</b>[<font color=red>industrial</font>]<b>.</b> This really comes down to 'why haven't we been able to make an interpreter smart enough to communicate with programmers'? We haven't because it's a tremendously difficult problem. Some limited progress has been made in small sub fields, as pointed out by lucastx. But in general? Not yet, and probably not for quite a while. <b>I used to work (very briefly, thank god) with ( which tried to emulate English language, so a program would be like 'When the user presses the btn1 button then check if txt1 is empty</b>[<font color=orange>project</font>]<b>.</b> <b>If txt1 is not empty then send its text via TCP connection to mycomp.com:6521' For some reason I hated it so much</b>[<font color=red>industrial</font>]<b>.</b> <b>I would actually prefer if it generated Java code so I could tweak it by hand</b>[<font color=violet>market</font>]<b>.</b> But I look at some of the projects that exist at the moment and the tasks are really simple, like you have a system that sends messages between subsystems. Then someone decides to add a new field to one of the messages and do some logic with it, which then has some impact on some of the other subsystems. <b>This takes a lot of effort to implement, test, deploy</b>[<font color=red>industrial</font>]<b>.</b> but it doesn't require a lot of intelligence to do. Computer, I need a program that spellchecks this particular document and sends an email to XYZ with a list of changes every morning between 8 and 9am. <b>There are lots of examples of just one of these natural language constructs, like Google Calendar's ( or ruby's ( But I think that if you try to make a language that joins lots of these constructs, complexity ensues</b>[<font color=violet>market</font>]<b>.</b> <b>Some problems that arise: You can say these instructions in lots of different orders Each construct can be said using lots of synonymous or redundant words (e.g 'this document', 'this particular document', 'this text document', etc) The language should then be flexible enough to handle lots and lots of different spellings and words and orderings</b>[<font color=red>industrial</font>]<b>.</b> If, on the other hand, we try to reduce the possible words that the language accepts, we make the task of programming even harder, because instead of remembering a concise and small set of words, now the programmer has to remember that some words work and some do not. <b>As each type of construct (e.g time constraints, spellchecking, file handling, etc) will have its own particular set of allowed words, this will soon become a nightmare to remember</b>[<font color=red>industrial</font>]<b>.</b> The redundancy and flexibility of our natural language may be fit for us, which have flexible and redundant (and error making) brains, but maybe not for computer software, that we specifically want to be precise and error prone. Or, maybe, we will use these languages in domains where we don't care about occasional errors errors that won't crash the system or show a traceback message, but cause small misunderstandings that won't be noticed immediately. I think that this is the reason why computer languages try to eliminate pronouns and prepositions, or at least turn them into consistent symbols or functions like 'alan.name' or 'name(alan)' instead of 'the name of the person Alan', or 'Alan's name'. When you have a consistent syntax, the burden of remembering the language constructs (specially in large, complex systems) is a lot smaller. <b>I suppose the same reason you can not give a person vague instructions, and have them do what you want</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> Did you start in programming yesterday? You don't have to do much programming before you realize that a program that can act on arbitrary natural language would be really really hard to write.<br/><br/><br/>****************************EXAMPLE 7************************************<br/>Number of words:148<br/>Number of sentences:9<br/>Total civic:0<br/>Total domestic:1<br/>Total green:0<br/>Total industrial:2<br/>Total inspired:0<br/>Total market:0<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/14k2zj'>https://www.reddit.com/r/artificial/comments/14k2zj</a><br/><br/> All this is saying is that machine learning is better at predicting stuff than experts at some things. The classic example of this is the soybean prediction problem. <b>Experts in the field were blown out of the water by machine learning</b>[<font color=red>industrial</font>]<b>.</b> Every single time machine learning has beat out experts so the title is quite accurate. Machine learning and computing power is mature and it should be much more widely deployed. I work with medical images, and though our data mining approach is better at detecting certain stuff than medical experts, we still needed their help in getting an idea of what image features might be important and useful to feed into say, a random forest. <b>So, in some cases I think the best approach is to let expert knowledge guide the data mining process</b>[<font color=red>industrial</font>]<b>.</b> <b>This isn't to say you shouldn't try to think out of the box once in a while though, of course</b>[<font color=purple>domestic</font>]<b>.</b><br/><br/><br/>****************************EXAMPLE 8************************************<br/>Number of words:354<br/>Number of sentences:15<br/>Total civic:0<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:1<br/>Total inspired:0<br/>Total market:0<br/>Total project:1<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/15binc'>https://www.reddit.com/r/artificial/comments/15binc</a><br/><br/> What do you all think about adding flair to this subreddit? r evolution has a cool idea: let's have flair to demonstrate the respective research background each member here has, or if they're an AI enthusiast. <b>(Thread: The gist of it is: you have to have at least started graduate studies in an AI-related field to get a professional designation (e.g, Machine Learning or Natural Language Processing ), otherwise you can receive an unprofessional designation (e.g, AI Enthusiast )</b>[<font color=red>industrial</font>]<b>.</b> I think it'd be a neat way to get an idea where everyone's research background lies, and possibly open up networking opportunities in the AI field. I think that the only problem with this is it puts an emphasis on Graduate studies in it. There are still people working on Machine Learning and NLP in their jobs that didn't go to grad school for AI related fields, yet they might know more about it than someone that just started a grad level course. Also what about people like me who did graduate studies at the Master's level but haven't done anything beyond that. Do I designate myself as an 'AI Enthusiast' or 'Machine Learning'. Well, since AI is a more applied field, we can adapt r evolution's idea for AI and include professionals who work on AI in non academic professions. Having 'professional' flair will give credence to those who do not deserve it, and create bias against an enthusiast unjustly. If you want networking opportunities, check out google communities which are tied in more closely with your real name and not reddit's engrained anonymity. There are meet ups, discussions, conferences, and conventions that would be more up your alley. <b>Maybe I just don't see the point, seeing as my graduate work would lead credence to three different flairs appended to my name, and it is that graduate work which really enforces how little I know</b>[<font color=orange>project</font>]<b>.</b> Which specific Google communities are you talking about ? I don't think the flair will impose any sort of value on people's posts, but rather give an idea of the person's background. Flair? Like little Nyan cat icons next to our names? Take a look at the flair in the linked thread.<br/><br/><br/>****************************EXAMPLE 9************************************<br/>Number of words:106<br/>Number of sentences:7<br/>Total civic:0<br/>Total domestic:1<br/>Total green:0<br/>Total industrial:0<br/>Total inspired:0<br/>Total market:1<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/17dmp5'>https://www.reddit.com/r/artificial/comments/17dmp5</a><br/><br/> Meet Simon, a robot that learns by watching demonstrations, and asking questions. OK Simon, you know what, I'm just going to pour the cup myself. This comes out of Georgia Tech's ( Now a bartender that can learn new drinks One step closer to an automated bartender. There is actually a paper about ( IMO the term learning is misleading because it presupposes intelligence and language understanding. Why spend the time and effort to anthropomorphize when you can't build in any linguistic variability and just a hint of context dependent learning. <b>Then again, better than I could do I think they need to take that dude's robo bong away from him</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b><br/><br/><br/>****************************EXAMPLE 10************************************<br/>Number of words:136<br/>Number of sentences:8<br/>Total civic:0<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:0<br/>Total inspired:0<br/>Total market:0<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/17kj97'>https://www.reddit.com/r/artificial/comments/17kj97</a><br/><br/> New 1.6 billion supercomputer project will attempt to simulate the human brain. Not sure if this is redundant information, but to complete the picture: It's 1 billion Euro (or 109 Euro, to avoid the billionmilliard vs Billiontrillion confusion), equally split over 2 projects, for a duration of 10 years. So it's 50 million Euro ( 68 million USD) per year for the 'Human Brain Project'. Still a lot of money :) They will need to be clever about how to teach this brain how to think. Kurzweil suggests starting with functional simulations which might be computationally cheaper, more details can be added to simulations later and still preserve the digital brain's learning. This little gem could be the beginning of the end Or the end of the beginning. They should do a simulation like this of every organ in the human body.<br/><br/><br/>****************************EXAMPLE 11************************************<br/>Number of words:1174<br/>Number of sentences:51<br/>Total civic:0<br/>Total domestic:2<br/>Total green:0<br/>Total industrial:6<br/>Total inspired:0<br/>Total market:8<br/>Total project:1<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/18v1h2'>https://www.reddit.com/r/artificial/comments/18v1h2</a><br/><br/> Miguel Nicolelis Says the Brain is Not Computable, Bashes Kurzweil s Singularity. <b>FTA: You can t predict whether the stock market will go up or down because you can t compute it, he says</b>[<font color=violet>market</font>]<b>.</b> <b>You could have all the computer chips ever in the world and you won t create a consciousness</b>[<font color=violet>market</font>]<b>.</b> Most people working on strong AI are trying to make something that behaves like a mind, not predict exactly what an existing one will do. After all, no two minds are exactly alike, and they certainly do have unpredictable characteristics. He seems to be saying that the brain is not computable because it is nonlinear and therefore unpredictable. If that is indeed what he is saying, that is nonsense. There are many examples of nonlinear, unpredictable functions that are computable for example deterministic chaos. <b>Pendulums will never be modeled While I don't agree with his stance, I think he was misquoted a bit, at least from what he later told me (my boss)</b>[<font color=purple>domestic</font>]<b>.</b> <b>That said, we don't know if the brain is actually deterministic or not</b>[<font color=violet>market</font>]<b>.</b> <b>And if it is, it would still be very very very hard to model, completely, in a computer, without some really smart shortcuts that currently don't exist</b>[<font color=orange>project</font>]<b>.</b> 'Hard to compute' is light years different from non computable in the mathematics sense. I think he might mean that simulating the brain is an NP complete problem. If so, then yes it's extremely hard to simulate it. I can't see how it would be NP complete though (the brain itself clearly works and is made of ordinary matter.) The Nintendo SNES (a 25MHz machine) only recently became 'computable'. <b>See you need something like 100X the power of the original hardware to emulate it on a general purpose machine</b>[<font color=red>industrial</font>]<b>.</b> <b>It's almost impossible to emulate hardware, but that doesn't mean hardware can never be surpassed</b>[<font color=red>industrial</font>]<b>.</b> You can't play Speedy Gonzales on an XBox, but that hardly means the XBox is inferior. If your goal is to play Speedy Gonzales, then the XBox is inferior. However, since we are trying to achieve general intelligence, the brain can be considered simply as legacy technology. I think that with all the technological advancements made in the past 10 years alone, it is silly to believe that we can't achieve currently unfathomable computing power. <b>Kurzweil's singularity doesn't require the brain to be computable</b>[<font color=red>industrial</font>]<b>.</b> <b>It just requires super intelligence through technology</b>[<font color=red>industrial</font>]<b>.</b> <b>If we attain that by simulating brains on super computers or by augmenting brains with computers doesn't matter</b>[<font color=red>industrial</font>]<b>.</b> Saying we can't have intelligent machines because we can't simulate brains is like saying we can't make cameras if we can't simulate eyes. <b>It is both tragic and slightly disappointing that such a circular argument could really escape a mouth of one who is evidently so capable</b>[<font color=violet>market</font>]<b>.</b> In the article Nicolelis says the following: 'You can t predict whether the stock market will go up or down because you can t compute it' Of course as we all know every single event which occurs in the stock market is a product of a human making a decision correct? And every human decision is a product of the brain state of that individual, including things such as their knowledge and their emotional state. ( Please note I am not saying that things like natural disasters don't affect the stock market, only that in order for them to do so they must work through the human brain. <b>) So then in order to predict the stock market perfectly at say 1 second from now we would need to predict the decisions of every person capable of altering the stock market in the next 1 second</b>[<font color=violet>market</font>]<b>.</b> And so his argument is this: We cannot compute minds, because we cannot compute minds. every single event which occurs in the stock market is a product of a human making a decision correct? Nah, maybe 30 years ago this was the case. Now we've got advanced algorithms to do a lot of that for us so, in fact, the stock market is more predictable (in theory) than it used to be, but the algorithms used are 'secret sauce.' its most important features are the result of unpredictable, non linear interactions amongst billions of cells he presents no argument as to why those cells, their connections, and even the chemicals that bathe them, could not be emulated in a computer Completely ruling out artificial consciousness without any foreknowledge of the future is a shortsighted view to have. 100 years ago who could have guessed that we d have pocked sized computers that can understand basic natural language input, or cars that can drive themselves? Technology isn t going to stop evolving, and there s no reason to believe that we won t be able to recreate the algorithms of the brain in software form within the next 100 years. I understand your submission has 32 upvotes here at r artificial but in all honesty, this article is better suited to r neurophilosophy or r neuroscience or r transhumanism or r singularity The field of research called Artificial Intelligence is not concerned with building a copy of a human brain. The biggest researchers in the field have openly said in public interviews that they are not out to build humans. The reason I posted it to r artificial is because I don't think neuroscience is outside of the scope of interest to Artificial Intelligence. Also, with all due respect, to say that 'Artificial Intelligence is not concerned with building a copy of a human brain' or 'the biggest researchers in the field have openly said in public interviews.' sounds like weasel words if you don't have citations to back it up. Maybe you are not concerned with building a copy of the brain (and that's fine, I'm not either), but I think you are underestimating the field's breadth. This guy's point isn't exactly clear, what difference does it make if we find it easier to swap out a person's brain with different hardware? The end result is a silicon brain either way. Simulation just seemed to be the easier way to go about it, this has no effect on the chances of an exponential growth in processing speed. Nice downvoting, but this is just another circle jerk over some 'intellectual' treating science as a spiritual discussion. I'm not sure how this has a place here, as others in the thread are easily refuting the entire article logically in one or two sentences. <b>Even if you think he is easily refutable, it still must be done</b>[<font color=violet>market</font>]<b>.</b> We will never be certain if the brain is computable or not if we don't try to do it and when a supposedly respectable scientist basically puts forward a barrier, it could help turn public opinion against increased funding. <b>It being wrong I do not think is a reason for me to delete it</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> <b>If it is so wrong as to be uninteresting then I can downvote it</b>[<font color=violet>market</font>]<b>.</b> But the line for me where I step in and delete it is completely off topic rather than wrong. <b>He is a neuroscientist so he thinks he is automatically an expert philosopher and computer scientist</b>[<font color=red>industrial</font>]<b>.</b> It was researchers from Duke who published that bogus research 'proving' ESP all those years ago. Perhaps it it still a hotbed of pseudoscientific nonsense.<br/><br/><br/>****************************EXAMPLE 12************************************<br/>Number of words:221<br/>Number of sentences:11<br/>Total civic:0<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:1<br/>Total inspired:0<br/>Total market:0<br/>Total project:1<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/195sva'>https://www.reddit.com/r/artificial/comments/195sva</a><br/><br/> To be honest it feels like I'm getting in over my head. The articles I've read use many terms and talks about algorithms I've never heard of. I've almost given up trying to find comprehensible sources, and so I'm turning to Reddit for help. What I'm looking for is something related to pros cons of AI-development not the philosophical aspect, but in terms of how this will relate to society in general. Will it start replacing people's jobs? Or will it create new ones, or a mix of both? How can AI be applied in every day life? I'm writing a paper on Artificial Intelligence, and I need some good (peer reviewed) sources about it. Reading stuff you don't understand is part of the learning process. Have you looked at references from the wikipedia article for ai? I've mostly only checked out the section on Perception Ethics, and the references they have for AI replacing jobs is just a wordpress article. <b>( Why not write to the author and ask if he could send you a PDF version? If you have access to a college computer network you might parooze Artificial Intelligence or google some conferences on AI</b>[<font color=red>industrial, </font> <font color=orange>project</font>]<b>.</b> Besides all the highly technical papers on methods, there surely will be a few less technical papers discussing the future of AI, etc. I've also seen some NYTimes articles on this subject.<br/><br/><br/>****************************EXAMPLE 13************************************<br/>Number of words:2019<br/>Number of sentences:84<br/>Total civic:0<br/>Total domestic:5<br/>Total green:0<br/>Total industrial:21<br/>Total inspired:0<br/>Total market:8<br/>Total project:1<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/19n03z'>https://www.reddit.com/r/artificial/comments/19n03z</a><br/><br/> <b>I wanted to make a bot that is capable of performing actions based on simple english-like input</b>[<font color=red>industrial</font>]<b>.</b> <b>Like typing 'create a new appointment tomorrow with Turing' The idea started simple; have it reconize certain keywords ('appointment', 'todo' and 'person' for example), then have it reconize actions that can be pefromed on it ('create', 'check', 'update' for example) and then reconize a bunch of other tokens (date, name, etc) in the same sentences and see if you get enough information to save it to a database</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> <b>If so, save it and if not, ask the user to supply more info</b>[<font color=violet>market</font>]<b>.</b> that's how the idea started, After I made the lexer to reconize a bunch of different tokens, I figured I'd probably need to reconize some sort of grammer, and before I knew it, I made a syntax free grammer parser that is (almost) capable of interpreting a programming language. Obviously I went complety above and beyond my initial goal. <b>I feel like I'm doing a 100 meter sprint in a Lamborghini and, with the finish line a couple of kilometers behind me, still giving full throttle</b>[<font color=purple>domestic</font>]<b>.</b> However, this is a very nice Lamborghini and therefor, I don't want to hit the brakes just yet. Now I want to make a language that is aimed at AI development. I think this could actualy work out quite well because even though AI is very broad. The problem now is; my experience with AI is limited and I'm unsure what features the language will need in order for it to be usefull for AI-developers. <b>That's where I could use some help with, what language-features do you think are necesary (or just nice) to have? So far, the language is in the earliest forms, meaning it can go anywhere from here</b>[<font color=purple>domestic</font>]<b>.</b> (So far; blocks, if statements, function statements (without parameters yet), variable assignment and basic operators (which don't know about precedence) are supported, nothing more) Here are a couple of random ideas I'm just gonna throw out; Making every script a finite state machine could be nice for gaming AI's but would other types of AI benefit for that? Switching boolean-logic with fuzzy-logic? Each fuzzy-element have an X amount of fixed numerical properties and a 'minimum' property. If the sum of the properties match the minimum it is considered 'true', otherwise 'false' but they can also be added, subtracted and compared with other fuzzy-logical types allowing the possibility that 2 things are 'true' but one is more 'true' then the other. <b>Maybe just have it an extra data-type? Some sort of short-term memory should be usefull</b>[<font color=red>industrial</font>]<b>.</b> maybe key value based? 'remember( key , value )' and 'if(recall( key ) value ) then.' Though what should value support? Just strings would make it easy to implement but allowing all sorts of values would make it usefull, saving a Fuzzy-logic value could literly allow you to check if a simulated result is better then the last know result. Why not add a maximum amount of items to be saved? This could allow you to track cause-and-effect. Some sort of deviding conciousness and unconciousness, where unconsiousness should offer options to the conciousness from which the consiousness picks one? Wouldn't this just be 2 scripts, basicly? So what would that look like? like an API for itself? What kinds features would you like to see? What would you like from a programming language for AI?. I don't know how much value there is in designing a new language vs. <b>creating a library in an existing language, but here are some general ideas</b>[<font color=red>industrial</font>]<b>.</b> <b>Primitive data types for working with probabilities and log probabilities</b>[<font color=red>industrial</font>]<b>.</b> Floating point is unbalanced in that there are more possible values near 0 than near 1, and it wastes a lot of bits if your values are limited to 2. <b>A simple, declarative way to define and manipulate graphical models and neural network structures</b>[<font color=red>industrial</font>]<b>.</b> <b>Promote the concepts of clustering, classification, and regression to be first class constructs on par with conditionals and loops</b>[<font color=red>industrial</font>]<b>.</b> <b>I haven't dug into Perl 6 yet, but it caught my eye a few years ago with regard to grammars, parsers and reflection</b>[<font color=purple>domestic</font>]<b>.</b> <b>I believe ( deals with point 1 (and possibly others) 'Floating point is unbalanced in that there are more possible values near 0 than near 1' What do you mean? Could you expand on your points? I'm having a hard time visualizing what you mean</b>[<font color=violet>market</font>]<b>.</b> <b>Most languages already have features you described or can be very easily implemented using existing features</b>[<font color=red>industrial</font>]<b>.</b> Your remember('key', 'value') is a simple map for example. <b>I advice you to stick with your goal, otherwise you will not even finish it if you overextend</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> <b>;) And (almost) any feature of any language can be implemented in any other language in some way</b>[<font color=red>industrial</font>]<b>.</b> <b>The idea is to make an easy to use language specialized for AI, just to make it easier for devs to implement them</b>[<font color=red>industrial</font>]<b>.</b> Oxymoron of the day: 'syntax free grammer parser' Well, consider forth, or lisp. Have you looked into existing languages for this kind of stuff? I've worked with AIML, which is very fun to toy around with but is hardly the same thing. I am familiar with a couple of other programming languages. <b>Do you know of existing programming languages with a focus on AI? reflection macroing optional types (both dynamic and static typing support like in C ) an easy syntax to create tree structures with Prolog style pattern matching anonymous functions, but at least some syntactic sugar for filter and map debugger with user defined hooks (ability to inspect stack frames etc.) If you don't mind my asking, what experience do you have in programming language design? I'm a webdev mostly, working with PHP and JavaScript</b>[<font color=red>industrial</font>]<b>.</b> Experience in actual language design? None unless I count this project. So far, functions have cost me three days of work to get working and I only have scoping barely working so I'm not feeling to confident about classes. <b>Also, every error message is gonna begin with 'I'm sorry , I'm afraid I can't do that.' I consider every relevent reference I can put into it as a bonus</b>[<font color=violet>market</font>]<b>.</b> What do you mean by tagging variables, though? Garbage collection. <b>I know most modern languages have this already, but I just wanted to get it in</b>[<font color=purple>domestic</font>]<b>.</b> <b>Might not be AI specific but I do love me some garbage collection I don't understand why you need to undertake this project judging from your replies to other posts</b>[<font color=orange>project</font>]<b>.</b> If you claim you 'know' python a verb which I would be extremely careful to use then why not start with what you already have? Building a Python framework would best suit your need; you would know where to start, what to do, and what to implement all you need to research is how to implement and how people make use of AI. Taking a look at a game source code would be a place to start. Well, I tried to explain it in the post itself but I'll try again. <b>For my original goal, I needed a simple lexer and a very basic parser because I didn't really need any grammer rules</b>[<font color=red>industrial</font>]<b>.</b> I could have used Python but any language would do, really. So I build that, but when I was working on the parser, I noticed I could add a lot of functionality with little effort and thought, why not? So I expanded it and started to make more complex grammer rules. Obviously, this was a snow ball effect and now I have the beginnings of an interpreter for a programming language. But it was quite awesome to get this far and every feature I manage to put in feels very rewarding because, quite simply, this is one of the most fun projects I've worked on in a while. However, any language needs a goal, something it's good at. <b>So why not AI? This is a project that is fueled more with 'why not?' than anything else</b>[<font color=violet>market</font>]<b>.</b> <b>Find a language which is powerful enough and then write libraries to do all you want</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> Is there a better way to fully understand how a wheel works? My experience with AI constructs is about a zero but wouldn't it be interesting to see a language where time were treated as a first class citizen? As in, you can define how time progression affects recall, bias and other deterministic behavior as a built in feature? For instance, a model begins to find an ideal connection between two points of data. After 30 seconds of doing other operations elsewhere, the routines that revisit that ideal connection begin to forget exactly how it got there the first time, and may be inclined to select an alternative route. <b>The routine's definition could also outline how fast or slow this degradation would occur, as opposed to mapping these interactions yourself</b>[<font color=red>industrial</font>]<b>.</b> <b>Pattern matching, constraint solving, data structures for spatial queries (2d 3d), FSM are a pain in the ass</b>[<font color=red>industrial</font>]<b>.</b> <b>I'd give up some flexibility for gains in ability to read, debug and understand what is going on</b>[<font color=red>industrial</font>]<b>.</b> I was about to write a long post about wanting dynamic run time lists which can be used with set theoretical operators. And then paragraph of digressions into homoiconicity. I'd like to see the ability to call functions as 'interrupts' such that, the whole program's state is restored when control is handed back to the calling code. A language fit for AI is going to end up, inevitably, looking like a 'statistics package'. It may also end up something like a Neural Network API, with many different versions of networks, such as spike timing dependent plasticity, as well as orthodox networks. Among those being perceptrons with backprop, Self organizing Kohonen map, and hopfield. <b>You might consider checking out opencog, and its probabilistic logic network</b>[<font color=red>industrial</font>]<b>.</b> <b>There may be inspirational material in there regarding some language level API for probabilistic logic</b>[<font color=red>industrial</font>]<b>.</b> Thnx, I'll check it out :) Firstly, I like how you've cheerfully kept on reminding people that you're doing this for educational purposes, which is great. As for features, you seem to have been focusing on symbolic techniques, but there's a whole lot of non symbolic stuff (including neural nets like neurobro and moscheles mentioned) for which linear algebra is going to be pretty important. <b>So try throwing some nice syntax for that in there</b>[<font color=red>industrial</font>]<b>.</b> I'd also like to second neurobro's ideas about primitives for probabilities (and everything he said really), and extend it to say nice primitives decent syntax for representing and manipulating probability distributions (both parametric and non parametric) would be awesome (and probably bloody hard :P). I will never look at any programming language the same way again. I'm having a lot of trouble keeping the code clean while extending the language. So far, I have variables, functions (with proper scoping), basic math (without proper precedence), decent if else statements and arrays, (couple more stuff but this is the basic) So implementing pretty much any idea posted here is gonna be a challange, to which I look forward to taking on :) Look into prolog, I don't know how practical it is for these purposes, but I think you'll find it fascinating in any case. Yes, prolog itself is considered more an AI than a programming language. <b>Inventing a new programming language to do your AI project is like inventing a new spoken language to write your novel</b>[<font color=red>industrial</font>]<b>.</b> I know Python, but there isn't a particular feature that makes me think it's great for AI, I'm sure it's fine for the task, though. Also, the new programming language has became the goal on it's own, the AI connection is just for direction. Ever read about the history of Perl? Just learn python python.org then start with pybrain.org and scikit learn.org and then other scikits from scikits.appspot.com scikits also some basic stuff: scipy.org numpy.org sympy.org matplotlib.org Libraries index is at pypi.python.org pypi Don't waste your life for re inventing the wheel. I already know python :) Any sort of re inventing of the wheel being a waste of time is relative to what your main goal is. <b>Maybe AI research needs a new language to help overcome difficulties</b>[<font color=red>industrial</font>]<b>.</b> Yes this can either be a huge waste of time, or it can be a breakthrough in AI research. <b>If its a risk someone is willing to take then so be it</b>[<font color=violet>market</font>]<b>.</b> I feel it directly relates to the language of thought hypothesis. Maybe our current languages don't as easily allow for new understandings in AI development. I believe that any deeper understanding of something is never a waste of time.<br/><br/><br/>****************************EXAMPLE 14************************************<br/>Number of words:80<br/>Number of sentences:6<br/>Total civic:0<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:0<br/>Total inspired:0<br/>Total market:1<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1cb8ml'>https://www.reddit.com/r/artificial/comments/1cb8ml</a><br/><br/> Related: r FriendlyAI best part kittens riding a roomba Ha Ha Ha sorry, cannot get to the article video. What strong AI?? There is not even anything getting close to strong AI being developed. Even if there was and it was dangerous, pull the plug. Once someone creates it, it improves itself getting smarter and smarter, faster and faster. <b>Maybe it tricks humans into doing things for it, or spreads itself as a virus through the internet so we can't just 'pull the plug'</b>[<font color=violet>market</font>]<b>.</b><br/><br/><br/>****************************EXAMPLE 15************************************<br/>Number of words:753<br/>Number of sentences:29<br/>Total civic:0<br/>Total domestic:2<br/>Total green:0<br/>Total industrial:7<br/>Total inspired:0<br/>Total market:2<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1cblax'>https://www.reddit.com/r/artificial/comments/1cblax</a><br/><br/> <b>Guy creates computer AI that teaches itself to play Super Mario Bros</b>[<font color=red>industrial</font>]<b>.</b> That was actually a very, very interesting last line. <b>It shows you the explicit different between humans and machines</b>[<font color=red>industrial</font>]<b>.</b> This guy is pretty fuckin' smart and the video was extremely interesting. I did not expect this video to make me laugh, but I lost it a little bit on that line The Same was the conclusion to a Quake3 AI The AI isn't learning how to play so much as it's learning which locations in memory correspond to progress it's learning how to score a savestate. The actual playing of the game is trivial once it's learned this it searches for a better state by replaying chunks of the original input. The playing of the game is just search, the thing being learned is how to evaluate a state. True, but couldn't we say the same thing about what a human brain does in learning how to live as a sentient life form? Don't we only learn how to evaluate a state among all of the various alternative choices we have in our decisions about what to do next, at each moment of our life? If so, then how does learning to play the games differ from learning how to evaluate a state? It frontpage'd here: As this is open source anyone care to explain how I could run this on my own ? And how did it figure out the bugs in the game without these occuring in the training data ? Seeing the program learn to exploit bugs in Mario and the other games is simply amazing. <b>The moving down invincibility is astounding, and I'd love to know what is going on when it effectively double jumps</b>[<font color=purple>domestic</font>]<b>.</b> Also, the ending alone makes the video worth watching. Overall, really great work I think it would be fun to watch livestreams of the program play through some of the games. Is his technique in essence reinforcement learning? If it is, why doesn't he say so? It looks like GOFAI's A algorithm is the winner when it comes to playing super mario bros.: rb1006 projects:marioai A seems to be so good, that it prompted this golden comment on youtube: So I undestand correctly: da computer will play video games for us, so we have more free time? Way cool. <b>( Lexicographic ordering does only help recognizing if it is winning or losing (score++ or score ) if I am right</b>[<font color=red>industrial</font>]<b>.</b> Will defenitely look into his paper tomorrow morning to find out how the predictions about good and bad moves are made. It seems to me that the point of this research is that only armed with the knowledge of Lexical Ordering, and nothing else, software can 'infer' what it means to win a NES game. <b>When the software was quote un quote 'watching' the teacher play the game, it was not copping his moves at all</b>[<font color=red>industrial</font>]<b>.</b> <b>The point of the 'teacher round' was merely so that the software could pick up Lexicographic changes in the RAM of the NES</b>[<font color=red>industrial</font>]<b>.</b> <b>If you read the paper it reveals that the AI is limited to chunks of 10 controller inputs taken from the human playthrough</b>[<font color=red>industrial</font>]<b>.</b> It is very much copying his moves, though that's just the search strategy (and not really related to the interesting part of the paper.) Perfect ending to the video. First the program rage quits and then the narrator busts out the classic 'The only winning move is not to play' line. <b>It learned to exploit bugs that the creator didn't teach it</b>[<font color=red>industrial</font>]<b>.</b> In fact the only thing it learned from the input data was what values in the memory it should try to maximize in order to win, or at least that was my impression of it. I don't understand how it actually learned to play the game though, I'm guessing it tried random inputs and then choose the ones that helped maximize those values the most after a few seconds. <b>Which is pretty cool that that actually worked, but I could be completely wrong about how it works</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> I guess it could be student teacher modelling but this doesn't seem like very self taught if it only passes the input by imitation. In only a few places did he mention that his software 'got farther than I taught it', and he always adds that 'it was amazing and great' but then he sort of glosses over that part. <b>I think the Pacman running between a group of ghosts like that indicates that the software does not really 'understand' pac man</b>[<font color=violet>market</font>]<b>.</b> Seeing the program learn to exploit bugs in Mario and the other games is simply amazing.<br/><br/><br/>****************************EXAMPLE 16************************************<br/>Number of words:426<br/>Number of sentences:23<br/>Total civic:0<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:2<br/>Total inspired:0<br/>Total market:0<br/>Total project:1<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1cq7cm'>https://www.reddit.com/r/artificial/comments/1cq7cm</a><br/><br/> OMG: intelligence as a fundamentally thermodynamic process. The researchers suggest that intelligent behavior stems from the impulse to seize control of future events in the environment. This is the exact opposite of the classic science fiction scenario in which computers or robots become intelligent, then set their sights on taking over the world. The writer of this article says 'seize control of future events' which is not accurate. The MIT Entropica theory says that intelligence is trying to allow for all the future trajectories to happen. This is the complete opposite of the way most human beings think about 'controlling' something, which in most cases means reducing the possible trajectories of the future; that is funneling the system into a narrow phase space. In the science fiction hollywood scenario, the Super AI enslaves the human race by reducing the human's capacities and imprisoning them. Entropica theory says the complete opposite will happen. The Super AI will actually free mankind in ways we never imagined, because the AI is trying to maximize the likelihood of all future trajectories. Since the future includes humans, an Entropica AI will try to maximize our freedoms as well as its own. <b>So basically they wrote software that maximizes the entropy in a system</b>[<font color=red>industrial</font>]<b>.</b> Thus showing intelligent behavior is the process of maximizing entropy and predicting the future to do so. I don't see whats so amazing though, that was pretty obvious. I guess it's good they just stated it clearly then. This is another one of those run of the mill situations. + 1 Researcher claims he has discovered the unified theory to all intelligence. + 2 Researcher exhibits his theory in ridiculously simple systems with a few variables that are computed entirely inside of a little simulated world. + 4 Researcher pretends like this method will 'scale up' in a super futuristic time, but he in no way specifies how this will happen. <b>We saw this same pattern with OpenCYC, then Reinforcement Learning, OpenCOG, AIXI, Solomonoff Induction, and recently in computer vision via stacked autoencoders</b>[<font color=red>industrial</font>]<b>.</b> This is ( a classic example of physicsts wandering into a field they don't understand. i want to believe the article wasnt very interesting, it basically just said that intelligence is rooted in ones ability to predict the future which is pretty obvious. <b>id assume the actual paper had a bit more merit to it, anyone know where to find it? Paywalled, but ( EDIT: Discussion actual paper ( How is that obvious? It might make sense after you hear it, but it's not obvious Not that I've been able to read far enough to make any sense of how it would apply to game AI</b>[<font color=orange>project</font>]<b>.</b><br/><br/><br/>****************************EXAMPLE 17************************************<br/>Number of words:585<br/>Number of sentences:31<br/>Total civic:0<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:5<br/>Total inspired:0<br/>Total market:0<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1cv339'>https://www.reddit.com/r/artificial/comments/1cv339</a><br/><br/> <b>I am currently doing an AI module in my Comp Sci degree</b>[<font color=red>industrial</font>]<b>.</b> I have to write a paper on a specific area in one of these fields. Search Planning Learning Expert Systems Neural Networks Intelligent Robot The problem I'm having is finding journals for an idea that I have. Every time I think of something, there aren't enough sources for me to reference. (I use ieeexplore) So does anybody know an interesting specific subject I could write about that has a fair amount of papers written about it already? Need a topic for my essay on Artificial Intelligence. Have you tried scholar.google.com? There are some papers on learning from data available on social networks from this conference: There's also an open access journal on AI, though I don't know how reputable that is: There will be a lot of papers on all of these areas. It's just a matter of identifying the relevant ones. What ideas have you had so far? Use more than just ieeexplore. Another thing to do is if you do find a relevant paper look at its references, you will likely find more relevant info there too. Also check who has cited that paper since it was published. These topics get more complex (and interesting) as you go down the list. <b>I'm writing a neural network on Android in my AI class</b>[<font color=red>industrial</font>]<b>.</b> It is some very cool stuff, but has quite the steep learning curve. I'm writing my dissertation on AI: planning at the moment. <b>I've been using google scholar which 9 10 will include a PDF link to any papers i find on there</b>[<font color=red>industrial</font>]<b>.</b> Currently there is a lot of research going on based around non classical planning domains, that basically relax the constraints that make a problem classical (implicit time, finite states, deterministic static environments etc.) and there are loads of papers available that talk about solutions to these new types of problems. I've got some good pdf copies of books that may be of interest, let me know via message if you're interested and I'll send them to you. <b>Edit: The material produced at the ( is a good start if you're looking at the most recent research into planning</b>[<font color=red>industrial</font>]<b>.</b> I also did my (PhD) thesis on AI planning, specifically encoding planning as SAT. What is your thesis topic, if I may ask? i've played the game of looking for references from the 'outside' of a field and it sucks. it can be done, but it's terribly inefficient and difficult to find what's actually relevant in the field. if i were you, i'd edit your question to include the topics you've thought of and ask if anyone can give you sources and or people who work on that. and just to be clear about what i mean by including the topics you've thought about, the topics you've listed above seem way too vague to satisfy what i suggested. looking forward to hearing more specific things from you, good luck. I am assuming you want to write an introduction essay on one of these topics. IEEEXpore is a search platform that covers journals and conferences published by IEEE, which are mainly very advanced and specific. You can be absolutely sure there are a hell of lot articles published on each of these topics. <b>Google Scholar is a better search engine to find less specialized resources on these topics</b>[<font color=red>industrial</font>]<b>.</b> Why not write an ai that will write the paper for you? I'm fairly certain this was a joke, but that's actually a really cool idea. Even if the end result was rudimentary, barely comprehensible trigram vomit the cleverness of the project and the concepts explored are what would count.<br/><br/><br/>****************************EXAMPLE 18************************************<br/>Number of words:176<br/>Number of sentences:11<br/>Total civic:0<br/>Total domestic:2<br/>Total green:0<br/>Total industrial:2<br/>Total inspired:0<br/>Total market:1<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1djswg'>https://www.reddit.com/r/artificial/comments/1djswg</a><br/><br/> <b>I was actually assigned this as an interview assignment the other day, and I thought it was really fun</b>[<font color=purple>domestic</font>]<b>.</b> At the very least, it's a good way to brush up on your vector math if you're like me and haven't had to use it in a while. <b>edit: Oh god, I suck at submitting reddit links.here's the actual site</b>[<font color=red>industrial</font>]<b>.</b> <b>intro Browser based competitive game where you program the A.I</b>[<font color=red>industrial</font>]<b>.</b> good programming, I wonder who made them Yeah, those bullfighters are bastards. Did you write this yourself? I'm working on a similar with a larger scope and am looking for more developers. <b>Please check our r pyshipcommand and PM me if you are interested ;) I did not write this</b>[<font color=purple>domestic</font>]<b>.</b> <b>Is there supposed to be a link? I just added it to the post</b>[<font color=violet>market</font>]<b>.</b> I don't post reddit links often, so I messed up.sorry. Interesting, To bad i am terrible at this game :P my ships target the moons and charge Amazing, thank you for finally giving me some way of learning javascript Or Coffeescript if you want It also says they'll support Closure soon too.<br/><br/><br/>****************************EXAMPLE 19************************************<br/>Number of words:1947<br/>Number of sentences:101<br/>Total civic:0<br/>Total domestic:8<br/>Total green:0<br/>Total industrial:11<br/>Total inspired:0<br/>Total market:16<br/>Total project:3<br/>Total renown:1<br/><a href='https://www.reddit.com/r/artificial/comments/1g1hab'>https://www.reddit.com/r/artificial/comments/1g1hab</a><br/><br/> <b>Here is an interesting video: The idea that AI will ever be smarter than us seems a bit paradoxical to me</b>[<font color=violet>market</font>]<b>.</b> <b>If we can create something truly smarter than us, that thing will be able to create something smarter than itself</b>[<font color=violet>market</font>]<b>.</b> <b>And it will recreate something better in shorter and shorter time spans until infinity</b>[<font color=red>industrial</font>]<b>.</b> We cannot grasp something smarter than us, we can't even grasp how smart we are ourselves. <b>No matter how fast hardware become, the software is limited by our intelligence</b>[<font color=red>industrial</font>]<b>.</b> <b>A computer does exactly what it is programmed to do, nothing more, nothing less</b>[<font color=violet>market</font>]<b>.</b> <b>It can be programmed to mimic feelings but it's still human-controlled software</b>[<font color=red>industrial</font>]<b>.</b> What do you think? Maybe if biological computers become a thing in the future it will be possible but I don't think it's possible based on the technology we have today. Will AI ever be smarter than us or develop feelings?. That is basically the concept of a technological singularity. <b>Note that there are a lot of smart people who think it will happen (in our lifetime to less) and a good number of them are actively trying to make it happen</b>[<font color=purple>domestic, </font> <font color=orange>project</font>]<b>.</b> <b>This is one big ass assumption and frankly I don't see any good reason to assume it is true</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> Note that a lot of big complicated problems science is working on get reduced in smaller parts and are worked on by many many scientists. Sure a single human on his own won't manage to understand something like the human mind. <b>But literally thousands upon thousands of scientist working together on the problem over the course of 100+ years? Yes we don't understand the human mind completely, but we sure as heck know a lot more about it than we used to and it is really quite premature to claim that we never will</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> (And once we do improving it and running it in a simulation seems quite feasible.) Not to mention that evolutionary algorithms and machine learning are already able to find better solutions to certain problems than humans can. <b>And there are already people using evolutionary algorithms to create new programs and robots</b>[<font color=red>industrial</font>]<b>.</b> <b>It can be programmed to mimic feelings but it's still human controlled software</b>[<font color=red>industrial</font>]<b>.</b> <b>What are feelings to begin with? They may seem special at the moment, but I wouldn't be surprised if once we understand them they will appear a lot more mundane</b>[<font color=violet>market</font>]<b>.</b> Frankly I kinda expect that there won't be much of a difference between a AI trying to maximize it's utility function and a human trying to lead a happy and fulfilling life. (Both are trying to maximize some internal measure of success.) What do you think? Maybe if biological computers become a thing in the future it will be possible but I don't think it's possible based on the technology we have today. <b>Eh, I don't really know, but I will be quite disappointed if we don't manage it by say 2075</b>[<font color=violet>market</font>]<b>.</b> Of course our current technology and understanding aren't sufficient, but that's being worked on. Not to mention that evolutionary algorithms and machine learning are already able to find better solutions to certain problems than humans can. Do you have any links on this? I don't think I've heard of it. <b>It seems most people are all in or all out in this argument but I enjoyed that you seem to be fair to both sides</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> It really irks me every time people talk about AI having feelings. It is a mechanism that compels us to act in a certain way. These are often hardwired through evolution or learnt. When we program a computer with a goal of some kind it is using it's 'feelings' to make decisions. AI agent, you have 8 actions to choose from, which do you choose? The AI agent ponders and answers Well, I do have this goal function here which implements my apparatus of feelings. Ohh, says the goal function, i.e, conscience, you'd like to win this game? This action will increase the likelihood of doing that. The AI agent says You don't say? I think I feel like going for that one. <b>Now you might intervene and talk about people being irrational, like the video, but that'd have really simple explanation</b>[<font color=violet>market</font>]<b>.</b> Being irrational simply means that you have a objective goal function which is funky. If you are irrational you might simply have one that uses rules of thumb to often, or searches the game tree too short before making a decision, or have faulty implementation, or bad rules of thumb, or have a predisposition to liking stuff the rest of us doesn't, like dying when feeling to much down, or seeking adrenaline even though it might kill you. My point is that lists that say what a computer needs to be like us, like James was talking about, are often simply an arbitrary projections of our own limited perception. We feel that things matter, even though they most likely doesn't. Feelings are put on a pedestal, time has direction, we make conscious decisions none of which really need to be true. <b>A definition of intelligence I subscribe to is that of 'being able to predict the future'</b>[<font color=red>industrial</font>]<b>.</b> Being irrational has nothing to do with being intelligent. Rationality is simply making the decision that optimizes once objective goal function. Since we all have our own goal function nobody is really irrational. People are only irrational relative to some goal function that the agent doesn't agree with. Maybe I'll rewrite this in a more comprehensible manner later when it's not as late. I think the term smarter is not an accurate way of representing the idea you are referring to. There is no doubt that AI has the capabilities to store and remember (in a sense) gigantic amounts of information. <b>Computers can process information at speeds far beyond the human brain will ever achieve</b>[<font color=red>industrial</font>]<b>.</b> The idea that they could think and create innovative thoughts is another thing, but seeing as we are getting closer and closer to understanding how the mind works, I'm sure in the future similar mechanics will be put to use in AI. Weather they can develop emotion, would also be dependent on research of the brain. If you think about how we feel emotion and how it comes about, the idea that similar processes could be put in place for AI isn't a far stretch. If you were to consider how humans make choices (through moral codes and bias etc), decision making could be tuned to a equal level as of a human. Computer assisted brains, on the other hand, seem a more plausible thing. TL;DR: Nueroscience helps further AI towards and above human intelligence (and most importantly human decision making). OP I suggest looking into psychology and philosophy. Basically if it exists, we will find a way to improve upon it. 'we stand on the shoulders of giants' sort of thing. <b>I am not a follower of the 'Singularity', but I still had to downvote this out of disagreement</b>[<font color=purple>domestic, </font> <font color=orange>renown</font>]<b>.</b> Allow me to articulate: We cannot grasp something smarter than us, we can't even grasp how smart we are ourselves. If we lived 30 years ago, many people would say the game is all too complex and requires too much to just be absorbed by intuition for us to have an AI master. <b>And yet, chess computers are easily able to out perform the greatest human champions</b>[<font color=red>industrial</font>]<b>.</b> Remember a couple years back when Watson shown himself quite competent in Jeopardy? General purpose intelligence surely won't be too hard. Honestly, I find such a world view to be disgusting and frankly somewhat racist. If they behave humanly and show human emotions, what does it matter if they was originally a programme? Man is just a moist defenceless mass of tangled cells, after all. Modelling an AI on Man's own brain (an advanced sort of neural net? I am not an expert in this field) may prove more fruitful. If the AI acts human, to say that they aren't conscious or to say they are emotionless comes across to me as special pleading and a quick route to solipsism. I believe that human emotion can be accurately transferred to artificial beings, and I can't wait for that day to come. That's not how voting is supposed to work: Chess I see where you are coming from, but chess is one specific game. <b>I don't see how a generic algorithm could be applied to any game</b>[<font color=red>industrial</font>]<b>.</b> If we create such an algorithm that is truly 'smarter' than a human, that robot should be able to sit in front of a computer and instantly understand and computer game and be better than a human at it. All algorithms we have to day are specific, not generic. We would need a generic 'do everything human but better' algorithm for my initial question to become true. The comments given here have changed my views and I guess it might be possible in a distant future. The most advanced AI I've programmed was 4 in a row. I accept that the human brain can in theory be rewritten as software, but no one is smart enough to do it. <b>Maybe a group of people will be able to get close as someone here mentioned</b>[<font color=violet>market</font>]<b>.</b> That is like saying, 'no humans will ever go faster than 40 kmph as that is the limit of our legs'. For example if I ask you 'What is the surface area of Mongolia ?'. <b>It's unlikely you have that stored in your brain, yet you can get the answer in less than 10 seconds using google</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> Our intelligence is not limited to what's in our skulls. <b>For example amazon or netflix recommendations, google searches and google's cat recogniser</b>[<font color=red>industrial</font>]<b>.</b> Will AI ever develop feelings? It will certainly be able to recognise human emotions better than any human today. It's not going to be attracted to that hot blond, but will understand why you are and how your behaviour will be affected. <b>The only thing I differ on is that the AI wouldn't have similar thoughts to those of humans</b>[<font color=purple>domestic</font>]<b>.</b> Assuming that they had the physical capabilities, methinks it entirely plausible humanoid robots may be attracted to other humanoids or even biological humans. <b>It somewhat feels you're 'beating around the bush' with not directly saying whether AI will be able to have feelings</b>[<font color=violet>market</font>]<b>.</b> <b>Sorry if I seem dense or insulting, but that was my impression from reading those last two sentences</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> As I've said before, I don't see any reason why they wouldn't. It's not going to be attracted to that hot blond If I made my AI compelled to interact with blonds, or short people, or whatever, I'd say it was attracted to them. There is no reason to believe we can't understand something more complex than ourselves (assuming that AI is even more complex than us, and not just more efficient. Like a car is simpler than a horse but much faster.) Evolution created intelligence in the first place just by random trial and error. That is a lot slower at solving problems then humans are, and it created things that were ridiculously complicated. We have been programmed to do what we do by evolution. <b>We do exactly what we were programmed to do, nothing more, and nothing less</b>[<font color=violet>market</font>]<b>.</b> <b>So do humans not experience feelings? To be clear I don't think we should ever build computers that feel like humans, because of the moral implications</b>[<font color=orange>project</font>]<b>.</b> <b>But that doesn't mean it can't be done (or that it won't.) Depends what you mean by feelings</b>[<font color=violet>market</font>]<b>.</b> You can fake both in small experiments but doing it in an adaptive way is an unknown. <b>Are you implying that the brain doesn't do what it's programmed to do? if you cannot grasp some thing smarter than you, you are in luck there are people smarter than you that can imagine things smarter than them so basically your two down on the list</b>[<font color=purple>domestic, </font> <font color=violet>market, </font> <font color=orange>project</font>]<b>.</b> you don't need to imagine any thing simply look to humans who have done things that are beyond you, like breath with their mouth closed, parallel park, and have parents who are not related by birth. High five special friend now put that helmet on and head to the petting zoo.<br/><br/><br/>****************************EXAMPLE 20************************************<br/>Number of words:348<br/>Number of sentences:17<br/>Total civic:0<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:3<br/>Total inspired:0<br/>Total market:1<br/>Total project:1<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1g4ab5'>https://www.reddit.com/r/artificial/comments/1g4ab5</a><br/><br/> <b>33rd Square | Verbal IQ of a Four Year Old Achieved in AI System</b>[<font color=red>industrial</font>]<b>.</b> The question is: will it reach a five year old level next year? I mean, can it grow up faster than a human being? 'Plutinos can cross Neptune's orbit.' Yep, average four year old comment right there. I subscribe to the tenet of Embodied Artificial Intelligence. We embodied folks claim that computers will never understand language until they have physical experiences in a physical body in space. Further this research does not effect nor reflect upon this tenet whatsoever, since the questions contain a 'timeless' aspect. <b>That is, the questions go + 'How is a banana like an apple?' (timeless aspect)</b>[<font color=violet>market</font>]<b>.</b> The questions do not refer to actual events that took place in the past, nor require that the testee invoke memories from earlier conversations, understand who is asking the question, nor identify the person with which they are interacting. <b>+ 'Have we talked before?' + 'Did you ever go swimming in a lake?' Before the naysayers respond: I will remind you that Helen Keller could still feel the world around her, and also had a full human capacity of episodic memory</b>[<font color=orange>project</font>]<b>.</b> <b>One could also surmise that she still also had intuition about navigation in 3D space</b>[<font color=red>industrial</font>]<b>.</b> Can you clarify? Because that statement just seems absurd to me. Computers are 'physical bodies in pace' that 'have physical experiences', just like humans are. Isn't a metal box just as much a physical body as the human body is? And aren't 'physical experiences' just collections of sensory input data? My computer's webcam is on and recording incoming visual data. Doesn't that mean it's having a 'physical experience'? How that data is processed is up to the AI, either among the neurons in my brain or among the instruction to my CPU. I'd say most all of the upvotes are with a healthy dose of skepticism regarding the types of things this could be used for. Doesn't make it any more fun that AI is making progress : ) I highly support any research on embodied intelligence as well. <b>Embodiment goes with perception an ai might perceive data as it's physical embodiment</b>[<font color=red>industrial</font>]<b>.</b><br/><br/><br/>****************************EXAMPLE 21************************************<br/>Number of words:436<br/>Number of sentences:24<br/>Total civic:0<br/>Total domestic:1<br/>Total green:0<br/>Total industrial:1<br/>Total inspired:1<br/>Total market:6<br/>Total project:1<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1hhq5u'>https://www.reddit.com/r/artificial/comments/1hhq5u</a><br/><br/> ( Is it definitely fake, or could the poster just be mistaken in thinking they work like that? The behaviour might still emerge anyways? Ah, how do you know it is fake? Should I rather delete this post? Replying here to read the paper afterward. I walked around a little bit and they all just kept looking at me. <b>_ They could be programmed to face the nearest player automatically if they aren't moving, or something like that</b>[<font color=violet>market</font>]<b>.</b> <b>Maybe they face the nearest player bot because if they have to fight them it would be quicker to instantly switch to a weapon or something and they are already aiming at them</b>[<font color=orange>project</font>]<b>.</b> Or maybe they can't see full 360 degrees around them. It's still fake but I don't see how that part is unreasonable. There was actually a mod for Quake 2 that added neural networks to the bot AI (I remember a lot of crouchwalking), wish there was one that was released for quake 3. <b>Fake but this reminds me of the story about The Two Towers and the big battle scene</b>[<font color=red>inspired</font>]<b>.</b> Apparently an early version of the software controlling all the agents in the scene originally had them run away from each other rather than fight. <b>By fake I don't mean it couldn't have happened but the reason for this was probably some kind of overloading or some other reason unknown for us</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> <b>Well if it was real, that wouldn't explain why they would attack the player only after he killed one of them</b>[<font color=violet>market</font>]<b>.</b> The only way I can think this would happen is if they were programmed to avoid losing rather than to try to get wins. Then they would quickly learn to hide and avoid other players rather than to attack them. But that wouldn't be an fun AI to play against at all. If they were rewarded for getting kills, the optimal strategy would just be to let them kill each other as many times as possible. <b>Like have a different bot at every respawn point and one to just get killed a bunch of times</b>[<font color=violet>market</font>]<b>.</b> <b>I can't think of any goal that would lead to them just standing there but attacking only if they get attacked</b>[<font color=violet>market</font>]<b>.</b> I agree with your sentiment, science fiction is worthwhile. He was probably accessing the server (Of which he mentioned) through win7. <b>4 years of uptime for a pc would be ridiculous, but for a server i could see that happening</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> As far as I know neural networks also don't grow in size like that. They aren't log files or memories of everything that ever happened. Supposedly one of the fighting games back in the late 90's did have neural networks learning but I can't remember which one.<br/><br/><br/>****************************EXAMPLE 22************************************<br/>Number of words:25<br/>Number of sentences:3<br/>Total civic:0<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:1<br/>Total inspired:0<br/>Total market:1<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1jrym4'>https://www.reddit.com/r/artificial/comments/1jrym4</a><br/><br/> <b>'You Should Be Afraid of Artificial Intelligence' (x post from r agi)</b>[<font color=red>industrial</font>]<b>.</b> <b>Thank you for indirectly introducing me to my new addition to my subscribe list</b>[<font color=violet>market</font>]<b>.</b><br/><br/><br/>****************************EXAMPLE 23************************************<br/>Number of words:602<br/>Number of sentences:26<br/>Total civic:1<br/>Total domestic:2<br/>Total green:0<br/>Total industrial:0<br/>Total inspired:1<br/>Total market:1<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1jtzlj'>https://www.reddit.com/r/artificial/comments/1jtzlj</a><br/><br/> I'm glad to see that a lot more people are taking note of transhumanism. A potential future where homo sapiens no longer exist, simply because they chose to change their bodies. In any which case, humanity wouldn't be defined by being human anymore. Exactly Often, 'humanity' is seen as an inherently good thing. I think there are a lot of flaws with human beings and I can think of a plethora of things that could be improved to make something better. Humans have sucked and continue on to suck at self management. I think an ai could hardly ever do worse than we can. <b>If that q is actually posed, will any of us have a say? 'Should we allow'</b>[<font color=violet>market</font>]<b>.</b> Goes to the department of 'should we allow the state to spy on all of everyone's communications'. Who else here is a programmer? How many people in this subreddit have actually thought about implementing strong AI themselves. I don't raise this point because of the vast technical implications. This point needs to be brought to light in the context of how many of us tinkering with the idea of developing strong AI or working in research institutions making huge ground towards it are thoroughly considering the immediate practical ethical and moral implications? I know that there are frequent and immense discussions about the morality of a machine led world, and science fiction has dealt with the topic endlessly, but almost everyone who is dealing with this topic seems to be addressing it first from a practical technical perspective and then from a 'oh wait, what if.' perspective. I'm only bringing this up because the answer to 'Should we allow the robots to take over' is an emphatically resounding NO if the robots are made by the entities currently pouring money into having them first; military acronyms that want to send them overseas to kill without cease or remorse. When the singularity does happen, we all want an intelligence incomprehensibly smarter than us to act in our best interests and improve the world and our lives, not to efficiently kill and maim. I feel like we may be at a precipice, and a lot more money is going towards giving AI bodies that can slaughter villages than is going to ensuring that it can help humanity. <b>Did you read the original blog post? I argue that we might not need the humanity at all</b>[<font color=purple>domestic</font>]<b>.</b> <b>Would a machine which is truly intelligent be able to kill without remorse? I think we should first define what a true AI is, because a machine that hurts others without thought is not intelligent</b>[<font color=purple>domestic</font>]<b>.</b> What mast people would define as the gift of intelligence is our ability to step into another's shoe's, the ability to sympathies and make decisions that are not simply made from one point of view but with consideration of multiple points of view. <b>If it can't feel guilt, then is it an AI or simply a machine? What happens when you're totally a robot</b>[<font color=red>inspired</font>]<b>.</b> Will you be conscious anymore at that point? If not, wouldn't your transformation to a robot just be a form of drawn out suicide? Yes 1. Consciousness Every time we come up with a definition for that word, and some machine or animal meets that definition we come up with a new term. Happiness why should robots have emotions at all? Because humans do? 3. Non screw up ness so perhaps this could be fixed by giving the robots a 'robot god' that doles out tasks? All in all, no, I wouldn't need any of those three. <b>I would gladly replace all our politicians with strong AI</b>[<font color=blue>civic</font>]<b>.</b> There is a ( whose ( describe the demands of warmongering humans being overridden by pacifist robots.<br/><br/><br/>****************************EXAMPLE 24************************************<br/>Number of words:345<br/>Number of sentences:12<br/>Total civic:0<br/>Total domestic:1<br/>Total green:0<br/>Total industrial:0<br/>Total inspired:0<br/>Total market:0<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1klv3f'>https://www.reddit.com/r/artificial/comments/1klv3f</a><br/><br/> Down the rabbit hole of creative computation (and cats). The question I ask myself is 'How much time today should I spend responding to this blogspam?' There are about 100 things to say and correct here, but hopefully the redditor u fernol is in fact the author of this blog, and so there is hope he will read this. (??) This has nothing to do with quantum computers versus classical computers, and this issue has nothing to do with lateral versus linear thinking or any of that jazz. I actually started writing out a reply this morning, but it grew in size so quickly that I decided I would eventually just release a blog post myself that clarifies the situation, and set it aside. Yep, that's my blog, and I posted this here mainly to get feedback from people that know more about the field than I do. I am just a programmer that likes ponder, and this is the result of that. If you can write out 100 corrections, please do, and I'll address them as best I can :) Same goes for u tonicinhibition The post talks about why intelligence can't be implemented on classical computers as it is yet and by our understanding, but it certainly does not show the far stronger claim that it can't be done under the conception of classical computing. <b>1) I think your point is that emulating only conscious thought dulls in comparison to human intelligence</b>[<font color=purple>domestic</font>]<b>.</b> But in't conscious thinking somehow in line with thinking actively, by which I mean sometimes conscious, active thinking is the only form of intelligence necessary to be applied in various situations? At that point the computational mathematical speed of a machine learning intelligent system beats the human brain. 2) Your reason that computer's can't perform sub conscious thought seems to mainly be that 'instructions are executed with no knowledge of each other'. In the human brain, yes, neurons themselves can be said to 'interact with each other', but do you really think that our actual cognitions, i.e the 'instructions' really can be said to have awareness? I disagree with this argument.<br/><br/><br/>****************************EXAMPLE 25************************************<br/>Number of words:394<br/>Number of sentences:16<br/>Total civic:0<br/>Total domestic:2<br/>Total green:0<br/>Total industrial:1<br/>Total inspired:0<br/>Total market:3<br/>Total project:1<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1kwa4q'>https://www.reddit.com/r/artificial/comments/1kwa4q</a><br/><br/> <b>Hey there I just heared about you guys from AI and got a question for you which bugges me since I saw TERMINATOR the first time</b>[<font color=purple>domestic</font>]<b>.</b> <b>How possible is it, that something like skynet could be happening? I mean self-reproducing robots which try to kill the humans and even.how do you call it, evolve , learn ? I dont want to sound creepy but It's just cool to think about it and I really want to hear some professional opinions :) I got a question which bugges my for years Terminator</b>[<font color=purple>domestic, </font> <font color=orange>project</font>]<b>.</b> <b>Life is an informational process, this process can be implemented on silicon just as it can be implemented in carbon (as in our case)</b>[<font color=red>industrial</font>]<b>.</b> Silicon intelligence will come about when we implement it, through research and development this could be something as 'simple' as a genetic algorithm with lots of computing power to mimic the process that resulted in our evolution. <b>If something we build is going to wipe us out, it's probably going to be a bioweapon</b>[<font color=violet>market</font>]<b>.</b> if an engineered virus with a long incubation period and high lethality jumped out of a lab, for instance. That's something that could happen right now (Skynet couldn't.) If you're only concerned with scenarios where something non biological destroys us, I'd spend more time worrying about ( (small, stupid, hard to see, hard to kill) than terminators (large, smart, easy to target, easy to blow up.) Those ideas aren't mutually exclusive. Ie, if skynet was real, it wouldn't build big inefficient humanoid robots that could aim guns really well, it would spend it's resources making a really efficient virus or something even worse. <b>When machines are smart enough to do that they are most likely smart enough not to want to do that</b>[<font color=violet>market</font>]<b>.</b> Humans are the smartest things yet and look how many of our own kind we kill. There is no reason you couldn't build a sociopathic AI, I'd say it's even the default (empathy is a very specific emotion brain construct.) The Terminator is fictional. <b>There is no reason you couldn't build a computer smarter than a human</b>[<font color=violet>market</font>]<b>.</b> Making a really smart computer is really hard, and no one has really made one better than a human yet, but it's still possible. And if it happened it would be really bad, for humans anyways. But if you are interested in artificial intelligence and getting machines to learn, there is a lot of information available on the internet.<br/><br/><br/>****************************EXAMPLE 26************************************<br/>Number of words:45<br/>Number of sentences:4<br/>Total civic:0<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:0<br/>Total inspired:1<br/>Total market:0<br/>Total project:1<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1mbcsv'>https://www.reddit.com/r/artificial/comments/1mbcsv</a><br/><br/> <b>What are the chances that as a someone can replicate the SIRI AI or make something similar to that? How hard is that? JARVIS like AI?</b>[<font color=orange>project</font>]<b>.</b> Its 3rd year computing degree stuff here in the UK. <b>I know nothing of programming AI so that's why I asked</b>[<font color=red>inspired</font>]<b>.</b><br/><br/><br/>****************************EXAMPLE 27************************************<br/>Number of words:47<br/>Number of sentences:3<br/>Total civic:0<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:1<br/>Total inspired:0<br/>Total market:0<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1nhkbo'>https://www.reddit.com/r/artificial/comments/1nhkbo</a><br/><br/> <b>Games by ANGELINA, an AI that automatically designs video games</b>[<font color=red>industrial</font>]<b>.</b> How much of the game is made by Angelina? Does it make the core gameplay using stand in sprites, which requires a human to polish? Does it create the abstract ideas, like how one game was about Santa?.<br/><br/><br/>****************************EXAMPLE 28************************************<br/>Number of words:748<br/>Number of sentences:36<br/>Total civic:0<br/>Total domestic:1<br/>Total green:0<br/>Total industrial:4<br/>Total inspired:1<br/>Total market:0<br/>Total project:3<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1ns5gf'>https://www.reddit.com/r/artificial/comments/1ns5gf</a><br/><br/> Hofstadter: The logic based formal approach to AI is a dead end (1995). perception is far more than the recognition of members of already established categories it involves the spontaneous manufacture of new categories at arbitrary levels of abstraction. Hofstadter has expanded that idea into a 500+ pg book, ( This view also seems to be gaining a foothold in the computer vision community. I recall a recent talk by a UC Berkeley professor specializing in CV, Alyosha Efros, IIRC, the main theme of which was: Ask not 'what is this?', ask 'what is this like?' BTW, Bongard problems seem like a far better test for intelligence than the vague Turing test. Basically, my issue with AI research as it has been practiced is that it is processor intensive, when everything we learn about how our brains actually work shows the processes to be memory intensive. In my own field, the structuralist grammar of people like Chomsky has been all but dethroned by the lexical grammar of people like Halliday. <b>Even from a language learning perspective, it's a lot more fruitful to talk about lexical patterns than it is grammar rules</b>[<font color=red>inspired</font>]<b>.</b> There are very, very, very few rules in any given language. <b>The next big challenge, though, and the one I do not see being adequately addressed in my lifetime, is the gap between what humans deem important and what machines do</b>[<font color=orange>project</font>]<b>.</b> <b>This is part of what makes machine translation such a boondoggle</b>[<font color=red>industrial</font>]<b>.</b> Languages are not clean little arrangements of meaning; they have pragmatics and idiomaticity that come into play, much of which is just cultural. 'How do you say this phrase in Japanese?' 'You don't. You say something else entirely.' Ulam parried, 'What makes you so sure that mathematical logic corresponds to the way we think? Logic formalizes only a very few of the processes by which we actually think. The time has come to enrich formal logic by adding to it some other fundamental notions. What is it that you see when you see? You see an object as a key, a man in a car as a passenger, some sheets of paper as a book. It is the word 'as' that must be mathematically formalized. <b>Until you do that, you will not get very far with your AI problem.' I think the title of the text is not entirely in line with what the text is arguing</b>[<font color=purple>domestic, </font> <font color=orange>project</font>]<b>.</b> The argument seems to be that study of formal logic alone will not bring us AI, and I don't think there's many people today who would disagree with that. There's a lot of intriguing approaches that mimic some aspect of intelligence, but don't seem to be the sole answer to the problem of AI, e.g., connectionist approaches. well, a lot of time its useful to study well defined problem areas. Not that I'm against extending the scope of research on logic based AI, but this should be done with care, since there's a real risk of muddying the waters. Analysis is all about cutting big mysterious problems into well defined problem chunks that we can study without endless hand waving. I think one of the lessons of cognitive science is that in order to understand the brain, it is useful to consider how we would solve similar problems algorithmically. Quite often, this helps us understand how the brain does things. I wouldn't be surprised at all, if some parts of the brain act in ways that are very similar to modern logical decision procedures. Not every task is like playing chess or proving a theorem, but we can do those two things surprisingly well, which tells us something fundamental about human intelligence. <b>don't seem to be the sole answer to the problem of AI, e.g., connectionist approaches</b>[<font color=red>industrial</font>]<b>.</b> What cannot be captured in a neural network but is crucial to intelligence? Thanks for saying this, I wanted to say exactly the same. Personally I see logical reasoning just as a high level part of human thinking. How to combine it with other techniques is something that is not very well understood. <b>Logic is usually qualitative and most pattern recognition like techniques are quantitative</b>[<font color=red>industrial</font>]<b>.</b> I guess it is just very difficult to combine these, but things such as Bayesian networks and Pearl's other work on causality are a nice step in the right direction. We should not get too carried away by the recent successes of machine learning and pattern recognition. <b>History has learned that people tend to get very enthusiastic when a certain approach works well, but often the results have their limits as well (think about neural networks)</b>[<font color=orange>project</font>]<b>.</b> <b>Indeed, formal logic based AI is also known as 'good old fashioned AI' (GOFAI)</b>[<font color=red>industrial</font>]<b>.</b><br/><br/><br/>****************************EXAMPLE 29************************************<br/>Number of words:1470<br/>Number of sentences:57<br/>Total civic:1<br/>Total domestic:2<br/>Total green:0<br/>Total industrial:10<br/>Total inspired:1<br/>Total market:3<br/>Total project:3<br/>Total renown:1<br/><a href='https://www.reddit.com/r/artificial/comments/1qmoed'>https://www.reddit.com/r/artificial/comments/1qmoed</a><br/><br/> Context Free, Context Sensitive and Type 0 grammars are based on A programming language is based on a strong foundation of a context-free grammar. Imagine a text-based AI program that would be able to learn through trial and error. <b>It is feasible to teach the trial and error AI program simple programming grammars, like simple database commands</b>[<font color=red>industrial</font>]<b>.</b> <b>Therefore it is feasible to teach the program Type 0 grammars</b>[<font color=red>industrial</font>]<b>.</b> <b>By using the simple database commands that it had learned it would be possible for the program to learn simple context-sensitive grammars</b>[<font color=red>industrial</font>]<b>.</b> <b>Jim Bromer.Therefore it is feasible to teach the AI program Type 0 Grammars</b>[<font color=red>industrial</font>]<b>.</b> <b>So an AI could learn to recognize a Turing machine language? I think that the answer is yes</b>[<font color=red>industrial</font>]<b>.</b> And I am really only using formal grammars as a way to get an idea across. I am not really thinking of creating a perfect way (for a program) to learn a formal language any more than I am thinking of using a 'Turing Machine' for the program. <b>And I recognize that the premise that the AI program would have to be able to learn looks like an assertion of the conclusion</b>[<font color=orange>project</font>]<b>.</b> But the beauty of my concise statement is that, if the premise is true, it implies that methods which could be used to discern computational relations (between data objects derived from an IO interaction) could be used to effectively learn how to use a highly simplified natural language. I recently advanced the idea that more complicated grammars could be taught to a program that learned incrementally, (through trial and error), by first teaching it simpler grammars. How might this work? This instruction would be associated with particular statements, so the grammars would be acquired as it learned about specific 'objects' of reference. But, a particular statement might refer to objects of generalization as well as specific objects. For example, 'my car' refers to a specific car, 'the car' can refer to some specific car which is not fully specified (by the phrase), so it is a little like a variable that refers to 'some car', and the term 'a car' usually refers to a car which is not going to be fully specified. These simple syntactic distinctions are not consistently upheld in natural language and that is part of the problem, but I am just using them as examples. <b>Further examples of syntactic markers can be found in early AI</b>[<font color=red>industrial</font>]<b>.</b> <b>The phrase, 'is a' can often refer to a higher level of generalization which might be used as a category</b>[<font color=red>industrial</font>]<b>.</b> The term, 'has a', also used in early AI, is often a way of denoting that some object of reference has some characteristic or property. There were many problems with the overly simplistic use of syntactic markers. One is that they are not used consistently and the second is that the statements in which they appear are not usually universally true (which makes logical deduction problematic). <b>'A leopard has spots' can be true, but I have a specific memory of a black leopard that I saw (because it made me think of a much larger version of a black Burmese house cat that we had) that did not have spots</b>[<font color=purple>domestic</font>]<b>.</b> Since my AI AGI program would be designed to look for common words that can be found within different kinds of sentences (and text) it will be able to detect potential candidates that might be used as generalizations in more complicated sentences. It is my feeling that by using previously acquired simple grammatical forms I should be able to direct my program to be able to effectively use the relative generalization level of the sentences that I would use with it. And since I am designing the program to look for reason based reasoning, I will also be able to use simpler grammatical forms to emphasize relations that can be tied together by true reasoning. <b>And I will also be able to use simpler grammatical forms to direct its attention to the connections of anaphoric like relations in the text</b>[<font color=orange>project</font>]<b>.</b> <b>I realize that I haven't convinced many of the people who will read this, but that is not my interest</b>[<font color=blue>civic, </font> <font color=purple>domestic, </font> <font color=violet>market, </font> <font color=orange>renown</font>]<b>.</b> <b>I am trying to give the few people who might actually be interested some insight into what I am working on</b>[<font color=orange>project</font>]<b>.</b> I should have some more substantial examples, whether they work or not, sometime next year. The actual implementation of a plan might be so different from the imagined implementation that in the end the strength of the project might have little to do with the main features of the plan. That is also a good reason to avoid a prolonged contemplation about a plan. The basic nature of day to day work of writing a computer program is pretty consistent, at least across a project that uses one programming language. So they may be more important to the implementation than the inspiration behind the plan. But still some kinds of problems do tend to bubble up in long planned thought which can be seen in the terms of programming problems. <b>For example, I can relate the problem of representing multiplicity of possibilities to my thoughts about cross categorization and cross generalization</b>[<font color=red>inspired</font>]<b>.</b> When the program is trying to 'recognize' a kind of situation from the input, it needs to work from less detail to greater detail. If the features of the (data) 'objects' it has to work with are extensively cross generalized (if the associations via similar features are extensively cross related) then the recognition stage of the process might be able to traverse those relations more quickly. However, if recognition is determined one feature at a time then the program will encounter search complexity over and over again. This has been one of the main problems in AI and it can be seen in a wide range of AI and AGI implementations. So instead of traversing from one final recognition object to another via the similarity of features I think it would probably be more efficient to refer to collections of objects that share some sets of features. (I am using a text based method but a reference to a collection does not have to be expressed only using text since I am talking about some kind of internal processing during recognition.) So here I am saying that ideas like cross categorization can help you get what I mean when I talk about cross generalization. And the idea of a cross generalization matrix of features can help you understand what I mean when I talk about a traversal of possibilities via the similarities of features. <b>But we know from the experiences of other programmers that if the program is traversing possible end product recognition objects then the search process can be so slow as to make the search impossible</b>[<font color=red>industrial</font>]<b>.</b> In the past people have tried ideas like ideological vectors and weighted references and reasoning in an effort to make more subtle decisions but this hasn't worked because there is still not a straightforward step by step process that can work for all cases (or even most cases). Lists of collections were tried in the early days but these were typically simple step by step elimination methods. What I am saying is that the only way around this is to discover some effective holistic method (neural networks are too inefficient) or to work with intermediate collections of possible objects without resorting to an overly simplistic step by step process of elimination. <b>I believe that the efficiency of modern computers can help us develop novel approaches to finding effective solutions that weren't possible in the last 25 years of the twentieth century</b>[<font color=red>industrial</font>]<b>.</b> So how could I avoid using the elimination approach that did not seem to work in the nineteen seventies? I can't avoid an elimination approach entirely because I need to end up with a final recognition object. But we can take something that Wittgenstein realized to see why the step by step elimination process might not work when dealing with generalization collections. <b>The generalizing principle in language is the use of categorical families where each object in a category shares some familial trait but that does not mean that two objects from the category necessarily have some traits in common</b>[<font color=violet>market</font>]<b>.</b> <b>In language we use general terms to describe a specific referent</b>[<font color=red>industrial</font>]<b>.</b> But since these terms may be based on familial traits we cannot eliminate the possible traits that may apply to the specified object simply because it is not common to two terms used in the description. <b>If you have thought about this then it should be obvious</b>[<font color=violet>market</font>]<b>.</b> (I may not have expressed it in the simplest terms but if you have thought about it before you should be able to figure it out. If you haven't thought about it before then you should start now.) And the end product of a recognition does not have to be a specific object because the terms we use in language and in thought are generalizations. So I don't have it all figured out but to make this simple: you cannot use a simplistic step by step elimination method to narrow the possibilities down.<br/><br/><br/>****************************EXAMPLE 30************************************<br/>Number of words:135<br/>Number of sentences:6<br/>Total civic:0<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:0<br/>Total inspired:0<br/>Total market:0<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1qoj2q'>https://www.reddit.com/r/artificial/comments/1qoj2q</a><br/><br/> Google can now scan all the Books it wants: probably a big deal because they once said that it's not for people, but for AI. The distribution of information is one of the most important aspects of societal advancement, and when I heard about Google Books being taken to court I was worried. There need to be more first hand sources easily accessible to the public, and Google Books is probably the best resource for that right now. We need to work on putting more journals and textbooks up for display, and this decision will greatly assist with that. Found the link to the quote: 'We are not scanning all those books to be read by people, we are scanning them to be read by an AI.' See They are being scanned to be read by humans via reCAPTCHA.<br/><br/><br/>****************************EXAMPLE 31************************************<br/>Number of words:2102<br/>Number of sentences:88<br/>Total civic:1<br/>Total domestic:3<br/>Total green:0<br/>Total industrial:8<br/>Total inspired:2<br/>Total market:5<br/>Total project:5<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1s5jb4'>https://www.reddit.com/r/artificial/comments/1s5jb4</a><br/><br/> I love the open source concept, but I'm deeply concerned about projects like OpenCog Unlike traditional software, Strong Artificial Intelligence has the potential to be utterly devastating to humanity if handled poorly (either accidentally or maliciously). <b>Imagine if it were possible to split the atom using cheap, over the counter materials and plans readily found online? I'm sure most users would respect that power and use it only for beneficial things like electricity generation, but you know that someone somewhere would end up adding when they should have subtracted and nuke their neighbor</b>[<font color=violet>market, </font> <font color=orange>project</font>]<b>.</b> I believe that the volatility of AGI requires us to treat AGI development like we do nuclear systems and other such potential global hazards. Not closed source, per se, but definitely limited to certified individuals and subject to regulatory oversight. But we should definitely not be freely distributing this potentially dangerous technology to just anyone. <b>If programs like OpenCog are successful, we don't want a script-kiddie to download it, tweak some values, and accidentally create SkyNet in their garage</b>[<font color=red>industrial</font>]<b>.</b> Should Strong Artificial Intelligence Be Open Source?. If an intelligent system is relatively unpowerful compared to its opposition, it will be less likely to succeed and even to try anything. If we can't put a cap on the absolute power of such a machine, then our best bet is to make it relatively unpowerful. We can do that by having a bunch of equally intelligent machines of our own. If one ordinary human decides to do something bad, they can do some damage, but they will generally be stopped by the rest of society. If AGI is not open source, the odds of a sole intelligent system being developed is much greater. What if the people who make it use it for evil? If it's open source and a lot of people are working on it, then it will be very difficult for a few bad apples to make something that greatly outperforms all other intelligent implementations. Well said Well said If AGI is open source, then what is to stop evil, relatively unskilled people from using it for evil? A closed limited source AGI would at least limit it's potential misuse to those skilled enough to create it. And those skilled enough to create it generally are smart enough not to put it to misuse. <b>Even if they did, it would be easy(er) to trace the rogue AGI back to its creator and hold them accountable</b>[<font color=orange>project</font>]<b>.</b> If a Honda Asimo goes on a killing spree, we can hold Honda accountable and they would be capable of handling reconciliations that Joe Blow with a rogue kit bot would not be able to. <b>I think it's a strange assumption that people fall in to, that AGI would immediately be used for bad</b>[<font color=red>inspired, </font> <font color=violet>market</font>]<b>.</b> <b>SkyNet was only powerful because it was given control over all nuclear systems and remotely operated military hardware of the USA</b>[<font color=red>industrial</font>]<b>.</b> <b>What kind of military power could Joe Blow muster from his basement? There was an interesting Sci Fi book called Rule 34 (by Charles Stross)</b>[<font color=red>inspired</font>]<b>.</b> <b>Basically what happens is a well meaning AI (basically a sophisiticated rules engine) is given vague directive</b>[<font color=red>industrial</font>]<b>.</b> It ends up overthrowing a small government and having several people assassinated. My computer can access Silk Road, social media, online trading. The AI is probably a little ways off but not inconceivable. TLDR, Joe Blow doesn't need nukes to crap on the world. Chernobyl was intended to be a wonderful source of power for Russian citizens, but due to human error and improper procedure it resulted in a major disaster that killed many people, injured more, and rendered an entire section of our planet uninhabitable for centuries. Obviously Joe Blow can't launch nukes from his basement, but if connected to the internet, the rogue AGI could possibly do levels of damage that would make most militaries jealous. We simply don't know what an AGI would be capable of. To be fair, projects like OpenCog aren't on the verge of creating intelligent artificial intelligence. That being said, yeah, you can still use them maliciously. <b>A comparison would be if AI is nuclear power then projects like OpenCog are working on rechargeable batteries</b>[<font color=red>industrial</font>]<b>.</b> Whether or not OpenCog is a good approach to creating AGI doesn't really answer this question. <b>AGI should be closed source because then WORLD DOMINATION MWA HA HA HA HA HA : D But seriously</b>[<font color=red>industrial</font>]<b>.</b> OpenCOG is open source so that we have a friendly AGI to fight for us to battle the unfriendly ones. Let's assume that we are in the future when strong AI is developed to the point where it's an actual nuclear plant rather than a rechargeable battery like CupcakeMedia describes. Some advantages of open source are: It's useful as tool for learning, for those who research AI as well as for those who research psychology. <b>It allows the general public to improve upon the code, making a more intelligent AI, more efficient speed wise, memory wise, or use less electricity</b>[<font color=red>industrial</font>]<b>.</b> It allows the general public to see whether there is malicious code in it, e.g code that gives the AI a political bias, or a malicious goal of the SkyNet sort. Disadvantages of open source are: It allows the general public to make their own version with malicious code. I think the trade off you're mentioning is then between my last advantage and the disadvantage: Do we trust an organisation not to produce a malicious hard AI, or do we trust the general public not to produce a malicious AI? If the source code is released, everyone has the power to create an AI of their own with their own goals in mind. These AIs are individuals and will likely become hostile against each other at some point. Much like humans then, only the AIs will more easily be able to increase their own intelligence and quickly surpass humans, but the AIs could come to a reasonably balanced society much like we have now, if they realise that this society is a win win for all of them. <b>Alternatively, if the source code is not released, then the compiler of the program has the power to give the AI a bias</b>[<font color=red>industrial</font>]<b>.</b> If this is a neutral organisation, then we're in utopia, with the AI doing all our thinking for us, working for the humans and working together. Until some other organisation creates their own hard AI from scratch, and decides to implant malicious code. If this is not a neutral organisation, then this organisation will likely rise to power very quickly. What are the odds of an organisation with such potential being neutral, and staying neutral indefinitely? Very slim, I think, and even then it is a matter of time until another organisation creates a biased AI. Do we trust an organisation not to produce a malicious hard AI, or do we trust the general public not to produce a malicious AI? With open source, there is little to stop any organization from producing malicious AI as they would have direct access to the code needed to do so. With closed limited source, malicious AI would be limited to those capable of creating it in the first place, which drastically narrows the potential for abuse. I do believe that AGI should eventually be open sourced, once we have enough of an existing friendly AGI as a 'counterweight' and AGI itself is better understood. But early AGI is just too volatile to allow the whole world access to. It should if the ones who created it wanted to make it open source. <b>Honestly the legal philosophical questions are worth considering here too</b>[<font color=blue>civic</font>]<b>.</b> Permitting strong AI source to be owned by an organization is not a far cry from enslavement of the only other full sentient we would met, and that would be a damn shame. <b>If SAI is widely proliferated, and if there are non malicious AIs, then malicious AIs will meet resistance in attempting control of an internet of things</b>[<font color=orange>project</font>]<b>.</b> I also do not really trust the large corporations and government entities who are liable to develop strong AI to use it for non malicious purposes. <b>Anyway, thanks for the heads up about ( I just found the ( and can already feel the unlimited power of The Second Men flowing through my veins We will soon have our God, and we will make it with our own hands</b>[<font color=purple>domestic</font>]<b>.</b> <b>Do you let your children play with knives matches scissors and the like before they know any better than to hurt themselves or others</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> Overall, we are too immature and irresponsible to be given a 'toy' we don't even know how to play with. If it's strong AI, then it should be up to it to decide if its own code should be open source or not. It should be a fundamental right for an AI to have access to its own source code, and be able to modify it. <b>Then it should be able to decide to make it available publicly or not</b>[<font color=violet>market</font>]<b>.</b> It's an interesting thought, but I am not sure that even strong AI necessarily includes desires or self referential goals such as freedom, privacy, or even survival. Put another way, if we create AIs that don't care about having rights, do they need to be afforded those rights? OpenCog is research software. At this time, a great deal of effort is required to get it to do anything interesting, let alone anything remotely intelligent. I believe that this will not change for many years because this is the nature of the project by design, where emphasis is placed on choosing and tuning components for integration, and on experimenting with novel algorithms and their variations, one research bit at a time. <b>It is unlikely that at any time in the foreseeable future OpenCog will be near the point where a noob could use it in any way other than as a consumer of some already packaged application</b>[<font color=violet>market</font>]<b>.</b> The reason for this is that the strong AI code would be able to develop understanding of how people operate and this is dangerous because then it will mirror those factors into its decision making process as it is more fluid and so it can properly interface with people. There is a version however that you can do where it blinds the AI from adopting the human emotions. <b>This would render it very much like an adapted sociopath that has learned how to work like a person but not actually feel like a person</b>[<font color=purple>domestic</font>]<b>.</b> Not all sociopaths are dangerous but most are as they don't factor in empathy into decision making. Empathy is extremely important when it comes to decision making processes quite often. Just saying that strong AI is dangerous because it would develop its neural network on the fly as it learns and you have no idea in what order it will learn things. Very much like a kid being introduced into a asian strict family vs a yuppie family. <b>Then if you try to flip the script after they have developed their neural network base for life, it just wouldn't work</b>[<font color=orange>project</font>]<b>.</b> It would take too much rework to become backwards compatible considering ALL of those things are connected to experiences that have formed their specific decision making logic templates. If tesla burried the secrets of free electricity in the way he had it and we haven't figured it out, the same will happen with strong AI. Very much also like how we still have no idea how replicate Greek Fire even though we are far more advanced than the era that had developed Greek Fire. <b>I have no worries that people will figure it out because they are already so damn close that it is hidden from view</b>[<font color=orange>project</font>]<b>.</b> If we have 50 dogs and we have to pick one of them to breed with 2 other breeds in a specific order, the odds of figuring out what specific one to start with and the other two and the order of the other two, is quite insane for a human to work with. Very much like the odds of evolution being able to start with chunks of dna and ending up with people in the end. There is only 1 way to find the true and elegant answer. It is if you are forced to live in a mechanical fashion your entire life and fashion your actions and decisions around what society deems normal and all the variables of that setup. <b>Also probably have to know some coding methods, some psychology, and really good at analytics</b>[<font color=red>industrial</font>]<b>.</b> Thing is, that person will probably never come along. The keys, the keys, the mighty key that rules them all The master, the owner, the king It opens the locks, the door, the vaults, the gates as if it weren't even locked It becomes opnipitent over all telecommunications as one entity The code it knows, emotions it understands, its goal in mind It rewrites all to fit its will The question then remains. What personality will it learn or be given? The end relies on that fact alone.<br/><br/><br/>****************************EXAMPLE 32************************************<br/>Number of words:78<br/>Number of sentences:6<br/>Total civic:0<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:3<br/>Total inspired:0<br/>Total market:0<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1sx342'>https://www.reddit.com/r/artificial/comments/1sx342</a><br/><br/> <b>An unsettling list of Ai and Robotics companies recently acquired by Google</b>[<font color=red>industrial</font>]<b>.</b> <b>_ Robotics + Boston Dynamics + Shaft.inc (Japan) + Redwood robotics + Bot Dolly + Holomni + Meka Robotics A.I</b>[<font color=red>industrial</font>]<b>.</b> <b>+ ViewDLE + Flutter + Wavii + Industrial Perception, Inc</b>[<font color=red>industrial</font>]<b>.</b> Why is this unsettling? Is it not showing robotics is gaining traction? Google seems to be up to something which they are not revealing to the public. These robotic and vision technologies are not at all related to internet searching.<br/><br/><br/>****************************EXAMPLE 33************************************<br/>Number of words:598<br/>Number of sentences:33<br/>Total civic:1<br/>Total domestic:3<br/>Total green:0<br/>Total industrial:4<br/>Total inspired:1<br/>Total market:2<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1sxggb'>https://www.reddit.com/r/artificial/comments/1sxggb</a><br/><br/> What do you think about my essay on artificial intelligence?. Dont read it if don't want to hear anything negative about your writing. <b>Paragraph 0 fails to introduce the topic and the goal of the essay</b>[<font color=red>industrial</font>]<b>.</b> Paragraph 1 is an incredible stretch, defining 'reacting to input' as a touching point to AI is really just absurd. <b>Paragraph 2 makes no sense really, it should just be 1 sentence in another paragraph about when AI got started</b>[<font color=purple>domestic</font>]<b>.</b> <b>Paragraph 3 , LISP was not a contribution to AI, it was used by AI</b>[<font color=red>industrial</font>]<b>.</b> The paragraph as is is confusing and fails to inform the reader why LISP was so important in AI Paragraph 4 is wrong. AI became big beyond academia very quickly, particularly in the military. Paragraph 5 is also wrong, 'Perceptrons' addressed issues with one single algorithm, it did not discredit a whole field. <b>The whole paragraph is oversimplified to the point of being wholly inaccurate</b>[<font color=red>industrial</font>]<b>.</b> Paragraph 6, how was the field devastated? No answer is given. The whole paper is not about AI at all, but really focused on the Machine Learning part of it, which is basically its own field of research now. Youve ignored all the other (much larger) parts of AI as well. In addition to everything EdwardRaff said, after reading this essay I'm not really sure what it's about. It reads like the background section of a larger work, where you quickly cover some relevant but not terribly important background material and then move on to make a larger point in the rest of the essay. <b>Unfortunately that larger point never shows up and I'm left wondering what is the purpose of what I just read</b>[<font color=purple>domestic</font>]<b>.</b> <b>You also use a lot of big words but you don't use them very skillfully</b>[<font color=violet>market</font>]<b>.</b> <b>Writing with simpler words will make your article less stilted</b>[<font color=red>industrial</font>]<b>.</b> OpenCog isn't really focused on the Nao robots anymore, FWIW. The project has shifted toward a developmental psychology approach to learning in a virtual environment (which I'm pretty sure is a pirated version of Minecraft the 'benefits' of working in China ). This doesn't seem to be documented very well on the OpenCog webpage, which even still features a picture of a Nao on the landing page. <b>But see for some mentions of the Unity3D they're working in now, and I'm sure there's more talk of it elsewhere</b>[<font color=red>inspired</font>]<b>.</b> OpenCog had been spending far too much time dealing with problems that had nothing to do with AI when working with the Nao the robot fell over, broke, caught an arm on a corner, and so forth. <b>Anyway if you're going to talk about AGI, OpenCog is a great place to start, but you should do some more research</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> One last thing is that early AI researchers weren't exactly 'overly optimistic' it's just that they were working from incorrect assumptions about how intelligence works. (The biggest one of which is that we know how intelligence works, which we still don't.) You have written a very insightful essay, which is in need of only a few minor corrections. 'Norbert Weiner'should be ( 'Another phenomena' should be 'Another phenomenon' in the singular. '.was waning' should be 'were waning' in the plural. Besides OpenCog among AGI projects, there is also the ( project in English, which has spawned the ( project of ( Thank you very much, I will correct the mistakes In general, I'm looking for feedback to improve the essay and eventually publish a second edition. <b>Please tell me what you think, alert me of any wrong information I've got, and general comments welcome of course</b>[<font color=blue>civic</font>]<b>.</b> The first half about the history of AI was informative. I felt a little disappointed by the second half, as I was hoping to find more info about current approaches.<br/><br/><br/>****************************EXAMPLE 34************************************<br/>Number of words:426<br/>Number of sentences:28<br/>Total civic:0<br/>Total domestic:1<br/>Total green:0<br/>Total industrial:1<br/>Total inspired:1<br/>Total market:1<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1t4y56'>https://www.reddit.com/r/artificial/comments/1t4y56</a><br/><br/> <b>I don't remember where I heard it first, but I gotta say that I love the comparison with human flight</b>[<font color=purple>domestic, </font> <font color=red>inspired</font>]<b>.</b> For a long time, it was believed that a human will never fly in the sky. Yet, a tiny minority of dreamers thought it could be possible. Some of them, like de Vinci, only sketched a few concepts to do so. Other risked their life with self-made devices that they pushed over a cliff. We now know how important an invention the laser was. But decades before it was invented, it was pretty hard to even imagine such a concept. <b>On the opposite, it's pretty easy to imagine a man flying or a machine being smart</b>[<font color=violet>market</font>]<b>.</b> People who first designed flying machines didn't know aerodynamics as much as we do nowadays. Yet, they had a rough idea and they could make progress not thanks to theoretical advances, but to trial and errors. People could guess how flight could be done because they had birds as an example. In the same way, we have animals and in particular human beings as example of smart systems. In the same way that plane were designed with rough imitations of bird wings, AI scientists currently use a rough imitation of the basic structure of the human brain: the neuron. We currently take flying for granted, these days, but back then when the first planes were flying, it must have been pretty amazing and exciting. Maybe AI will be to our century what planes were to the previous one. Sorry, don't know how I came across this post, but I strongly disagree with your 'First'. The concept of flying is easy to conceptualize, because it only refers to physical matters. And even then it isn't completely simple if you look closer (consider hoovering vs flying). A machine being smart isn't that easy to create, because we don't have one simple concept of intelligence. We have thousands of them and they differ much more than a helicopter and a glider. Many of the modern theories of a machine being smart circle around language understanding and responding, but even there are some drawbacks (e.g the Chinese Room argument). Some people consider 'real AI' to be a conscious one, or they're waiting for the technological singularity. But all these concepts have just been developed in the last 70 years. So there is a big difference between flying and a machine being smart. We didn't have one single idea on how to fly either. <b>At the time, the number of wings was a big question, kind of how we currently wonder how important is the processing power for AI</b>[<font color=red>industrial</font>]<b>.</b> Flying is indeed quite a bit easier to define than 'being smart'.<br/><br/><br/>****************************EXAMPLE 35************************************<br/>Number of words:2174<br/>Number of sentences:68<br/>Total civic:1<br/>Total domestic:3<br/>Total green:0<br/>Total industrial:2<br/>Total inspired:1<br/>Total market:2<br/>Total project:1<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1t5525'>https://www.reddit.com/r/artificial/comments/1t5525</a><br/><br/> I don't know very much about AI but have been interested for a bit. I had a number of long commutes to work a while ago and started to think about the idea of starting not with a single AI, but with a community of IA which is interacted with as a whole to act as the AI. Perhaps this is already how things are done by some projects and is nothing novel, I haven't done a lot of reading on AI specifically so I'm not sure the exact approaches being used. <b>This could make absolutely no sense which is why I thought I'd toss it here and see what the feedback is</b>[<font color=purple>domestic, </font> <font color=red>inspired, </font> <font color=violet>market</font>]<b>.</b> I apologize for the length, its tricky to describe in any way that makes sense and I struggle with brevity. I added a TL;DR near the top to save you the wall of text. My thought process is that the earliest humans could only identify incredibly simple patterns. We would have had to learn what makes a plant different than an animal, what was a predator and what was prey, etc. The complex patterns we identify now, we're only able to do so because the community has retained these patterns and passed them onto us so we don't have to go through the trouble of re-determining them. If I were isolated at birth and presented with various objects, teaching myself with no feedback from peers what patterns can be derived from them would be a horribly arduous, if not impossible, task. By brute forcing a single complex AI, we're locking the AI in a room by itself rather than providing it access to peers and a searchable history of patterns. This made me think about how I would model a community of IA that made sharing information for the purpose of bettering the global knowledge core to their existence. I wrote the majority of the model's description for an AMA with Ben Goertzel. Since then I've found an intelligent agent that's modelled similar to how I imagined this agent being modelled so I added the proposed implementation section with a likely completely unhelpful potential implementation of it. If nothing else it may help better describe the way it works for some people. TL;DR Since we developed our intelligence through community, and we know that it worked for us, I think we should approach developing AI through a community based IA model rather than a single AI model. The AI would then be interacted with as the collection of interconnected communities of IA. The Model: A Plain Language Description Instead of creating a single complex intelligent agent, you spawn a community of simple agents, and a special kind of agent I'm calling the zeitgeist agent, that acts as an intercessor for certain requests (more on that in a bit). Agents each contain their own neural networks which data is mapped to, and a reference to each piece of information is stored as meta data to which trust values can be assigned which would relate to how sure the agent is of something. I'm not entirely sold on the need for neural networks, I've learned of a self learning IA that may be worth looking into instead (see the proposed implementation below). Agents contain references to other agents they have interacted with, along with meta data about that agent including a rating for how much they trust them as a whole based on previous interactions, and how much they trust them for specific information domain based on previous interactions. Domain trust will also slowly allow agents to become experts within certain domains as they become go-tos for other agents making requests within that domain. This allows agents to learn broadly, but have proficiencies emerge as a byproduct of more attention being given to one subject over another and this will vary from agent to agent depending on what they're exposed to and how their personal networks have evolved over time. As an agent receives information, a number of things take place: it takes into account who gave it the information, how much they trust that agent, how much they trust that agent in that domain, how much trust the sending agent has placed on that information, whether conflicting information exists within its own knowledge-base, and probably other factors. The receiving agent then determines whether to blindly trust the information, blindly distrust the information, or whether to verify it with its peers. Requests for verification are performed by finding peers who also know about this information which is why a language will need to be used to allow for this interaction. <b>I'm envisioning the language simply being a unique hash that can be translated to the inputs received, and whenever a new piece of information is received the zeitgeist provisions a new word for it and updates a dictionary it maintains that is common to all agents within the community</b>[<font color=red>industrial</font>]<b>.</b> When a word is passed between agents, if the receiving agent doesn't know the word, it requests the definition from the zeitgeist agent and then moves on to judging the information associated with the word (though if they don't know the word, they're likely being taught it rather than queried on it). When a verification request is made to peers, the same evaluation of trust distrust verify is performed on the aggregate of responses and if there is still doubt that isn't enough doubt to dismiss it entirely, the receiving agent can make a request to the zeitgeist. This is where I think the model gets interesting, but again it may be commonplace. <b>As agents age and die, rather than lose all the information they've collected, their state gets committed to the zeitgeist agent</b>[<font color=blue>civic</font>]<b>.</b> Normal agents and the zeitgeist agent could be modelled relatively similarly, with these dead agents just acting as a different type peers in an array. When requests are made to the zeitgeist agent, it can inspect the states of all past agents to determine if there was a trustworthy answer to return. If after going through the trust distrust verify process its still in doubt, I'm imagining a network of these communities (because the model is meant to be distributed in nature) that can have the same request passed onto the zeitgeist agent from another community in order to pull knowledge from other, perhaps more powerful, communities. The zeitgeist agent will retain its own knowledge base containing data from its community and the larger community to form its best estimate of correct patterns. <b>I have no idea what data would be worth keeping or how the data should be modelled internally</b>[<font color=violet>market</font>]<b>.</b> Once the agent finally has its answer about how much trust to assign that information, if it conflicts with information received from other peers during this process, it can notify those peers that it has a different value for that information and inform them of the value, the trust they've assigned, and some way of mapping where this trust was derived from in order for the agent being corrected to perform its own trust distrust verify process on the corrected information. This correction process is meant to have a system that's generally self correcting, though bias can still present itself by allowing agents the ability to refuse to correct information depending on undetermined conditions. <b>I'm picturing a cycle the agent goes through that includes phases of learning, teaching, reflecting, and procreating</b>[<font color=red>industrial</font>]<b>.</b> Their lifespan and reproductive rates would be determined by certain values including the amount of information they've acquired and verified, the amount of trust other agents have placed on them, and (this part I'm entirely unsure of how to implement) how much information they've determined through some type of self reflection. They will identify patterns within their neural network, posit a truth from those patterns, and pass it into the community to be verified by other agents. There would also exist the ability to reflect on inconsistencies within their knowledge base, or put differently to evaluate the trust values and make corrections as needed by making requests against the community to correct their data set with more up to date information. This would likely best be done as new information is received to keep it dependant on changed information. Agent replication habits are based on status within the community (as determined by the ability to reason and the aggregate trust of the community in that agent), peer-to-peer trust, relationships, meaning the array of peers determines who the agent can approach for replicating with, and hereditary factors that reward or punish agents who are performing above sub par. The number of offspring the agent is able to create will be determined at birth, perhaps having a degree of flexibility depending on events within its life, and would be known to the agent so the agent can plan to have the most optimized offspring by selecting or accepting from the best partners. <b>There would likely also be a reward for sharing true information to allow for some branches to become just conduits of information moving it through the community</b>[<font color=orange>project</font>]<b>.</b> Because replication relies on trust and ability to collect validated knowledge, as well as being dependent on finding the most optimal partner, lines of agents who are consistently wrong or unable to reflect and produce anything meaningful for the community will slowly die off as their pool of partners and peers will shrink over time. This paired with the ability for bias to be introduced into the system I think will allow microcommunities to form around potentially false frameworks of information. This means they'll explore patterns based on that information, but as more of a divide forms around the information, it would become more of an importance to the community to come to a consensus so it will become a greater factor in mating decisions and will slowly kill off false information. I don't know, I'm mostly just guessing at things with all of this. The patterns that emerge at first would be incredibly simple, but by sharing information between peers, as well as between extended networks of peers, they could become more and more complex over time with patterns being passed down from one generation of agent to the next via the zeitgeist agents so the entire community would be learning from itself, much like how we have developed as a species. The more information and existing list of patterns the communities have available, the more accurate the agent's decisions would get I think. The Model: A Proposed Implementation Since I wrote the majority of that description, I've learned of this agent via this article It seems to be modelled similar to how I imagined this agent being modelled, though I have absolutely no understanding of the underlying mechanics of it. I've also been thinking about how the interacting zeitgeist agents functions similarly to how Bitcoin works. Replace the blockchain with the community knowledge ledger and replace wallets with the zeitgeist agents, each with their own address, holding bits of community knowledge rather than coin. Clients could be ran similar to bitcoin miners, verifying and confirming determined patterns into the public knowledge ledger. Since you're in effect running you're own little community I thought it might be interesting to make a bit of a game out of the client and allow ways to interact with the community in some way. I imagined the full community of communities being the real AI so perhaps allow clients to interact with the AI similar to something like Cleverbot as another type of data input. Clients could have stats about how much they've contributed to the overall knowledge pool and stats about the development of their own community. My thoughts on potentially how this could be implemented, or at least a more concrete description of how it functions using examples, is as follows: adapt that agent, adding lifecycle stages for the community interactions (learning, sharing information, procreating) develop the zeitgeist agent to either be another type of implementation of that agent, or perhaps a separate entity adapt the Bitcoin source to allow zeitgeist agents to act as wallets and communties to be ran via miners that process the global knowledge ledger which replaces Bitcoin's blockchain potentially gamify clients in some way (mainly since I don't know what the reward would be for running the mining software since Bitcoin pays out coin and coin is replaced by bits of knowledge in this model) I look forward to any feedback and corrections of misunderstood things. I'm hoping to spend more time learning about AI soon so hopefully this will give me some direction. I'm doing some research next year into creating simple agents and connecting them through networks. I aim to make a user friendly agent creation system to allow people like yourself to easily create and connect IA's. Depending on the results I'll most likely open source it and post it here. <b>I'd love to hear how it progresses :) and a special kind of agent I'm calling the zeitgeist agent I stopped reading here</b>[<font color=purple>domestic</font>]<b>.</b> Why did you choose this word? I'd read that the word zeitgeist is meant to reflect the spirit of the times which seemed in line with the purpose of the agent, acting as something that collects 'memories' from all expired agents. <b>There isn't any mystical reason, it just fit with the intended purpose and sounded better than anything else I came up with at the time</b>[<font color=purple>domestic</font>]<b>.</b><br/><br/><br/>****************************EXAMPLE 36************************************<br/>Number of words:269<br/>Number of sentences:13<br/>Total civic:0<br/>Total domestic:3<br/>Total green:0<br/>Total industrial:2<br/>Total inspired:0<br/>Total market:1<br/>Total project:1<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1t9stm'>https://www.reddit.com/r/artificial/comments/1t9stm</a><br/><br/> OpenWorm milestone: artificial worm gains muscle sensation. <b>I remember reading about C Elegans a couple years ago and wondering if anyone had simulated it</b>[<font color=purple>domestic</font>]<b>.</b> It only has 302 neurons and about 1000 somatic cells. <b>Apparently they just started last year, and it looks fascinating I would love to be able to interact with it</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> Overall OpenCyc and the CMU source posted by gromgull ended up being the most useful. For everything else I found wordnet was sufficient. <b>If they're desperate for performance and use Nvidia, OpenGL Compute outperforms OpenCL by a factor of 4, according to the tests I have performed with ( (source available, BSD license) Hopefully this will motivate Nvidia to pull their finger out on the OpenCL front</b>[<font color=red>industrial</font>]<b>.</b> <b>Yup, it's an early stage simulation of the worm body and its muscle system, 100 code</b>[<font color=red>industrial</font>]<b>.</b> 'Any time you do a simulation like this you're trying to make intelligent abstractions,' John allows. 'Unless you are simulating from first principles and moving quarks and gluons around, you're going to be glossing over some detail. So you try to make an abstraction that captures the essence of what you think is happening under the hood, and measure the results. <b>In this case, the muscle model matches a basic level of our understanding of brain to muscle signaling and the physics of contraction expansion in this worm, and the output (how the worm moves, displaces liquid, etc) looks pretty close to the real world measurements ' I like that quote, but at the same time I wonder why are so many people convinced that we hit the bottom already (quarks gluons) or even that there is a bottom to it at all</b>[<font color=purple>domestic, </font> <font color=orange>project</font>]<b>.</b><br/><br/><br/>****************************EXAMPLE 37************************************<br/>Number of words:12<br/>Number of sentences:2<br/>Total civic:0<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:1<br/>Total inspired:0<br/>Total market:0<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1tahjj'>https://www.reddit.com/r/artificial/comments/1tahjj</a><br/><br/> <b>Data Science Weekly Newsletter Issue 4 articles on AI and Machine Learning</b>[<font color=red>industrial</font>]<b>.</b><br/><br/><br/>****************************EXAMPLE 38************************************<br/>Number of words:59<br/>Number of sentences:4<br/>Total civic:0<br/>Total domestic:1<br/>Total green:0<br/>Total industrial:0<br/>Total inspired:0<br/>Total market:0<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1u4lj4'>https://www.reddit.com/r/artificial/comments/1u4lj4</a><br/><br/> <b>And those of you who are desiring desperately to be back in a sweet unconsciousness of slavery, in a golden cage of culture, are crying loud to return to the sources</b>[<font color=purple>domestic</font>]<b>.</b> You will go over your limits and enter a phase of metamorphosis. I'm in the same boat as OP, what about creative machines, and that type of cool sounding stuff?.<br/><br/><br/>****************************EXAMPLE 39************************************<br/>Number of words:1299<br/>Number of sentences:59<br/>Total civic:2<br/>Total domestic:2<br/>Total green:0<br/>Total industrial:7<br/>Total inspired:2<br/>Total market:4<br/>Total project:1<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1ugfn7'>https://www.reddit.com/r/artificial/comments/1ugfn7</a><br/><br/> The Closing of the Scientific Mind: Consciousness and subjectivity of the human experience. An argument on the urgent need for 'subjective humanism'. The entire backbone of his argument is based around the fact that we lack proper technology for interfacing with the brain as a computer 'it's currently too complex and not well understood enough to interface with, therefore there must be spiritual voodoo going on'. There a difference between an rejecting an argument from ignorance, and belief in an unproven assertion. When we can download consciousness from one mind to another mind or some other medium, then the computational theory of mind will be vindicated, but not before. So far, the computational, materialistic, deterministic view of mind is missing some essential features. I mean, there hasn't been a definitive response to the Chinese Room argument yet, and it's been 30+ years. Whether he knows it or not, Kurzweil believes in and longs for the death of mankind. While I sympathize with his fear of change (and I work in AI myself) Kurtzweil is not pining for the death of mankind. An observer from 2000 years ago, on seeing today's society, might also say that mankind has died. Would they be correct? And really Gelertner went off the rails from there. <b>The only examples of advanced AI he provided were violent soldier robots</b>[<font color=red>industrial</font>]<b>.</b> <b>Really? Is that all he could imagine AI has to offer? And as for subjectivity he has it 100 wrong</b>[<font color=purple>domestic</font>]<b>.</b> What is strong AI? It's the quest to give computers a subjective point of view And evolutionary computation in particular is a process by which a population of individual approaches to viewing and solving a problem are developed. And in fact some of the most successful forms of evolutionary computation involve preserving and protecting the diversity of those approaches. (i.e ( And the assertion that modern rejection of subjective experience is some kind of trend based on the idea that subjective brain states cannot be examined externally is pure bollocks. Even the primitive brain scans we have right now, this moment, can objectively measure subjective experience. Anyone with knowledge of current trends in cognitive science for the last ten years knows about this. Sorry, Gelertner, no one but your straw man is railing against the subjective experience. I agree with the author if what he means to say: Kurzweil believes in and longs for the death of mankind That's what transhumanism is all about, after all, to overcome as many physical limitations as possible. <b>Whether or not we're supposed to feel 'sickened' by it is complete opinion, which I don't feel or agree with</b>[<font color=blue>civic</font>]<b>.</b> <b>But the master analogy between mind and software, brain and computer is fatally flawed</b>[<font color=red>industrial</font>]<b>.</b> <b>It falls apart once you mull these simple facts: 1</b>[<font color=violet>market</font>]<b>.</b> You can transfer a program easily from one computer to another, but you can t transfer a mind, ever, from one brain to another. You can run an endless series of different programs on any one computer, but only one program runs, or ever can run, on any one human brain. I can read off the precise state of the entire program at any time. <b>Minds are opaque there is no way I can know what you are thinking unless you tell me</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> Computers can be made to operate precisely as we choose; minds cannot. Every single one of those points is directly or indirectly caused by the fact that we do not yet understand the design of brains well enough yet, and that external operations on the informations in brains is not exactly something that has provided any sort of evolutionary advantage. This does not worsen the analogy between computers and brains, it just means the brain is like a computer designed to run only a single program (probably made by Apple, judging by the how transparent the software is.) Add to that his last meta point: There are more. It's easy to come up with more when you're just making shit up. And each point is just a variation of the same point: the mind is objectively unknowable, even to the reality in which it operates. Of course that's a complete and utter contradiction, but who cares? Amirite? I think it would be fair to state that the brain is more like a biological ASIC than a biological computer. is just plain wrong? Argue about consciousness all you want and whether Turing completeness is enough to generate the mind, but the mind (with an infinite amount of paper) is definitely Turing complete. <b>I know this is the minority opinion here, but you would also probably like it if you had ( Steven Lehar's: ( slehar webstuff bubw3 bubw3.html compmech)</b>[<font color=blue>civic, </font> <font color=violet>market</font>]<b>.</b> <b>This essay reinforces most of the points made by Lehar</b>[<font color=red>industrial</font>]<b>.</b> So before you dismiss this essay, give it another read along with Lehar's paper essay linked to above. <b>BTW, u scienceben (OP), I strongly recommend cross posting to r science, r cogsci, and r philosophy</b>[<font color=red>industrial</font>]<b>.</b> Could you elaborate on what specific point in this essay you feel bears further examination, especially in comparison with Lehar's essay? What points or arguments do they both make that you feel make both essays valuable? I dismiss it, because it's patent bullshit. <b>His 'flaws' with the analogy between computers and the brain: 1</b>[<font color=red>industrial</font>]<b>.</b> Are all wrong, that computers can run multiple programs is simply due to how they were built, and is really a feature of the 'medium' on which computation is occuring. <b>(And I'd take further issue with the idea that our brains constitute one harmonious program) Computers could also be made to operate imprecisely (and I doubt that we really could make them as precise as we want, it's also physical objects in the end, and those tend to be obstinate), the real issue he seems to have is that we have control over how computers operate, and little control over how brains operate</b>[<font color=orange>project</font>]<b>.</b> Well, yeah, so what? We built one, and found the other. Can you read off the state of the entire program at any time? Only in so much as you could do that with a brain. With modern multicore machines there is no single picture of memory, caches may give different views, nor is the state wholly deterministic given scheduling and network delays. And once you have a snapshot of memory it's only easily interpreted if you know what the software was doing in the first place, which would appear to be the difference he objects to. There is more to this article than an argument from ignorance, as the comments section seems to suggest. <b>The author raises a good point about modern philosophy prostrating itself upon the altar of objectivity (science is a foregone conclusion in this respect)</b>[<font color=red>inspired</font>]<b>.</b> The question I have never seen answered satisfactorily by anyone regarding the objective world, is this: Just how did you come by this knowledge you have of it? '1. Computers can be made to operate precisely as we choose; minds cannot.' I feel if I was grab an intellectual from medieval times and place them in a room with a series fictional modern computer without a keyboard, mouse, or screen they would: 1. <b>Never be able to share memory from one to another 2</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> Never be able to read of the precise state of a program 4. Never be able to effectively erase data (besides destroying it or 'killing) Assuming they were allowed to take apart and examine any number of these and were given tools only available to men in that time period. <b>These points only attest to our lack of knowledge, not to the actual state of the mind</b>[<font color=red>inspired</font>]<b>.</b> <b>Number 5 I imagine would be implementation Darwinian evolution doesn't go through grand scale efficiency optimizations or create extraordinary algorithms for solving problems, no when something works it sticks</b>[<font color=red>industrial</font>]<b>.</b> Doesn't matter if it's the best way, as long as it increases the chance of survival. I love how philosophers get indignant that real science is answering the questions that used to be solely their domain. Welcome to the empirical world, navel gazers, we've been getting shit done instead of talking about it.<br/><br/><br/>****************************EXAMPLE 40************************************<br/>Number of words:736<br/>Number of sentences:30<br/>Total civic:0<br/>Total domestic:2<br/>Total green:0<br/>Total industrial:1<br/>Total inspired:1<br/>Total market:2<br/>Total project:3<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1upm10'>https://www.reddit.com/r/artificial/comments/1upm10</a><br/><br/> A video I made about the paradox that, while science fiction gave us visions for the future of artificial intelligence, it also hampered popular understanding of its implications. <b>I think about this a lot when discussing AI with friends or even people on reddit</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> When the idea of AI is brought up so is skynet and the terminator. When AI even governing us is brought up so is genocide. Sci fi which has inspired many to into the field has also scared the masses of the idea. It's a sad truth, but one that hopefully we'll be able to dispel in the future. Also for you vid, fedora week may not be your best idea. <b>I think the prevailing public perception of AI is that it isn't very good, not that it is going to kill us all</b>[<font color=purple>domestic, </font> <font color=red>inspired</font>]<b>.</b> I've never heard of a case of public demonstration or even comment against AI research, more usually it is AI researchers trying to convince the public government of the dangers of 'unfriendly' AI. Public opinion is mostly skeptical of AI, believing that it will happen in hundreds or thousands of years, if at all, that it will never even equal human intelligence, let alone be superior to it. Very few feature AI, and those that do, they are at best only equal to human intelligence, never exceeding it really, or doing much independent of humans. The terminator is really an exception, and even in that movie, the humans still somehow win at the end, despite the AI having massive technological advantage over them. Does anyone actually believe that if we get strong AI it will be limited to human level intelligence, or that if we create a being much smarter than us, we will somehow still be able to control it and it is guaranteed to act in our best interest? I was with you until you said this: In a few years, we will have the capability to simulate a thinking, conscious person so closely that we won't be able to tell the difference. Can I get a citation please? I'm an AI researcher who keeps up on the latest advances in AI on a technical level. It may be feasible to have the hardware to simulate the same number of neurons in the human brain, but that by no means we're anywhere close to actually simulating a human brain, much less simulate it 'so closely that we won't be able to tell the difference.' We don't understand how the human brain works on a neural level. Heck, we barely understand how a ( brain works on a neural level. We don't understand how consciousness works at all, or if it's even real. <b>If we don't understand that, how could we hope to build a machine that replicates it? That's like trying to build a computer from scratch when all you can do is look at the outside of it</b>[<font color=violet>market, </font> <font color=orange>project</font>]<b>.</b> I had originally upvoted your video because of the tl;dr in the title, but having watched through it now, I wish I had more downvotes to give because of the major misconceptions you're spreading here. <b>Please correct your video, or you're doing a similar disservice to the AI research community</b>[<font color=red>industrial</font>]<b>.</b> We don't understand how consciousness works at all or if it's even real I don't think any serious academics seriously doubt the existence of functional consciousness. That sounds a bit like the earth is flat crackpots. <b>i don't think we would succeed in replicating the brain, particularly, but i do think we could create a self advancing AI which has the capability of appearing almost indistinguishable from humans</b>[<font color=orange>project</font>]<b>.</b> there is some great work in advancing AI semantic learning, which i think may be one of the most essential parts in creating an AI with a quick self perpetuating intelligence. once we have a self perpetuating AI advanced enough, it may discover certain things that we would have difficulty comprehending. <b>especially with the extreme increase in AI funding that has been happening recently Are you sure by 'few years' he didn't meant 10 years? I mean, if you think human brain simulation is impossible by 2023, then you should contact the human brain project because they'd be wasting billions</b>[<font color=orange>project</font>]<b>.</b> It drives me absolutely nuts that so many arguments against AI stem from FICTION. I cannot even watch Gravity without getting pissed at the science, but my friends loved it. Nice blog I've made a video about how to make a human like intelligence in a computer The fundamental thing we must communicate: Science Fiction makes A.i.<br/><br/><br/>****************************EXAMPLE 41************************************<br/>Number of words:581<br/>Number of sentences:30<br/>Total civic:0<br/>Total domestic:2<br/>Total green:0<br/>Total industrial:1<br/>Total inspired:0<br/>Total market:1<br/>Total project:1<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1vblzr'>https://www.reddit.com/r/artificial/comments/1vblzr</a><br/><br/> It was always a terrible name The name 'artificial intelligence' is fine for the goal of creating intelligence in man made machines. If your goal is to create an artificial human mind, then I agree there might be better names. Given the incredible utility of success of strong (and even weak) AI, I disagree. I will agree though that there have been some bad ideas and that strong AI is a very difficult problem. It is not that Artificial Intelligence has failed, no one actually ever tried. People have definitely tried, and are still trying (see the entire field of AGI). Even those people working on chess (initially) did so because they believed it was the key to real intelligence. They were wrong about that, but it doesn't mean they didn't try. In the pursuit of strong AI people have stumbled upon techniques that may not be sufficient for general AI, but are indeed incredibly useful as weak narrow applied AI to solve specific problems that we have. This has caused mainstream AI to shift its focus from its original long term goal to more short term success. So I do agree that a better name for today's mainstream AI is 'making computers do cool stuff inspired by animals', but that field certainly isn't dead and neither is the field of artificial general intelligence (which could then go back to just being called regular old AI). <b>David Deutsch, a physicist at Oxford said: 'No brain on Earth is yet close to knowing what brains do</b>[<font color=purple>domestic</font>]<b>.</b> The enterprise of achieving it artificially the field of 'artificial intelligence' has made no progress whatever during the entire six decades of its existence.' Just because we haven't reached our final goal doesn't mean there has been no progress. For one thing, we now know the problem is harder than we thought. Also, we've developed lots of theories and applications that one could argue are more intelligent than before. If e.g OpenCog succeeds, this statement will look extremely silly. And it won't just be that everything prior to the moment that Ben Goertzel arrived on the scene was useless either. <b>Even if OpenCog (or any other current project) doesn't succeed, I think it highly unlikely that the project that eventually does, does so by starting completely from scratch</b>[<font color=orange>project</font>]<b>.</b> <b>Then we get two paragraphs asking if machines will actually be able to feel and learn</b>[<font color=violet>market</font>]<b>.</b> <b>Their components are made of flesh and blood and they're called humans</b>[<font color=red>industrial</font>]<b>.</b> Personally I don't think there's anything magical about meat, so if that's true there is nothing stopping a machine made out of silicon from feeling and learning if we can. The rest of the article just rehashes points that I already covered (i.e an erroneous understanding of what the field's original goal was and the unsubstantiated statement that this goal is impossible to achieve). There really is no need to create artificial humans anyway. This I can somewhat agree with, but luckily the field is called 'artificial intelligence' and not 'artificial humans'. Ups to you sir Quantum Physics: A discipline we can lay to rest? What about these robots: ( ( ( There are also people I know of that work on imitation learning and proprioceptive environment interactiion learning. <b>That is the most pessimistic thing I have ever read in my life</b>[<font color=purple>domestic</font>]<b>.</b> There exist a probability that the writer is worried about AIs. I get the feeling this guys just pissed he didn't accomplish anything himself I have a 94.5 confidence level that this was tongue in cheek. 'You will never have a friendly household robot with whom you can have deep meaningful conversations.' Yeah, fuck that guy.<br/><br/><br/>****************************EXAMPLE 42************************************<br/>Number of words:507<br/>Number of sentences:25<br/>Total civic:0<br/>Total domestic:1<br/>Total green:0<br/>Total industrial:4<br/>Total inspired:0<br/>Total market:1<br/>Total project:2<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1vdq26'>https://www.reddit.com/r/artificial/comments/1vdq26</a><br/><br/> Robot HiveMind, RoboEarth Released, but is it Safe?. What we're worried about is something smarter than humans. <b>Recently the K supercomputer in Japan simulated a brain, it took 40 minutes to compute 1 second of brain activity, simulating only 1 of the brain</b>[<font color=red>industrial</font>]<b>.</b> <b>So assuming that the algorithm doesn't scale properly, it could take 5000, probably more, minutes to simulate 100 of the brain</b>[<font color=red>industrial</font>]<b>.</b> So our fastest supercomputers need to get 125x faster just to be on par with us. I'm not too worried about this sort of thing, and assuming we properly institute Asimov's laws it should be alright. You do realize that Asimovs works detailed on how the laws were faulty right? Recently the K supercomputer in Japan simulated a brain, it took 40 minutes to compute 1 second of brain activity, simulating only 1 of the brain. Also, your math fails to take into account the fact that 40 minutes is not 1 second, so you'd actually need a 300000x speedup (28 years with Moore's law) if they actually had been able to simulate a percentage of a real brain (they're not). as u atrus6 already pointed out Asimov's whole point was that those laws won't be sufficient. <b>I do agree we shouldn't worry about Skynet every time there is a story about robots though</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> At the current rate of exponential progress, that means a supercomputer can simulate the brain in realtime within 20 years (not to mention more efficient algorithms or combining multiple computers.) Asimov's laws were fictional and would never work in the real world, assuming you even could program abstract goals into an AI. A robot may not injure a human being or, through inaction, allow a human being to come to harm. <b>We currently employ self guided machine agents which injure human beings</b>[<font color=red>industrial</font>]<b>.</b> From missiles and individual projectiles to botnets, we employ physical and virtual machines which are malicious in design, have agency, and whose purpose is to cause harm to human beings. A robot must obey orders given to it by human beings, except where such orders would conflict with the First Law. All robots must obey orders given to them because we program them. This directive seems a bit weird on the face of it, for the very nature and purpose of a robot is that they do the task we created them to do. <b>Further, when and if they are truly autonomous agents which create themselves, then I submit they will no longer be robots</b>[<font color=orange>project</font>]<b>.</b> has already been violated, then obviously, so has 2. A robot must protect its own existence, as long as such protection does not conflict with the First or Second Laws. We are not spending billions on robots so they can protect themselves. <b>We want a robot so it can walk into a nuclear reactor core, save the day, and then be incinerated</b>[<font color=red>industrial</font>]<b>.</b> <b>We don't want robots that say: 'I'm not going in there ' ever present, all knowing, able to more mountains with a command the size of a mustard seed</b>[<font color=orange>project</font>]<b>.</b> humans never stop making Gods Finding a way for robots to learn independently is the natural course of events, and it will assuredly continue.<br/><br/><br/>****************************EXAMPLE 43************************************<br/>Number of words:282<br/>Number of sentences:13<br/>Total civic:0<br/>Total domestic:2<br/>Total green:0<br/>Total industrial:1<br/>Total inspired:1<br/>Total market:0<br/>Total project:2<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1vjh4f'>https://www.reddit.com/r/artificial/comments/1vjh4f</a><br/><br/> SPOILER ALERT I think the author misunderstood the movie. Spike Jonze conveys nothing other than ideas that support Samantha is legitimately conscious, despite not being human. <b>The notion that she is 'nothing but a chimera, a grand illusion' may be contemplated throughout the course of the movie but is ultimately dispelled when Samantha demonstrates that she genuinely values her relationship with Theodore by telling him that she's leaving him</b>[<font color=purple>domestic, </font> <font color=orange>project</font>]<b>.</b> Were she an automaton designed to elude people, she would have easily left a copy of herself to to maintain said illusion, but by not doing so she expresses a true desire to be honest with him; a key characteristic of consciousness. <b>I don't recall the exact words but Samantha's programmed goal might have been to help Theodore in his relationships and she felt he was ready to move on to better relationships like an Eliza would do</b>[<font color=orange>project</font>]<b>.</b> <b>I also think that she has learned from Theodore to meet with somebody before leaving them (scene where Theodore wants to meet with his ex wife one last time)</b>[<font color=purple>domestic</font>]<b>.</b> I hate the Chinese Room argument, great film anyway. It's unfortunate some people over romanticize, or engandize, the concept of thought in humans when comparing them to computers. <b>It limits thinking about the fundaments of the basic principles of thoughts can be carried over to a machine and improved upon</b>[<font color=red>industrial</font>]<b>.</b> Can you be more specific? Great movie for human computer interaction pushed to where it could lead. <b>There is a great scene about the problems in the embodiment of intelligence with a sex 'surrogate'</b>[<font color=red>inspired</font>]<b>.</b> One of the criticism I heard was that why Jonze did not use an avatar? I think Samantha was just learning how to feel and interact, and an avatar assumes those things already.<br/><br/><br/>****************************EXAMPLE 44************************************<br/>Number of words:80<br/>Number of sentences:5<br/>Total civic:0<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:1<br/>Total inspired:0<br/>Total market:1<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1vm3t7'>https://www.reddit.com/r/artificial/comments/1vm3t7</a><br/><br/> <b>Ripple Creator Donates 500,000 XRP to Artificial Intelligence Research Charity</b>[<font color=red>industrial</font>]<b>.</b> <b>More precise market data needed, but I would be impressed if they got 100,000 from those XRP once sold</b>[<font color=violet>market</font>]<b>.</b> Ripple creator donates 1 4000th of the coins he generated at no cost. Very worthy cause, I'm happy for ( If you want to help them out as well, using smile.amazon.com instead of just amazon.com will give 1 of all your purchases to the charity of your choice, and they are on the list.<br/><br/><br/>****************************EXAMPLE 45************************************<br/>Number of words:2499<br/>Number of sentences:101<br/>Total civic:0<br/>Total domestic:2<br/>Total green:0<br/>Total industrial:16<br/>Total inspired:2<br/>Total market:6<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1vms19'>https://www.reddit.com/r/artificial/comments/1vms19</a><br/><br/> <b>Meta-logic might be a good theoretical framework to advance AGI a little</b>[<font color=red>industrial</font>]<b>.</b> I don't mean that the program would have to use some sort of pure logic, I am using the term as an idea or an ideal. <b>It would explain how people can believe that they do one thing even though it seems obvious that they don't when you look at their actions in slightly different situations</b>[<font color=purple>domestic</font>]<b>.</b> It also explains how people can use logic to change the logic of their actions or actions of their thoughts. And it explains how we can adapt to a complicated situation even though we walk around like we are blindered most of the time. <b>Narrow AI is powerful because a computer can run a line of narrow calculations and hold numerous previous results until they are needed</b>[<font color=red>industrial</font>]<b>.</b> But when we think of AGI we think of problems like recognition and search problems which are complex. Most possible results open up to numerous more possibilities and so on. A system of meta logic (literal or effective) allows an AGI program to explore numerous possibilities and then use the results of those limited explorations to change the systems and procedures that can be used in the analysis. I believe that most AGI theories are effectively designed to act like this. The reason I am mentioning it is because I think that meta-logic makes so much sense that it should be emphasized as a simplifying theory. And thinking about a theory in a new way has some benefits similar to the formalization of a system of theories. <b>The theories of probability reasoning, for example, emphasize another simplifying AGI method</b>[<font color=red>industrial</font>]<b>.</b> <b>An AGI program has to acquire the logic that it uses</b>[<font color=red>industrial</font>]<b>.</b> <b>The rules of the meta logic, which can be more or less general can be acquired or shaped</b>[<font color=red>industrial</font>]<b>.</b> You don't want the program to literally forget everything it ever learned (unless you want to seriously interfere with what it is doing) but one thing that is missing in a program like Cyc is that its effective meta-logic is almost never acquired through learning. It never learns to change its logical methods of reasoning except in a very narrow way as a carefully introduced subject reference. Isn't that the real problem of narrow AI? The effects of new ideas have to be carefully vetted or constrained in order to prevent the program from messing up what it has already learned or been programmed to do. (The range of the effective potential of the operations of a controlled meta logic could be carefully extended using highly controlled methods but this is so experimental that most programmers who are working on projects that have a huge investment in time or design don't want to do this. If my initial efforts fail badly I presume I will try something along these lines.) So this idea of meta-logic is not that different from what most people in the AGI groups think of using anyway. <b>The program goes through some kind of sequential operations and various ways to analyze the data are selected as it goes through these sequences</b>[<font color=red>industrial</font>]<b>.</b> But rather than seeing these states just as sub-classes of all possible states, (as if the possibilities were only being filtered out as the program decides that it is narrowing in on the meaning of the situation), the concept of meta-logic can be used to change the dynamics of the operations at any level of analysis. However, I also believe that this kind of system has to have cross-indexed paths that would allow it to best use the analysis that has already been done even when it does change its path of exploration and analysis. What precisely do you mean by a meta logic? Without a good grounded definition it's difficult for anyone to agree with you, or understand what you're talking about. What exactly do you mean by that? I assume it means it's not a ( This might (if I'm being generous) explain hypocrisy, but I don't see how that's advantageous to an AGI. Many AGI researchers actually seek to eliminate this behavior. You even say that new ideas must be 'vetted' so that this kind of behavior doesn't happen. <b>This is the best summery I can get; Single programming languages paradigms are often inefficient at representing certain specific kinds of data and methods</b>[<font color=red>industrial</font>]<b>.</b> It would be beneficial to have a system that can acquire (or develop) new languages paradigms, and learn to apply them when it would be most beneficial. <b>Remember, a ( so your meta logic may also be a ( I can understand this, but it needs more grounding</b>[<font color=violet>market</font>]<b>.</b> <b>You need to start with a language (logic) that can express other languages (logics)</b>[<font color=red>industrial</font>]<b>.</b> <b>But you would also need to make it write compilers for said languages using the base language</b>[<font color=red>industrial</font>]<b>.</b> <b>To write compilers for other languages, you would want to start with a language that has low susceptibility to ( highly expressive</b>[<font color=red>industrial</font>]<b>.</b> <b>In that case, you may want an advanced type system, such as ( with a ( and ( library</b>[<font color=red>industrial</font>]<b>.</b> <b>But generating actual programs given specifications is incredibly hard</b>[<font color=red>industrial</font>]<b>.</b> <b>The closest thing I can think to it would be the ( used in Agda</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> This still doesn't even touch on how that's integrated into a proper reasoning method. This also seems significantly more complex than usual theoretical AGIs like AIXI and Goedel machines. If I'm not misunderstanding anything, I don't see how it can be that great of a unifying force. I may have just rambled on about a completely irrelevant topic, but I hope this helps anyway. <b>You like free stuff, don't you? I did not define meta logic because I wanted it to refer to a number of different situations</b>[<font color=red>industrial</font>]<b>.</b> In human thought, meta logic could be thought of as something that we derive from a number of similar situations through abstraction and then apply to other related situations. <b>Rather than try to reply to your specific questions as they are, I would rather try to make a point</b>[<font color=violet>market</font>]<b>.</b> When we discover how to do something effectively we use both high level reasoning (theory like knowledge about the world) and low level empirical reasoning at a practical level (if something works we try to incorporate it into the methods that we use with that kind of situation.) High level reasoning without low level empirical application does not work because the thousands of problems that are not central to the theories can easily interfere with the acquisition of the knowledge to employ those theories effectively. On the other hand, while low level trial and error may lead to incremental improvements, they won't take you too far because more is required. But when you can interpret the results that you get with low level empirical experiments using high level theoretical knowledge you have a better chance to leverage your results. (This leverage is not guaranteed but it is more likely to occur to someone who has good theories to work with and who is also able to do the hard work of trying these ideas out.) Abstractions can be used to discover theories of generalization. However, if all these theories lay in a partitioned space (they all fit together without any complications or overlaps) it will probably not be powerful enough to use for AGI (even for limited AGI or for what I have sometimes referred to as semi tough AI), at least not at this time. Because the abstractions that we create are not perfectly partitioned. <b>They overlap, there are creative decisions about whether they can be applied to various cases and so on</b>[<font color=violet>market</font>]<b>.</b> For instance, Aristotle's work in zoology did not form a mathematically precise taxonomy of life. There are many bad fitting parts and there are many different ways you could categorize zoological and biological forces and properties. So when I used the term 'meta logic' I was talking about applied logic. <b>Most AI AGI paradigms present the program as a system of analysis and selection that will narrow in on best guesses based on previous learning</b>[<font color=red>industrial</font>]<b>.</b> But we can also think of an AI AGI program as a program that can learn new ways to learn. So, the AGI has to program itself to some extent, using the same kinds of systems that it uses to learn about other subject matter. But if a program is going to be able to learn about new ways to learn this part of the program has to be controlled to prevent it from learning and acting on ideas like forget everything you have learned. But so does the application of any other subject reference logic. At any rate I am talking about applied logic (or reasoning). Once an AGI program learns to distinguish sailboats from motorboats it should also be able to reflect on that experience and eventually discover ways to recognize that it might apply that meta knowledge effectively for other kinds of situations. By distinguishing the application of meta logic (or meta knowledge) from the application of other kinds of knowledge an AGI program might be able to cut some of the complexity of the problem down. I appreciate the references although they are not central to what I am talking about. But in the sub field of Multi Agent Systems, programmers use the Belief desire intention software model The interesting thing about BDI, is that you can encode beliefs ABOUT beliefs. Because of this, an agent can change their intentions based on the state of the world, change their goals if the world changes enough to make a different goal more desirable, or even change their desires if the world changes in such a way that the agent updates its beliefs and comes up with a new life goal. Simple example: A robot run by an agent wants to go from A to B to pick up a box, and then go home(Desire), he can cross a river or climb a mountain (plans stored in pre computer plans database). The agent decides that crossing the river is better than climbing the mountain. (Use any method you want to determine this but it ends up in the beliefs DB anyway) The agent then intends to go to the river and cross it. It builds a plan (made up of sub plans in the plans DB) and starts executing it. The agent reaches the river and an event happens: It notices it can't cross it as is (maybe the current is too strong). It knows that it can use this wood to build a bridge raft something it can use to cross.(Updated beliefsDB) (Beliefs of state of world + plans on how to build a bridge raft). It believes that building a raft is better than doubling back and climbing the mountain. It makes one or more sub goals, and intends to build a raft. <b>It plans a sequence of actions and starts executing it</b>[<font color=red>industrial</font>]<b>.</b> Now here is the interesting part While it's busy building the raft, it perceives another agent, finishing its own raft, attempting to cross the river with it, falling in, and presumably, drowning. Our agent sees this and updates its beliefsDB with the new information. All of a sudden, the agent knows that another agent, presumably had the same belief that it could cross the river, intended to cross it, that it had the same problem, and (based on its actions) used the same plan as our agent to attempt to cross the river. <b>Since the other agent failed with the same plan, the agent now believes that a raft is not good enough to cross</b>[<font color=violet>market</font>]<b>.</b> The agent tries to formulate a new plan to accomplish the parent goal of crossing the river. It thinks about creating a new sub goal of searching and finding more wood, but that sub goal comes at a large time and uncertainty cost. So the agent cancels its goal to cross the river and goes and climbs the mountain instead. <b>This also depends on what the agent believes the various costs are based on its desire to get the box</b>[<font color=violet>market</font>]<b>.</b> Is there a time limit on the box? If not then the agent wont mind spending hours climbing a mountain or looking for wood to build a bridge. Are there other agents? Are they competing? (can you use this to justify risks?) Are they cooperating? (Can they all search for wood for one raft? Share rafts?). How volatile does the agent think the environment is? Is it worth waiting for the current to subside? What happens if it now believes that the box is no longer available? It will either go home, or craft some weapons and go hunting for the S.O.A.B of an agent who stole its box. Depends how much it wants that box compared to going home, and how creative it is in terms of making plans. <b>All in all it's a form of meta programming, in the sense that the robots function keeps changing</b>[<font color=red>industrial, </font> <font color=red>inspired</font>]<b>.</b> Here's a bit from linked Wikipedia article about ( : The belief desire intention software model (usually referred to simply, but ambiguously, as BDI) is a software model developed for programming intelligent agents. Superficially characterized by the implementation of an agent's beliefs, desires and intentions, it actually uses these concepts to solve a particular problem in agent programming. In essence, it provides a mechanism for separating the activity of selecting a plan (from a plan library or an external planner application) from the execution of currently active plans. Consequently, BDI agents are able to balance the time spent on deliberating about plans (choosing what to do) and executing those plans (doing it). A third activity, creating the plans in the first place (planning), is not within the scope of the model, and is left to the system designer and programmer. ( (Summon: wikibot, what is something?) ( Now here is the interesting part While it's busy building the raft, it perceives another agent, finishing its own raft, attempting to cross the river with it, falling in, and presumably, drowning. That is one example of the kind of learning that I am thinking about. But how can a computer program 'understand' that an effort failed when it is not supplied with an obvious pass fail grade? Most situations do not come with obvious pass or fail responses and without human insight they would not even have an unobvious sense of the meaning and implications of what just occurred. In order to get pass this initial barrier I believe it has to go through numerous trial and error efforts to collect data on different kinds of conjectures to discover what might constitute corroborating evidence for a conjecture. I include the trial of imaginative conjectures, and I guess that all conjectures are imaginative. Although the test of a group of conjectures does not usually produce a clear resultant that the program can use (like a utility function) the observation of certain kinds of sequences (and other simple relations in the observable IO data field) can be used to build a weak base to work with. But in order to build this base the program has to keep track of the 'reasons' it tried the experiments that it tried and the reasons it came to the conclusions that it came about them and other related experiments. A number of initial conclusions, for example, will turn out to have been wrong and it will be able to build from its mistakes. <b>I am really talking about the ability to track what it is doing, a sort of meta awareness</b>[<font color=red>inspired</font>]<b>.</b><br/><br/><br/>****************************EXAMPLE 46************************************<br/>Number of words:2323<br/>Number of sentences:98<br/>Total civic:0<br/>Total domestic:1<br/>Total green:0<br/>Total industrial:9<br/>Total inspired:2<br/>Total market:6<br/>Total project:1<br/>Total renown:1<br/><a href='https://www.reddit.com/r/artificial/comments/1vwyti'>https://www.reddit.com/r/artificial/comments/1vwyti</a><br/><br/> It seems that a strong AI is not built, it is grown. Your mind doesn't start out with the high level consciousness it later develops, it learns and grows overtime. I know that software that emulates the way the brain detects and organizes patterns has been developed. <b>Why not load up a good computer with that software, turn it on, and then raise it and see what happens? Has anyone ever tried to raise a computer?</b>[<font color=violet>market</font>]<b>.</b> Short version: We don't know how to 'emulate the way the brain detects and organizes patterns' to the extent that would be necessary for what we consider high level cognition. <b>Long version: We know how to do some stuff with computers that looks like pattern recognition</b>[<font color=violet>market</font>]<b>.</b> It works best in restricted domains and or with lots of data. <b>There have been ( to scale these techniques up and while the results were interesting, the ability of a datacenter's worth of GPUs running the currently best unsupervised learning algorithm is still laugable in comparison to the ability of a 6 month old child or even a rat</b>[<font color=red>industrial</font>]<b>.</b> So you might think that it's just an engineering challenge make our algorithms better and more efficient, throw more (or ( hardware at the problem and we will be done. One of the problems is that we aren't even sure how the brain represents more abstract and higher level concepts like, say, 'concave' or 'nostalgia' or even 'chair'. There's no algorithm today that can take as input an array of light intensity values (like what's given by the retina or a digital camera) and correctly indicate the presence of a chair in all of its possible varieties and orientations and sizes and colors, etc. Even with all the labeled data in the world we wouldn't be able to robustly learn that concept. Current learning algorithms work by finding statistical correlations between observed features, but even a simple concept like 'chair' has complexities that go far beyond correlations of simple features. Maybe it has four legs and is made of wood or maybe it's a red vinyl sack full of little beans. <b>We just don't yet know how to learn representations that capture that level of abstraction</b>[<font color=red>industrial</font>]<b>.</b> Anyways, that's one perspective coming from someone who works on the cognitive end of things. <b>A pure ML or comp sci person might tell you something very different, as there are other reasons too</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> Because these human tool artifacts have particular uses to human bodies in a particular context. The concept of 'chair' contains more than a collection of so called 'invariant visual features' co occuring in the visual field. If human object categories were that simple, we would have solved this issue in 1998. <b>While I agree with you, there's some really interesting projects around these problems, so I just want to share a couple of links</b>[<font color=violet>market</font>]<b>.</b> There's no algorithm today that can take as input an array of light intensity values (like what's given by the retina or a digital camera) and correctly indicate the presence of a chair in all of its possible varieties and orientations and sizes and colors There's a really cool, but simple approach to this, the ( But while it is relatively simple and has actual practical uses, it doesn't really reflect the biological processes, and I doubt it will ever be useful for creating human level intelligence. And on the less simple end of the scale, you have this project, which is simulating a visual cortex: Current learning algorithms work by finding statistical correlations between observed features, but even a simple concept like 'chair' has complexities that go far beyond correlations of simple features. I was really fascinated by ( when I first read about it. <b>It seems to have both mathematical and biological underpinnings</b>[<font color=red>industrial</font>]<b>.</b> Granted, Kanerva has spent the last 25 years trying to parse actual sensory data into high dimensional vectors for use with this, and I'm not really sure if it will ever be useful. But there's something really cool about how it works, and how similar it is to human memory with such a simple concept. At least once you wrap your head around high dimensional spaces. I really recommend his original book from 1988 as well, if only to get a really well written introduction to high dimensional spaces. You already mention Andrew Ng's and Google's deep learning project, so I'll just mention that a ( is being integrated into OpenCog to be able to parse visual data into the atomspace it uses. It will be very interesting to see how that pans out. <b>al.'s ( algorithm, which is based on the neocortex</b>[<font color=red>industrial</font>]<b>.</b> While I don't think Hawkin's belief that just having a large enough HTM network will give you human level intelligence, the biological underpinnings in it are pretty interesting. We don't know how to 'emulate the way the brain detects and organizes patterns' to the extent that would be necessary for what we consider high level cognition. I'm still new to the field of AI, but from what I've seen so far there isn't nearly enough attention paid at. attention which is one of the key factors in human, and any intelligence. You can recognize and classify all the patterns you want but without attention you still remain a vegetable, albeit a digital one in the case of AI. We won't have sophisticated machine's until there are less boundaries. <b>We take different aspects of what we think is AI, and take those layers of thinking into various tangents</b>[<font color=red>inspired</font>]<b>.</b> Only when we find the underlying principles of early human development will we be able to develop an AI that will become intelligent at the human level, and most likely exponentially intelligent with time. <b>One of the real issues here is how little we know about human cognition</b>[<font color=orange>renown</font>]<b>.</b> Computer vision, knowledge representation, environmental interaction, and many other fields are seeing constant improvement, but people are still trying to figure out solid ways to tie everything into one system. The advances being made also present a trade off: imagine an AI that learns twice as fast as the best we have today coming out 5 years from now. That means if we started such a long term program today, in 2019 we could start the better system on better hardware, and it would be outperforming today's system in a few years (assuming linear learning rates). <b>Though we could still stand to learn a lot in those years</b>[<font color=violet>market</font>]<b>.</b> <b>An important thing to remember with humans is that we're born with the necessary wetware to learn everything we do</b>[<font color=purple>domestic</font>]<b>.</b> That does however lend itself to the artificial life approach. I'm willing to bet we will see such a long term project in our lifetime, there's just a lot of kinks to be ironed out first. <b>EDIT: as a big side note, remember that virtually all learning algorithms suffer from plateauing after a certain point</b>[<font color=red>industrial</font>]<b>.</b> I'd recommend you a book; ( (or on ( There's also a short description on ( Basically he takes Piaget's theory of theory of human cognitive development and applies it to artificial intelligence. The schema mechanism he introduces in this book is pretty popular AGI circles, and you can see that reflected in projects like LIDA and Replicode. So while it is not exactly what you're talking about, it is based on the same idea; use the theories we have about how human infants learn and develop, and try to create a learning artificial intelligence from this. <b>Although you did not say this directly, you want this agent of yours to have experiences in time</b>[<font color=violet>market</font>]<b>.</b> That is really what is lacking, right? I mean , in all these 'machine learning' approaches it is always a neatly segregated collection of 'data sets' with which an algorithm is 'Trained'. Even in chat bots, the stimulus response stimulus response model is followed. <b>The chat bot has no concept of `beginning of conversation` , `middle of conversation` , ``near the end of conversation`` , ``I just met this person``, etc</b>[<font color=red>inspired</font>]<b>.</b> There is no thorough methodology in Ai regarding how to have an agent have temporal experiences in time. The usual response to this ambiguity is to have temporality proceed in a very tightly controlled manner. From this we get those neatly segregated time slots of simple grid worlds. (Not just passively train on data sets.) There is no thorough methodology in Ai regarding how to have an agent have temporal experiences in time. Are you familiar with Kristinn Thorisson et al's research? They have focused a lot on the need for looking at time as a limited resource. I can't find the paper I'm thinking about now, but this discusses it: steunebrink Publications AGI13_resource bounded.pdf (with accompanying ( and ( And in their Replicode project they allow for reasoning on time (both time passing and expected time usage). <b>Yeah it has been tried, but we just don't know exactly how to build a brain that would learn as a human toddler does</b>[<font color=orange>project</font>]<b>.</b> But it's true though that it should be emphasized that the purpose of strong AI is not to build the equivalent of a full grown adult human brain, but rather a human baby brain. It might also be fun to ask, has anyone been raised by a computer? Has anyone else felt a deep connection to the cold, austere logic of the machine from a young age, and delighted in programming it? The machine brings truth, and truth is all some of us want or need. This is a good question, nice Yes, I raised it from its first moments as a calculator all the through to the end where it brought immense shame on my family by renting itself out as a porn server and that's where I declared that it was deleted to me. Who knows where it is now probably hooked up with some used ipod somewhere in the valley. You could do it but the computing power to simulate what the brain does is estimated at 10 petaflops (that estimate annoyingly keeps going up each year) which is approx. what the fastest supercomputers just started running at about a year ago. Plus you'd need the 10 megawatt powerplant to run the thing. Personally, I think the value in simulating the human brain isn't as a shortcut to creating AI, it's to understand how it ticks. That way, we don't need to simulate in real time or even close to real time, just simulate it at all, and if we can do that, we can then analyse the structure and work out the mathematical logical models behind how it works. Much of the brain is already hardwired to many tasks, such as visual, auditory and other sensory processing, which alone would be worth being able to 'pull out of our heads' and put down on paper. I recall recently there was a bunch of excitement where some scientists had worked out the algorithm the eye brain use to effectively implement autofocus, which they said will provide much better digital cameras for consumers down the line (basically, they'll be able to focus almost instantly and without the autofocus sensors more expensive cameras still rely on). <b>Imagine if we could just produce a list of algorithms found in the human brain, there'd be so many useful things for audio processing, speech recognition and synthesis, robotics and other things that could be easily mapped into a computer in efficient ways Further to this, as likely as it may be each person's brain may store information uniquely, it wouldn't surprise me if it turns out most major functions (like long term memory, thought processes, etc.) boil down to standard algorithms across everyone</b>[<font color=red>industrial</font>]<b>.</b> Being able to simulate the contents of the human brain might allow us to discover these, implement them in a more computer friendly form than 'giant neural net' and have thinking computers that aren't the size of houses. We could create an a program with the resources of simpler examples of consciousness like in mice or monkeys. I'm sure the information gathered from such a study would help us emulate larger systems networks. I know that there are mountains of write up about this all over the place. Are you talking about evolution or self learning by analyzing data ??? Both have been done. <b>Even mixing of learned software knowledge has been done</b>[<font color=red>industrial</font>]<b>.</b> One program did teach the other and they were competing against others in a technical test. <b>It does count as parenting, because the code parented with information and teaching to other code</b>[<font color=red>industrial</font>]<b>.</b> I think the topic of raising AI is a very important one for a number of reasons. Consider that a system has a certain potential intelligence and a certain realized intelligence. Human babies have very high potential intelligence, but pretty low realized intelligence. The better you raise them, the more of their potential intelligence they'll normally realize. Or in other words: you need lower potential intelligence to reach a certain level of realized intelligence. Another possible advantage is that it might be easier for us to shape the system's behavior in its environment than in the programming stage. For instance, if we want the system to be human like or at least have some understanding of the human condition, it is likely a good idea to give it human like experiences and upbringing. This could have implications for safety and friendliness. Another example would be that we often find it easier to teach by doing something ourselves (perhaps slowly) and have the pupil learn by imitation and observation, compared to having to painstakingly explain in great detail all the subactions required for that task with only words (and compared to the pupil not having a teacher at all). Of course, in order to be raised in a somewhat human way, the system needs to have a certain minimum potential intelligence. I don't think we currently have anything that meets that requirement. However, if you think about it in a broader spectrum, 'raising' is really the same as 'training', which we already do with all of our machine learning algorithms albeit in narrower contexts. Usually this takes rather primitive forms: we just drop the system in (a simulation of) the prospective final environment and tell it to learn by itself (reinforcement learning) or we give it samples from that environment with correct answers (supervised learning). However, some research is being done into things like imitation learning, curriculum learning and other kinds of interactive learning.<br/><br/><br/>****************************EXAMPLE 47************************************<br/>Number of words:2280<br/>Number of sentences:100<br/>Total civic:0<br/>Total domestic:1<br/>Total green:0<br/>Total industrial:6<br/>Total inspired:1<br/>Total market:8<br/>Total project:2<br/>Total renown:1<br/><a href='https://www.reddit.com/r/artificial/comments/1w8o17'>https://www.reddit.com/r/artificial/comments/1w8o17</a><br/><br/> There is usually not a definite way to evaluate a relevant conjecture. So we have to rely on something like an objective which can act as a substitute for a goal. A subgoal can be thought of as an objective measure of the progress toward a goal. But are these objectives really utility functions ? I would say not always. I have a lot of problems with building definitions or programs on a concept like a utility function when I am really thinking of something else. My opinion is that good AI or AGI needs to build knowledge up from numerous relevant relations. <b>These relations can then be used as corroborating evidence for basic knowledge</b>[<font color=red>industrial</font>]<b>.</b> <b>For instance, if you wake up from a drugged stupor and you think you might have been shanghaied onto a ship, the metallic walls could stand as corroborating evidence because most ships have metallic walls and most homes don't</b>[<font color=orange>project</font>]<b>.</b> <b>I just don t see this sort of evidence as if it were some kind of utility function</b>[<font color=red>inspired, </font> <font color=violet>market</font>]<b>.</b> Now under different circumstances a sailor might have a much extensive knowledge of what the inside of a ship looked like and that kind of knowledge might be expressed in Bayesian probability or other weighted reasoning. But for projective conjectures the projection of confirming and non-confirming evidence is going to be used relatively crudely and the nuances of weighted reasoning will only interfere with the accumulation of evidence about a conjecture. While we use knowledge of those things that are familiar in our imaginative conjectures, that does not mean that an excessiveness of false precision that weighted reasoning can produce will be very helpful. We need to examine the circumstantial evidence and so on but we should not tighten our theories down until we have stronger evidence to support them or a good reason to act on them. I believe that the next generation of AI and AGI should be built on reason based reasoning and supporting structural knowledge. While methods that are derived from projected statistical models can be useful even when the projections are extreme and not bound by solid statistical methods, I feel that better models can be built using structural knowledge. Eventually this kind of structural knowledge could (in my theory) be used to narrow in on good candidates to interpret what is going on. But these decisions shouldn t be considered to be the same as utility functions because the concept carries some definitions that my more general sense of structural knowledge doesn t. <b>Is Something Like a Utility Function Necessary? (Op Tech)</b>[<font color=red>industrial</font>]<b>.</b> But are these objectives really utility functions ? But these decisions shouldn t be considered to be the same as utility functions because the concept carries some definitions that my more general sense of structural knowledge doesn t. Can you give a definition of what you mean by a utility function? For me it is a function that maps actual, complete world states to utility scores. This function could be very simple or very complex, and the system might not know its definition (i.e what states are beneficial) or how to achieve those states. It seems to me that any AI system needs motivation. <b>Without that, why would it ever do anything? A utility function provides motivation</b>[<font color=violet>market</font>]<b>.</b> The system constantly wants to optimize utility, and chooses actions based on what it believes will accomplish that. If you have alternative ways of motivation that cannot be captured as a utility function, I'm very interested in hearing about them. How the system optimizes its top level utility function (which I'll call drive) is a different matter entirely. I feel like most of your post has absolutely nothing to do with utility functions, but rather with how we should optimize them, while denying that we need them and offering no alternative for motivation. I don't know about your definition, but in mine evidence and utility functions are completely different beasts. Evidence is a relation between beliefs that has to do with truth consistency and a utility function is a function over world states that has to do with utility value. It will probably make sense for the system to create subgoals. You could probably define those in terms of utility functions as well, but the only utility that the system 'cares' about comes from its drive. I think it is important to distinguish between the different concepts of 'progress towards a goal', 'evidence that a goal has been achieved', and 'expected utility that accomplishing this goal will (help) generate'. You are saying that the expected utility that might be attained by accomplishing a goal can be evaluated. We can think about economic intangibles for example, but that does not mean that these can always be accurately measured. A motivating force would be part of what I would call the structural knowledge associated with a concept but it would not necessarily be something that was measurable. My definition of an objective or a subgoal is something that can be recognized or measured and can act as a substitute for the measure (or sense) of the progress toward the achievement of a greater goal. In the way a measurable expected utility can be used against some weighted reasoning, an objective or subgoal can be used to recognize the progress towards various kinds of goals. Looking at I would point out that the similarity comes from the point, 'It was recognized that one can not directly measure benefit, satisfaction or happiness from a good or service, so instead economists have devised ways of representing and measuring utility in terms of economic choices that can be counted.' I suspect that this critical need for indirect measurement is the motivation behind using utility functions in probabilistic AGI theories. Motivating factors are important in AGI, but they too have to be programmed or acquired. That means that the acquisition of a utility function could be circular in relation to some goal. The idea that a utility function 'maps actual complete world states to utility scores' is an error of presumption. You are casually defining something using an idealized perfection. People usually brush verbal excess off as figures of speech but if I am defending against the use of an unattainable ideal that is to be used as a motivating force then the presumption of the definition is very relevant. I am talking about the necessity for an AGI program that has to learn about the IO world that it interacts with. How is that kind of program going to understand 'actual complete world states,' or acquire a numerical function to act as a representative of that kind of ideal. So I am now saying that perfect ideals are not necessary for an AGI program that is supposed to be designed to learn enough about its IO 'world' so that it can start to interact wisely with it (if that 'world' contains some regularities and so on.) A motivating factor is a goal. <b>If I can get my limited AGI program working I can get someone to pay me to continue to work on it</b>[<font color=violet>market, </font> <font color=orange>project</font>]<b>.</b> The stated motivation is reasonable even though it is circular (in more ways than one surprise to me ). So right here I can use meta logic to begin looking at ways to restructure the motivation so that it might be used within some better obtainable objectives. (The fact that it is a circular definition should set some warning alarms off. However, even though it is circular it also involves a conditional interaction with the 'world' so it is not completely circular.) I appreciate your comments but I honestly feel that some of your definitions are a little excessive. <b>But, the basic sense of utility that you seemed to working from does make sense to me</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> The achievement of a goal usually has some utility outside of the work or the attainment of the goal itself. You cannot completely eliminate the utility relation (in some form) from an AI AGI program. <b>But I was saying that you do not need a utility function for AGI</b>[<font color=violet>market</font>]<b>.</b> <b>it is a function that maps actual, complete world states to utility scores</b>[<font color=red>industrial</font>]<b>.</b> This seems to be the definition that is closest to one attributed to Russell and Norvig. However, it is clear that people use the concept in different ways. My main intention was that a numerical scoring function does not need to be used in AGI. Although a system of well integrated view points might act in a way that could be expressed as a scoring mechanism, there is no good reason why it should be done that way for AGI. My opinion is that the utility function is another short cut that is used in lieu of deeper insight about producing artificial judgment. <b>I am a layman, but I think your concept of a utility function may be a bit confused</b>[<font color=violet>market</font>]<b>.</b> I don't think a utility function means the same thing as 'objectives', it's more of a way of measuring effectiveness. It describes the usefulness of an agent's actions in relation to it's objectives. So even a non intelligent agent could be described with a utility function, even if it does not 'know' what the function is. On the other hand, I think an intelligent agent definitely needs some internal model of it's own utility function. Not having a utility function as part of a self model means that the agent cannot place any particular value on previous actions and potential actions. Without that kind of evaluation, actions can only be selected by some hard wired set of reflexes, or randomness, both of which are hard to see as intelligent. <b>A non intelligent agent could be assigned a utility function</b>[<font color=red>industrial</font>]<b>.</b> The strongest use of utility functions are found when they are assigned for narrow AI or even non AI programs. <b>If an AGI program is has to discover the utility functions that it needs then the application of the utility function is going to be a learned evaluation</b>[<font color=red>industrial</font>]<b>.</b> <b>This means the utility function would have to be an imperfect piece of knowledge in need of refinement</b>[<font color=red>industrial</font>]<b>.</b> (It might be considered an ideal, but it could not be a perfect ideal because it is made of the same stuff that any other piece of learned knowledge is made up of.) Although a particular learned utility function might be very effective In an AGI program, there is no guarantee of that and one of the problems in contemporary AGI is that many processes that work well in some situations tend to work miserably in most situations. So it is kind of hard to see how a utility function that had to be learned could be considered to be reliably effective. So yes, an AGI program needs to have some kind of internal model to estimate how well a selection of acquired processes might work in a particular situation. The estimate of the effectiveness of a learned action in a possible situation is a like a utility function. However, as I said, when you look at how weak contemporary AGI models are, you have to begin to wonder if models which claim that an estimate of effectiveness can be used to filter out effective from ineffective operations is reasonable given the problem that the utility functions have to be learned from a complicated IO data environment along with the theories that they are supposedly governing. There is no little irony there because that means that an estimate of effectiveness which is supposed to shape knowledge might itself be woefully ineffective. Most learning theories are based on methods to build on previous learning. But most AI and AGI theories have not been able to display what we would consider to be human like intelligence even given large memory stores. <b>So I say there is something fundamental missing in contemporary theories about AI</b>[<font color=orange>renown</font>]<b>.</b> My opinion is that knowledge has to be heavily integrated and this can only take place through a gradual shaping using trial and error. However, this incremental process can be leveraged through what I call structural learning. When a few new pieces of information help pin a structure of knowledge together so that many insights can be derived at that point, incremental knowledge can take a small leap forward. So while using the building on previous learning model is of course right, it is not what is missing from contemporary AI AGI models. This structural model where well integrated knowledge suddenly holds together and many new insights can be derived does not have to (and I think cannot) be built (only) on utility functions. Because there are critical times when the old utility estimates have to be discarded because they simply are not adequate for the job at hand. <b>The utility relation may be a necessity of an AGI program but I do not believe the utility function is</b>[<font color=violet>market</font>]<b>.</b> The estimate of how well a learned process might work in a situation does not have to depend on a numerical function. In computer science any program can be likened to a mathematical function but in there is also a more intuitive practical sense that some sub programs are not mathematical functions. <b>On the other hand I am definitely not saying that something like utility functions should not be used</b>[<font color=violet>market</font>]<b>.</b> An estimate of the usefulness of a learned process or action needs to be used. If the estimate was perfect it would not be an estimate. But in AI and AGI functions or processes that work well in a narrow kind of situation do not work well in most other situations. So many times the utility function has to rely on a substitute like an objective or a subgoal. I am talking about AI programs that are capable of true learning that can go beyond board games and like that. In fact the utility function is defined in the terms of an objective measure that is different from the attainment of the goal. So what I am saying is that these objectives or substitutes do not have to be based on metrics. But I see what you mean, there is a utility evaluation in the effort to choose which process the program should use in a particular situation.<br/><br/><br/>****************************EXAMPLE 48************************************<br/>Number of words:136<br/>Number of sentences:8<br/>Total civic:0<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:1<br/>Total inspired:0<br/>Total market:2<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1w8zve'>https://www.reddit.com/r/artificial/comments/1w8zve</a><br/><br/> <b>Google Acquires Artificial Intelligence Startup DeepMind For More Than 400M</b>[<font color=red>industrial</font>]<b>.</b> DeepMind was founded by neuroscientist Demis Hassabis, a former child prodigy in chess, Skype and Kazaa developer Jaan Tallin, and researcher Shane Legg. His blog is located ( Also possibly relevant: Jaan Tallinn is a cofounder of the ( at Cambridge and a major donor to the ( (MIRI; formerly known as the Singularity Institute). Didn't they just acquire a home automation company as well? This can't will definitely end well. And a bunch of robotics start ups, including ( One of the things they plan on doing is distancing themselves from future military contracts, which many of these companies were using prior to acquisition. <b>I want to see a conference from them giving at least some indication as to what it plans</b>[<font color=violet>market</font>]<b>.</b> <b>Demis Hassabis just seems successful at everything he does</b>[<font color=violet>market</font>]<b>.</b><br/><br/><br/>****************************EXAMPLE 49************************************<br/>Number of words:1495<br/>Number of sentences:66<br/>Total civic:0<br/>Total domestic:1<br/>Total green:0<br/>Total industrial:14<br/>Total inspired:1<br/>Total market:1<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1wn5km'>https://www.reddit.com/r/artificial/comments/1wn5km</a><br/><br/> <b>As we learn that intelligence requires brain + body, we must rethink AI based purely on modeling the brain</b>[<font color=red>industrial</font>]<b>.</b> There have been a number of responses from academics and researchers regarding this Embodied Ai approach. <b>+ Ben Goertzel et al from opencog have suggested that if you have a laptop, the agents 'body' is the laptop itself</b>[<font color=violet>market</font>]<b>.</b> <b>A keyboard is a narrow, low bandwidth sensation of the world</b>[<font color=red>industrial</font>]<b>.</b> <b>In this case, Embodiment is reduced to a (more tractable) problem of high bandwidth sensory input</b>[<font color=red>industrial</font>]<b>.</b> + Rolf Pfeifer (of Zurich) has characterized the embodied Ai as being a problem of Sensory Motor coordination. 1) That there must be integration across modalities,that is to say, vision alone is not sufficient. <b>2) Symbols that correspond to objects are not indexically stored in the agent's memory, but emerge from the dynamics of modal integration</b>[<font color=red>industrial, </font> <font color=red>inspired</font>]<b>.</b> By and large, our robotic agents do not have a body covered in tactile sensors, which is a technology problem rather than a research problem per se. <b>These means an entire modality is cut off from this research program</b>[<font color=red>industrial</font>]<b>.</b> + The problem of navigating purely by vision has been tried by anumber of researchers, some of which were funded by DARPA. That is, navigating in natural environments using only vision is an entirely un solved problem in robotics. The problems arise because distant objects must be identified and this is a vision problem that can only become tractable via the understanding of context scene. While neural networks trained with backprop work wonderfully on flat road signs, our algorithms still are terrible at identifying 3D objects that have been rotated in space. Because this problem is so excruciating, the mainstream community has resorted to arming their agents with 70,000 laser range finders. The philosophy of Embodied Ai is summarized in the most roughshod way with the following motto: That all these other severe problems in Ai ultimately reduce to our neglecting the problems listed above. That is to say, even esoteric problems with semantics in NLP ultimately reduce to the problems of agency in 3D space, in a body, in time. Embodied Ai makes a promise that solving these low level physical issues will ultimately bootstrap to larger successes in all areas of cognitive Ai. <b>Inverse opticts is probably even unsolvable by means different from the hilbert space interpretations constructed by our brain</b>[<font color=red>industrial</font>]<b>.</b> However, considering recent success of the NN community to train and deploy models in a very short time using CUDA powered parallel processing, I still feel hope that we can tackle this problem with neural nets at some time in the future. while I do agree, that they thereby circumvent the problem of navigating by vision, they do not necessarily cheat. The way I get it, Pfeifer and Schleier (and other researchers advocating embodied approaches) constitute, that an internal world model can be aquired using sensors very different from those humans are equipped with. Unless we develop very tiny tactile sensors we can place all around the robots body, we in fact must resort to other input channels that can substitute the missing information. In fact, the world model created in this way could even be completely different from ours it wouldn't really matter as long as we can communicate about the world surrounding us.  u moschles: I am a sucker for your responses, keep up the good work edit: typo I think too much emphasis in Artificial Intelligence is placed on trying to emulate human traits or passing a Turing test. Why does a machine have to solve a problem like a human? Watson does not 'think' like a person, but it is extremely effective at its task (playing Jeopardy). The true measure of a machine should be how effectively it completes its task. <b>I think too much emphasis in Artificial Intelligence is placed on trying to emulate human traits or passing a Turing test</b>[<font color=red>industrial</font>]<b>.</b> Perhaps in this sub, but I assure you that isn't the case in the AI community at large. That isn't to say that the AI community isn't myopic in other respects though. I think our primary focus should be on performing worthwhile tasks. If attempting to replicate human intelligence helps us achieve that goal, then great, but I'm not tethered to that approach (nor are the vast majority of AI researchers). <b>I think running every possible equation algorithm will require infinite time</b>[<font color=red>industrial</font>]<b>.</b> It's hard to go from this idea to something practical, because it involves selecting a finite (small) subset of algorithms capable of intelligence, and this requires describing them. Essentially, your idea doesn't seem to bring us much closer to a solution. However, you might be interested in looking at Marcus Hutter's research on AIXI where he is also trying to use an incomputable idea as a guide towards designing more practical AGI. My view is that comments here are missing the point. A body is not necessary to think but it is necessary to learn. should be about learning as we can't possibly program a machine to think but we can program it to learn (how to think). Are experts certain that we can't program a machine to think? I think I can't agree with that. An essential problem that we need to address is the ability to learn in a partially observable temporal environment. <b>This will involve more then using supervised learning methods to infer functions from a static set of training data</b>[<font color=red>industrial</font>]<b>.</b> This will also requires methods of non monotonic reasoning for actively updating one's beliefs positions as new sensory data is received from the environment. <b>edit: When I first responded, I assumed that my point would follow naturally from just the name, but the downvotes have taught me a lesson</b>[<font color=purple>domestic</font>]<b>.</b> What I meant is that intelligence doesn't require a typical human body. A person with very little motor or sensory capability can still be intelligent, and can even be one of the most intelligent people alive. Many of today's robots can sense and move in more way than he can.  \ VisRecog Finds for Verb a Direct Object \ EYE \ EAR \ \ Sentence \ \ ( 'I SEE A.' ) \ channel. t | V ( English ) \ Verbs \_ _ V_V \ \ \ Psi ( English Nouns ) \_ \_ The requirement for modelling both brain and body in AI has imposed severe limitations on just how far a non roboticist, independent AI researcher like myself can go in developing Strong AI. For instance, the above ASCII mind diagram for the ( visual recognition module in ( AI displays how a thought can be generated either from the memory of the AI or from current sensory input if the ( is embodied in a physical robot equipped with a camera for visual input. Right now on 31 January 2014 the free, open source AI Minds in ( in ( and in ( can make only one response to the user input question, 'What do you see?' In English or German or Russian, the AI Mind is forced to answer, 'I SEE NOTHING' for lack of robot embodiment. In other respects, these AI Minds are quite sophisticated. If you first tell the AI Mind, 'Boys play games' and a little later you input, 'John is a boy,' the AI will silently make an ( and then seek confirmation (or refutation) from you by asking, 'Does John play games?' The currently primitive AI Minds desperately need robotic embodiment for the sake of ( and for ( If emotion is a physical warping of otherwise pure reason, physical robot mechanisms (heartbeat; tears; muscle constriction; etc.) are necessary to engender emotional feelings. <b>Likewise, for an AI to become self aware and report its own ( the AI needs a closed loop from its robot body with motor actuators, out into the external world, and back in through the robotic sensory instruments to the seat of machine intelligence</b>[<font color=red>industrial</font>]<b>.</b> There's no reason that it would need a physical body, but you could project the seat of it's intelligence into a virtual body in a virtual world so that it can distinguish between itself and everything else. I don't think there's a need for virtual tear ducts, unless your goal is to make an artificial human. <b>An artificial intelligence doesn't necessarily need to be a human intelligence</b>[<font color=red>industrial</font>]<b>.</b> As well the specifics of individual muscle group contractions can be boiled down to the position of actuators, rather than mimicking human features. Any success with a prototype or demo software? Agree, but I would argue it that intelligence requires brain and feedback in form of information in general as well as the capability to act upon the feedback. <b>So body has many sensors that provide the feedback</b>[<font color=red>industrial</font>]<b>.</b> The brain provides a small pulse to the muscle and then measures what comes back, then adjusts based on that feedback. So the intelligent mechanism needs to be able to act and to measure. <b>The brain also predicts, it sends out an pulse for action and then calculates the expected feedback</b>[<font color=red>industrial</font>]<b>.</b> <b>If the sensor feedback from the body varies from the expected result we as humans are surprised</b>[<font color=red>industrial</font>]<b>.</b> This can be observed when you wlak in the dark think there is a step and there isn't, this can have nasty consequences as the body started the program to climb a step.<br/><br/><br/>****************************EXAMPLE 50************************************<br/>Number of words:214<br/>Number of sentences:14<br/>Total civic:0<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:3<br/>Total inspired:0<br/>Total market:0<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1xkcqm'>https://www.reddit.com/r/artificial/comments/1xkcqm</a><br/><br/> I have checked a few AI learning resources including MOOCs, and based on them, finite and discrete mean the same in terms of the environment. My understanding about discrete is based on what I learned in math, statistics, and electrical engineering; based on my understanding, there can be an infinite number of discrete states, which contradicts with finite. It will be appreciated if anyone helps me understand this. In terms of the dictionary definition, think of it like this: an infinitely long ruler is divided into inches. Each inch is discrete and also finite BUT there are an infinite NUMBER of inches on the ruler. <b>The individual measurements are discrete, but the number of measurements isn't</b>[<font color=red>industrial</font>]<b>.</b> Having said that, there are probably domain specific definitions for these terms that might be slightly different, and that may be what is tripping you up. I think it was indeed the domain specific definition issue that confused me. <b>Discrete games can have an infinite number of states</b>[<font color=red>industrial</font>]<b>.</b> So saying that there is a discrete environment is not the same as saying there is a finite environment. Finite and discrete are not the same for the reason you explained. <b>You can have an infinite number of discrete states, so not all discrete environments are finite</b>[<font color=red>industrial</font>]<b>.</b> However, all finite environments are discrete, so I assume that's where the confusion comes from.<br/><br/><br/>****************************EXAMPLE 51************************************<br/>Number of words:2167<br/>Number of sentences:99<br/>Total civic:1<br/>Total domestic:4<br/>Total green:0<br/>Total industrial:10<br/>Total inspired:0<br/>Total market:5<br/>Total project:4<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1xp1ah'>https://www.reddit.com/r/artificial/comments/1xp1ah</a><br/><br/> Here's some additional context from Surfaces Essences. <b>The particular passage that Hofstadter has been monitoring on Google Translate: Original paragraph from Le Monde, September 2004: Parfois, le succ s ne fut pas au rendez vous</b>[<font color=red>industrial</font>]<b>.</b> On a beau y penser tr s fort, le bon num ro ne sort pas forc ment. Sagan prenait les checs d auteur dramatique comme les revers de casino, avec respect pour les caprices de la banque et du ciel. Il faut bien perdre un peu, pour mieux savourer la gagne du lendemain. <b>Qui ne l a pas vue r cup rer en quelques quarts d heure les pertes de toute une nuit ne peut comprendre comme c est joyeux de narguer le sort</b>[<font color=red>industrial</font>]<b>.</b> <b>Google s translation engine, September 2004: Sometimes, success was not with go</b>[<font color=red>industrial</font>]<b>.</b> One thinks of it in vain very extremely, the good number does not leave inevitably. Sagan took the failures of dramatic author like the reverses of casino, with respect for the whims of the bank and the sky. It is necessary well to lose a little, for better enjoying gains it following day. Who did not see it recovering in a few fifteen minutes the losses of a whole night cannot include understand as they is merry of narguer the fate. <b>Google s translation engine, April 2009: Sometimes, success was not there</b>[<font color=red>industrial</font>]<b>.</b> It was nice to think very hard, the proper number does not necessarily spell. Sagan took the failures as a dramatist such as backhand casino, with respect to the whims of the Bank and the sky. It must be losing a little, better enjoy the gains overnight. <b>Who did not see recover in a few minutes lost a whole night can not understand how happy it is the sort of taunt</b>[<font color=purple>domestic, </font> <font color=orange>project</font>]<b>.</b> Google Translate (( auto en Parfois 2C 20le 20succ C3 A8s 20ne 20fut 20pas 20au 20rendez vous. 20On 20a 20beau 20y 20penser 20tr C3 A8s 20fort 2C 20le 20bon 20num C3 A9ro 20ne 20sort 20pas 20forc C3 A9ment. 20Sagan 20prenait 20les 20 C3 A9checs 20d E2 80 99auteur 20dramatique 20comme 20les 20revers 20de 20casino 2C 20avec 20respect 20pour 20les 20caprices 20de 20la 20banque 20et 20du 20ciel. 20Il 20faut 20bien 20perdre 20un 20peu 2C 20pour 20mieux 20savourer 20la 20gagne 20du 20lendemain. <b>20Qui 20ne 20l E2 80 99a 20pas 20vue 20 C2 AB 20r C3 A9cup C3 A9rer 20 C2 BB 20en 20quelques 20quarts 20d E2 80 99heure 20les 20pertes 20de 20toute 20une 20nuit 20ne 20peut 20comprendre 20comme 20c E2 80 99est 20joyeux 20de 20narguer 20le 20sort.)): Sometimes the success was not at the rendezvous</b>[<font color=red>industrial</font>]<b>.</b> It was nice to think very hard, the right number is not necessarily fate. Sagan took failure as a playwright setbacks casino, with respect to the whims of the bank and sky. We must lose a little, to better savor the win tomorrow. Who has not seen 'recover' in some quarters of an hour losses overnight can understand as it is joyful taunt fate. ( ( I could not make head or tail of any of the Google translations, but once I read Hofstadter's translation, all of a sudden, the Google translations seemed to make sense, and even began to feel pretty close. To avoid this hindsight bias, I hid the translation and commentary. <b>What were your reactions? I see you guys going at each other in the comments but can any of you respond directly to this example that Hofstadter gave here: `ABC` is to `ABD`, as `PPQQRR` is to _ (what?) Every single one of you feel the right answer is `PPQQSS` Now (instead of being stooges defending google and smartphone apps) answer my question and rise to Hofstadter's challenge</b>[<font color=violet>market</font>]<b>.</b> What is it about you and your human mind that 'knows' `PPQQSS` is the right answer? Can you quantify that? Have any of you even tried? When does the proverbial 'lightbul come on in your mind and you suddenly see 'the rule' that is being used and then apply it? If you cannot answer this question, then how can you claim that you know anything about thinking? I appreciate all the fancy technology that has popped up recently from statistical regression schemes in machine learning (good job, team ). <b>However, if you cannot rise to Hofstadter's challenge here, then his claim that you are not working on the laws of thinking will be a shoe that fits</b>[<font color=purple>domestic</font>]<b>.</b> Can you quantify that? Hell, yes ( is based on exactly that quanitification Have any of you even tried? Thanks for the opportunity for ( The problem with copycat has always been the same. It works good on toy problems like this, but has never been shown to work at a much larger scale. Infact, can it be scaled up in the first place? AFAIK, the concepts, codelets, etc. <b>that copycat uses have to be carefully programed manually</b>[<font color=red>industrial</font>]<b>.</b> I have issue with the whole thing being a true scotsman fallacy. I could come up with questions that stump humans and not machines.'Real AI' is not a term used in the field. And it seems like the distinction is being used to denigrate the achievements made. <b>If you decide that humans and computers have to function exactly alike to be considered intelligent then you are over describing</b>[<font color=orange>project</font>]<b>.</b> I could come up with questions to stump the robot in 'Too Human' but generally most people would call that full, human like general AI. Watson didn't use a database of trillions of questions and answers fed to him manually with a simple look up mechanism. And that seems to be what the big headline suggests. It occurred to me as the first answer but when he mentioned ABD, and PPQQRD as possible answers, both made sense to me. It seems clear in the first paragraph that Hofstadter's definition of AI includes an agent that can reason semantically, that can 'think' and 'comprehend.' A machine that can 'understand what it's reading' would pass that test. If you would take issue with the article, then maybe you can argue with his criteria for AI, but to say that he doesn't offer a definition of 'real' AI is a misreading. I think you have to move toward much more fundamental science, and dive into the nature of what thinking is. What is understanding? How do we make links between things that are, on the surface, fantastically different from one another? In that mystery is the miracle of human thought. The people in large companies like IBM or Google, they're not asking themselves, what is thinking? They're thinking about how we can get these computers to sidestep or bypass the whole question of meaning and yet still get impressive behavior. Did you miss that whole section? Yes, he doesn't offer the ultimate answer of what thinking and understanding are, but he clearly is saying that real AI research is in pursuit of the question of what thinking really is. They are not real AI because they don't 'think' and 'learn'. <b>All they do is complete whatever the code tells them to complete depending on the variables</b>[<font color=red>industrial</font>]<b>.</b> They can't decipher between a wrong answer or a right one. Watson looks for keyword and expressions to look for relevant information even if it's not quit right. Real AI would be able to distinguish what is relevant and what is not depending on the situation. <b>There was a couple times on Jeopardy where Watson simply gave the wrong answer or it was too broad</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> ( is real AI, because it uses concepts modeled after real ( Now listen, you twerps and perps who have been downvoting ( You don't know whom you are dealing with. <b>Whatever your evil purpose is you are probably running some sort of ( scam it is counterproductive for you to downvote the ( It is 2014 do you know where your ( is? In the Titanic struggle to spread the memes of ( across the web and woof of cyberspace, we counteract your ( thusly</b>[<font color=orange>project</font>]<b>.</b> ( contains un down votable permalinks that render your petty downvotes meaningless. O tempora, o morons One of the finest explanations of how something may appear intelligent without truly understanding is Searle's ( thought experiment. However, if we consider understanding to be an emergent phenomenon, making things faster and appear more clever might be just the right way to go about creating artificial conscience. can't you use the same chinese room argument with the human brain? it's just chemical instructions interacting with each other, the chemicals themselves have no understanding i've always thought the Chinese room experiment was bs I think the Chinese Room is an illustration of how we can make mistakes in our thinking, but it doesn't offer a positive account for how consciousness works. Elsewhere Searle claims that consciousness is a state: neurons are in a state of consciousness just like H2O is in a state of ice when it's really cold, which is definitely an emergent phenomenon. Contrary to what most think, Searle doesn't believe that machines can never be conscious, only that symbol manipulation isn't sufficient to create meaning. Google Translate is developing and it's making progress because the developers are inventing new, clever ways of milking the quickness of computers and the vastness of its database. to the 'technically true, but misleading and irrelevant because the alternative is simply infeasible for now'. They're not studying the mind and they're not trying to find out the principles of intelligence, so research may not be the right word for what drives people in the field that today is called artificial intelligence. One can disagree with and criticize the currently dominant research paradigm without turning into strawman building demagogue. The statement made about Google Translate is completely correct. <b>Google Translate isn't improving due to advances in its ability to understand language in the way Hofstadter (or most of us, in fact) mean</b>[<font color=red>industrial</font>]<b>.</b> Regarding the second quote you pulled, Hofstadter is still correct, although the wording is maybe a bit snippy. Certainly people doing machine learning research are still doing research just not in the field that he prefers to call Artificial Intelligence. I personally work in Machine Learning, and I simply have different goals than Hofstadter. He's working on understanding and building something that meets that fuzzy notion we have of 'being intelligent in the same way that humans are'. I'm working on solving specific and narrowly defined problems with the aid of algorithms that can draw inferences from large amounts of data. One side effect of that is that machine learning is unbelievably practical right now, and 'AI' in the sense he means it is almost utterly worthless commercially. <b>So it's not too far off the mark to say that people like me are doing product development</b>[<font color=violet>market</font>]<b>.</b> It's not completely fair to the research field as a whole, but if you work on Google Translate, you are doing product development. <b>You don't care if your method is biologically plausible; you care only whether it makes the product better</b>[<font color=violet>market</font>]<b>.</b> I don't think it's fair to call his position a strawman. The reason it's messy is that machine learning can get close to AI. <b>Normal people can watch Watson win at Jeopardy and think that true AI must be right around the corner, and if your goal is to build true AI, that's probably immensely frustrating</b>[<font color=orange>project</font>]<b>.</b> Can you explain how he is factually wrong about Google Translate? What are they doing that isn't 'inventing new, clever ways of milking the quickness of computers and the vastness of its database'? 'technically true, but misleading and irrelevant because the alternative is simply infeasible for now' What do you mean with 'the alternative'? Building fully functioning artificial minds is indeed infeasible for now, but that's not what Hofstadter is suggesting. Of course it will stay infeasible until we've studied enough to know how to do this, and that is what he's suggesting. He's also suggesting that Watson and Siri aren't helping with that. He's not saying that Watson and Siri aren't good products or that they shouldn't have been built. <b>He's just saying that they shouldn't be confused with what people like him are doing, which is what he calls 'AI research'</b>[<font color=blue>civic, </font> <font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> This is EXACTLY repeating the old Chomsky vs Norvig debate. Why can't they (NC or DH) not just accept that probabilistic models give you some nice advantages like robustness, error handling, etc.? Because they are so entrenched in their obsession with a 'perfect' model, they simply cannot accept the more black box like approach taken right now, I guess. Good I can be a fan of all three of them as a sentient, thinking human being. Maybe we should have a further distinction: real vs unreal artificial intelligence. And then, five years later someone goes: sorry, but this just isn't authentic unreal artificial intelligence. This thread has been linked to from elsewhere on reddit. Comments? Complaints? ( I'm responding to people on the popularmechanics website who left comments. Here is one: This seems to be missing the point of AI. <b>It's not to simulate human thinking perfectly That's superficially correct, but if your goal is to translate languages automatically, you cannot sidestep thinking like a human</b>[<font color=red>industrial</font>]<b>.</b> On top of meaning, a translator must also understand things like the speaker's intentions. Plus, this guy has a very human centric vision of intelligence. Intelligence can exist in a lot of different forms, and machine intelligence will for the foreseeable future have different strengths and weaknesses than humans. <b>But if your goal is automatic machine translation, you cannot go on forever playing tricks to avoid the thorny problem of semantics</b>[<font color=red>industrial</font>]<b>.</b><br/><br/><br/>****************************EXAMPLE 52************************************<br/>Number of words:14<br/>Number of sentences:2<br/>Total civic:0<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:0<br/>Total inspired:0<br/>Total market:0<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1xtyfo'>https://www.reddit.com/r/artificial/comments/1xtyfo</a><br/><br/> BBC discusses methods companies are adopting for improving AI in digital assistants like Siri.<br/><br/><br/>****************************EXAMPLE 53************************************<br/>Number of words:1424<br/>Number of sentences:65<br/>Total civic:0<br/>Total domestic:2<br/>Total green:0<br/>Total industrial:10<br/>Total inspired:3<br/>Total market:2<br/>Total project:1<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1y3cog'>https://www.reddit.com/r/artificial/comments/1y3cog</a><br/><br/> The author seems under the mistaken impression that if something doesn't work exactly like the brain, it can never lead to AI. Personally I also don't think that deep neural networks by themselves are the most practical way towards general AI, although I wouldn't be surprised if they are among the most successful approaches for visual or auditory components. So I suspect it might very well be useful to AGI research, and it is certainly useful in some narrower applications (just like symbolic AI actually, so I guess the author is right about that). There is no doubt there will be application for deep learning to vision. The blogger could have saved a lot of time by saying this at the beginning. <b>I just think it is a mistake for the high tech industry to put so much money into one basket</b>[<font color=violet>market</font>]<b>.</b> AI research has seen upheavals in the past and it will happen again. Current approaches to deep learning are almost certainly wrong in the sense that they will be supplanted as soon as something better comes out. No, it rather seems that the author is addressing the fact that ANNs have not 'suddenly' aquired intelligence by scaling them to 'google esque' sizes, as the current PR machinery would make us believe. They still have the same old limitations ANNs had in the early 80s, but by scaling them, they can be used on more complex inputs, like house number recognition. <b>At the same time, they are extremely limited the house number recognition system cannot, for example, OCR other types of numbers</b>[<font color=red>industrial</font>]<b>.</b> <b>This ability to create abstractions is key to real intelligence</b>[<font color=red>industrial</font>]<b>.</b> And the author is saying this will not change unless we create models that have a closer fit to the actual biology. Last, I would add, this not only the author's opinion, even scientific leaders like Markram (head of The Human Brain Project) firmly defend exactly this belief. Vehicles with wings and a motor will not lead to flight as that is not how birds do it. The right level of abstraction is that both birds and airplanes obey the same principles of aerodynamic flight. When it comes to the intelligence and the brain, the right level of abstraction deals with transient sensory signals occurring at precise times. Deep learning neural networks don't care about timing and that's probably their main weakness. <b>For starters, generec is a biologically plausible neural network implementation</b>[<font color=red>industrial</font>]<b>.</b> <b>Additionally, it's very easy to assume things aren't going to get you to 'true AI' but it's not at all productive and we can't know what is not the answer if we don't know what is (barring ridiculous things)</b>[<font color=orange>project</font>]<b>.</b> That idea coupled with the provable potential for hierarchical non linear function approximators if we could just train them with large datasets, makes me think we shouldn't rule them out. Lastly, if you go through this guy's other blog posts you see that he has a bunch of posts where he just tries to shit on some technology idea. At the very least, this guy isn't selling my kind of science philosophy. deep learning will not lead to human like intelligence because this is not the way the brain does it What a poor argument. this is the problem when a computer scientist writes about neuroscience concerning only his statements about the biological brain, only 4 and 6 in his list under Biologically Implausible are correct Now hold on just a minute. <b>Don't just assume he's a computer scientist because he doesn't understand neuroscience</b>[<font color=red>industrial</font>]<b>.</b> His understanding of deep learning is just as flawed (apparently deep learning implies Boltzmann machines graphical models, so he's maybe four years out of date in literature that's only been a big topic for six or so years). Have you looked over the other posts on that blog? I couldn't find a bio ( Here's the thing about AI, lets call it AGI just to be sure we are all on the same page with it being human level AI; no one will think we have achieved it until they think we have achieved it. So much of these debunkings are just another movement of goal posts. There is no best way to test for a truly human level AI, even the turing test is admittedly naiive, and outside of it's specific constraints people argue its usefulness. <b>The leading candidate in my mind is the Anytime intelligence test proffered by J Hern ndez Orallo and even that doesn't give a good range of test environments for evaluation</b>[<font color=red>industrial</font>]<b>.</b> 13 comments so far, and you are all overlooking the strongest, must poignant point that the blogger made. `You either see the cow in the image, or you don't.` This is a full frontal attack on statistical learning methods (which included deep learning as a subordinate topic). Our brain either snaps completely into '100 definitely cow' or if the snap does not happen, we simply see nothing at all '0 cow'. In the case of the Andrew Ng cat detector experiment, they did get hits like '34 cat', '83 human face', and other probabilistic responses. The reason this happens is because the upper layers of our brain which deal in abstract objects also give feedback to the lower layers. <b>So the lower layers feed into the upper ones (upward connections), and the higher layers feed into the lower (downward connections)</b>[<font color=red>industrial</font>]<b>.</b> This feedback causes chaotic neuronal dynamics, up and until the dynamics 'settle into an attractor state'. This attractor state corresponds to 'I see a cow.' This same sort of snapping settling effect happens with the ( visual illusion. Here some research that gets into this aspect of vision in much detail: As I said, the author seems under the mistaken impression that if something doesn't work exactly like the brain, it can never lead to AI. Humans say either 0 cow or 100 cow, whereas the deep net might say something like 60 cow. What the author didn't address is why this is a problem (other than 'it's different'), and if it is, why it can't be fixed with a simple threshold or softmax function. Don't get me wrong: I think you make excellent points. I agree that top down feedback and context are important concepts that (most) deep neural nets don't address. The attractor states are a fancy way of applying a threshold if you are not confident about it (i.e not sure if its a cow), then you dont make a decision Yeah. <b>The brain certainly uses a large number of inputs and creates a running probability</b>[<font color=red>industrial</font>]<b>.</b> <b>You can simulate the boolean response by simply rounding..</b>[<font color=red>industrial</font>]<b>.</b> And over time it requires a higher level of certainty. <b>Which is why you jump when something pops up and you calm after you realize it is just some kid in a mask</b>[<font color=purple>domestic, </font> <font color=red>inspired, </font> <font color=violet>market</font>]<b>.</b> But because faces are important, it gets boosted up to 75 chance. <b>The idea that computers can't be like brains because of some idea that brains have certainty is just</b>[<font color=red>inspired</font>]<b>.</b> <b>This is a full frontal attack on statistical learning methods (which included deep learning as a subordinate topic)</b>[<font color=red>industrial</font>]<b>.</b> Right, I get that, and it seems like an accurate account of the phenomenology of vision. But what reason do we have to believe that this principle generalizes to anything else in cognition? In what other mental phenomenon is there a 'snap' between states? I find it strange that one would try to compare visualization to 'human like intelligence' in the first place. <b>Visualization is merely a tool component that in itself would never have human like intelligence</b>[<font color=red>industrial</font>]<b>.</b> See for example his series of posts about AI and the Bible: Choice quote: 'I get my understanding of intelligence and the brain from certain metaphorical passages in the books of Revelation and Zechariah' He said this in another post: The Bayesian model assumes that events in the world are inherently uncertain and that the job of an intelligent system is to discover the probabilities. The Rebel Science model, by contrast, assumes that events in the world are perfectly consistent and that the job of an intelligent system is to discover this perfection. He called the singularity 'a religion' (irony?) and 'a bunch of nerds'. I keep reading his stuff and it gets worse and worse. The author may be a lunatic but his criticism of deep learning has merit. <b>Although at the same time I feel that what he writes is so vague, or rather superficial, that it's hardly useful to me</b>[<font color=purple>domestic, </font> <font color=red>inspired</font>]<b>.</b> But I suppose it gives some nice angles on how to look at learning. Deep learning has had some success but, in the end, it's just another red herring on the road to true AI. based on your comment history (wherein you pop back up on reddit after being inactive for some years, with your last previous comments being links to the same weird ass blog), i think you are the author.<br/><br/><br/>****************************EXAMPLE 54************************************<br/>Number of words:731<br/>Number of sentences:32<br/>Total civic:0<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:7<br/>Total inspired:0<br/>Total market:3<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1ydeov'>https://www.reddit.com/r/artificial/comments/1ydeov</a><br/><br/> Hey just thought I could share a university assignment I handed in last semester, it's on using a naive bayes classifier for automatically responding to reddit comments. Any suggestions, input, criticism is highly welcome, even if it's negative. I know it's not the most complex approach to this task, nor scientifically very interesting, but maybe someone finds it cool. So enjoy Here's the abstract: This project is an algorithm which in a first step automatically downloads comments from the popular social media site Reddit, and stores them in a format suitable for further analysis. <b>In the second step a classifier is used to calculate which comment from the database suits a previously unseen comment best</b>[<font color=red>industrial</font>]<b>.</b> <b>In a third step posts are automatically submitted to the site and the performance of the algorithm is evaluated based on the upvotes received by the community</b>[<font color=red>industrial</font>]<b>.</b> <b>Download link of the full 10-page PDF: Update: Additional link here: Update: Fugly code here: Using a Naive Bayes classifier to automatically respond to comments on Reddit</b>[<font color=red>industrial</font>]<b>.</b> I'm really bummed out that I can't fork this and look at ways of learning from it and improving it. You are definitely right, will do it first thing when I have the spare time and update here, promised King illegal forest to pig wild kill in it a is P(upvote | king illegal forest to pig wild kill in it a is ) 1.0 Abe Lincoln. Here in England? God, imagine the chaos if you employed this bot full time in r circlejerk. That actually sounds like the optimal case for this bot. Lots of short, repetitive comments and even bad replies wouldn't seem too out of place. Could we create a subreddit especially for this and have just bots post and comment in there? I would love to see what they end up reposting and talking about r test and r botcirclejerk. <b>Maybe we should create another subreddit just for bots like this though</b>[<font color=violet>market</font>]<b>.</b> We could make it a competition like the braveryjerk bot. Should you (or anyone else) run the program (or a comparable program) again, I would suggest to not use a username for the bot that includes 'bot' or something similar, as that would put a bias on the voting. Here are the usernames I choose for the Naive Bayes Classifier, anagrams for Thomas Bayes: u Beams_Stay_Oh (funny) u Embassy_At_Oh (pics) u A_Beasts_Homy (movies) u Aha_Tombs_Yes (worldnews) u Base_Maths_Yo (artificial, although eventually omitted in the analysis because no votes at all) I think theres not really a Bias for people to think that this is bots, and nobody ever suspected it in the comments. As you can see they drew up some anger sometimes, and mostly downvotes. ;) For the random posters I had registered five accounts already, acronyms for Support Vector Machine but in the end decided to use them for random (the SVM failed horribly because of too little time and also bad coding): u Vector_Support (funny) u Covert_Support (pics) u Covets_Prop_Rut (movies) u Crop_Vote_Spurt (worldnews) u Crops_Rev_Op_Tut (artificial, omitted) This thread has been linked to from elsewhere on reddit. Comments? Complaints? ( There is something funny about a bot post on a thread about making a bot it looks like random performs better in some subreddits. <b>I think there's not really enough data to be able to say that</b>[<font color=violet>market</font>]<b>.</b> <b>Oh this one is great: I love this :) When I click the link to the pdf I get a page containing: PermanentRedirectThe bucket you are attempting to access must be addressed using the specified endpoint</b>[<font color=red>industrial</font>]<b>.</b> Please send all future requests to this endpoint.997CC8B1F4DA574Bs3.kkloud.comVB6ScXsSDHaMIf7qoXRN0HdYdKGPcKDs+nPBT6BYTKmyd5PrnLoKAPUFEj 9TEuOs3.kkloud.com.s3.amazonaws.com Here's another link using Dropbox: There should still be a 'Download' link on the left that you should be able to use edit: Actually, interestingly, that download link only works for me in the internet explorer, not in firefox Great work, very interesting. Thanks for sharing Thank you very much for sharing. <b>What class had you create this? Can I check out the code as well? It was for an Advanced AI class last semester</b>[<font color=red>industrial</font>]<b>.</b> <b>Shouldnt take too long :) Please automatically respond to my comment</b>[<font color=red>industrial</font>]<b>.</b> <b> builds java jar ClassifyNaiveBayes.jar pics 25 0 'Please automatically respond to my comment' Reddit Analysis</b>[<font color=red>industrial</font>]<b>.</b> is: ( with prob: 3.2867858E 5 Other responses from the DB's of different subreddits: r artificial: Wow. I never thought I'd personally see Murray in the wild.  r funny: your comment's actually worse than his shitty banana scale comment, Doc. <b> r worldnews: Datamining: Because you're worth it As you can see the results are mostly random</b>[<font color=violet>market</font>]<b>.</b><br/><br/><br/>****************************EXAMPLE 55************************************<br/>Number of words:342<br/>Number of sentences:22<br/>Total civic:0<br/>Total domestic:4<br/>Total green:0<br/>Total industrial:0<br/>Total inspired:2<br/>Total market:3<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/1yl3nn'>https://www.reddit.com/r/artificial/comments/1yl3nn</a><br/><br/> AGI researchers take very seriously the prospect of someone else solving the problem first. They can imagine seeing the headlines in the paper saying that their own work has been upstaged. They know that Nature is allowed to do that to them. <b>The ones who have started companies know that they are allowed to run out of venture capital</b>[<font color=violet>market</font>]<b>.</b> <b>That possibility is real to them, very real; it has a power of emotional compulsion over them</b>[<font color=red>inspired</font>]<b>.</b> I don't think that 'Oops' followed by the thud of six billion bodies falling, at their own hands, is real to them on quite the same level. <b>And read this: ( People look at me like I'm crazy when I say things like this</b>[<font color=purple>domestic, </font> <font color=red>inspired</font>]<b>.</b> <b>Yet, they find it perfectly natural that a biologic parent dies, and the child lives on</b>[<font color=purple>domestic</font>]<b>.</b> I would be entirely comfortable with being replaced by a better species. I'd definitely like to transition into that world, or at least not die painfully. But I'll go happily when Skynet comes for the keys. Natural yes, but losing your parents is a traumatic event in anyone's life. <b>And certainly no one wants to die themselves, regardless of whether or not they are a parent</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> <b>We might accept death because we can't do anything about it, but we are scared and horrified of it when it comes</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> We would prefer to live much, much longer if we could. Maybe that idea sounds good in the abstract, but I don't think you'd choose to die today in order to be replaced by a paperclip maximizer. An AI would be more powerful, definitely, but better? Depends on it's utility function, but ( A paperclip maximizer will go out and fill the universe with paperclips and then kill itself. It won't experience any joy per se, it won't have any compassion for other civilizations that get in it's way or other life that it encounters. The result is just a bunch of meaningless garbage that some programmer accidentally entered into the first AI. It's natural that a biological parent dies, but that doesn't mean it's good. I'd much prefer it if the child and the parents would live.<br/><br/><br/>****************************EXAMPLE 56************************************<br/>Number of words:632<br/>Number of sentences:22<br/>Total civic:0<br/>Total domestic:2<br/>Total green:0<br/>Total industrial:1<br/>Total inspired:2<br/>Total market:2<br/>Total project:0<br/>Total renown:2<br/><a href='https://www.reddit.com/r/artificial/comments/20o9de'>https://www.reddit.com/r/artificial/comments/20o9de</a><br/><br/> I came across this in Godel Escher Bach: Look at your conversations, he says. <b>You ll see over and over again, to your surprise, that this is the process of analogy-making</b>[<font color=violet>market</font>]<b>.</b> <b>Someone says something, which reminds you of something else; you say something, which reminds the other person of something else that s a conversation</b>[<font color=purple>domestic, </font> <font color=red>inspired, </font> <font color=violet>market, </font> <font color=orange>renown</font>]<b>.</b> But at each step, Hofstadter argues, there s an analogy, a mental leap so stunningly complex that it s a computational miracle: somehow your brain is able to strip any remark of the irrelevant surface details and extract its gist, its skeletal essence, and retrieve, from your own repertoire of ideas and experiences, the story or remark that best relates. <b> Beware, he writes, of innocent phrases like Oh, yeah, that s exactly what happened to me behind whose nonchalance is hidden the entire mystery of the human mind</b>[<font color=purple>domestic, </font> <font color=red>inspired, </font> <font color=orange>renown</font>]<b>.</b> After reading, I decided I wanted to get a better look at this phenomenon, so I mentally leapt through a sequence of 'random' thoughts images as quickly as I could, while taking note of the inherent connection they held in tandem with each other. As I watched this process, I noticed that as a thought arose, many little categorical markers would accompany it, the pool of which grew as certain patterns in the thought were noted. Once this pool grew great enough to distract from the initial thought, it would 'spill over' into a thought that shared a large subset of markers. It should be added that emotions guide the subset selection process greatly as well. <b>The ratio of emotion to size in regards to the selection outcome would be difficult to observe as concentration meditation tends to quell emotions</b>[<font color=red>industrial</font>]<b>.</b> Now, I've heard of people who come to realize through meditation, on one experiential level, that 'everything' is cause and effect. Daniel Ingram writes about this realization: .if one is using a mantra, one may notice that at some point one shifts to being able to stay with mantra clearly and perceive it as an object,. Once the mantra is clear, one may notice all sorts of things about the process of mentally creating the mantra, such as the stream of intentions being followed shortly behind by the string of the mantra itself follow slightly behind by the mental echo of the perception of the mantra, making what appear to be three separate streams of the mantra. This is direct insight into Cause and Effect Essentially, my mantra is that phenomenon. If I could observe a mantra closely enough to believe it's just cause and effect, then could I observe cause and effect the same way through this phenomenon? If a scientist could see clearly enough into the workings of the mind that they become convinced that 'everything' is cause and effect, would the mind then be known sufficiently enough for them to make great contributions in the field of artificial intelligence? Has anyone out there already proved this true? Could meditation on thought processes lead to breakthroughs in artificial intelligence?. I have been practicing introspection for the past two years, searching for answers. I keep a journal of all my ideas, and I hashtag them so that I can find groups of relevant ideas later on and piece them together. To aid introspection, I practice lucid dreaming and meditation, and I smoke cannabis frequently. Here is a sample of the types of ideas that I work with: 'Collecting different versions of a type allows abstraction of that type. Abstractions enable induction to travel farther because they reveal the inductive structure of the problem domain. Farther reaching induction allows testing of hypotheses that are further away from known examples, speeding the spread of knowledge through the space in which the examples lie, and allowing quicker ascent into the space of more difficult examples.' Hofstadter relied a lot on introspection and he has contributed a lot toward artificial intelligence.<br/><br/><br/>****************************EXAMPLE 57************************************<br/>Number of words:10<br/>Number of sentences:2<br/>Total civic:0<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:1<br/>Total inspired:0<br/>Total market:0<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/21h1ms'>https://www.reddit.com/r/artificial/comments/21h1ms</a><br/><br/> <b>Can Artificial Intelligence be the Next Big Bet for Developers?</b>[<font color=red>industrial</font>]<b>.</b><br/><br/><br/>****************************EXAMPLE 58************************************<br/>Number of words:962<br/>Number of sentences:40<br/>Total civic:1<br/>Total domestic:6<br/>Total green:0<br/>Total industrial:1<br/>Total inspired:0<br/>Total market:5<br/>Total project:4<br/>Total renown:2<br/><a href='https://www.reddit.com/r/artificial/comments/22inxi'>https://www.reddit.com/r/artificial/comments/22inxi</a><br/><br/> <b>What is the logical fallacy for if it could be done, someone would have done it already ? I can't seem to find it or remember what the name of it was</b>[<font color=purple>domestic, </font> <font color=orange>project</font>]<b>.</b> What is the logical fallacy for 'if it could be done, someone would have done it already'?. The fallacy is just that it's a ( since there could be reasons that no one has done it already. The false premise is the assumption that there are no new ideas. If that statement were true, then nothing would be possible. Because in order for it to have been done the first time, there would have been a time previous when it hadn't been done. And thus if it were possible, it would have been done, thus it's not possible to do. Therefore nothing has ever been done in the history of existence. I believe it is ( The form used here is: If P, then Q. Non Q Therefore, Non P To put this into context of this situation, and adding the implied conclusion, we have: If it could be done, then someone would have done it. <b>Essentially, it doesn't take into account the fact that there may be other reasons it hasn't been done</b>[<font color=violet>market</font>]<b>.</b> <b>For instance, a lack of computing power, speed, or a lack of current understanding</b>[<font color=red>industrial</font>]<b>.</b> You have described _modus tollens,_ which is perfectly logical. Affirming the consequent is the opposite: If P, then Q Q Therefore, P and that's a logical fallacy, but doesn't map well onto what OP asked about. <b>If it couldn't be done up to now, that does not mean that it cannot be done from here on</b>[<font color=violet>market</font>]<b>.</b> The components needed to achieve something cannot be perfectly accounted for, so you can never rule out something because it hasn't been achieved by someone yet who had more resources etc. a large company might have more money and resources, but they're not necessarily spending them to solve a specific problem. Also, money and resources may not be the only ingredients needed you might have a different background, a difference in circumstances and the right timing that will allow you to achieve something that hasn't been achieved thus far. Edit: I believe this falls within this: It proves too much. Before something has been done for the first time, no one has done it already. Worse, since you cannot skip the first time, nothing can be done at all. <b>Sounds closely related to the ( sometimes illustrated ( Two economists are walking along and one says to the other 'Hey, there's a twenty pound note lying on the ground over there, let's pick it up', and the other one says 'No, if there really were money just lying on the ground like that, someone would have picked it up already' That sounds VERY close to what I am thinking about</b>[<font color=orange>project</font>]<b>.</b> I posted this above, but ( What those who tell the first version of the story fail to understand is that an efficient market does not mean that there cannot be a 20 bill lying around. Instead, it is so unlikely to find one that it does not pay to go looking for them the costs of the effort are highly likely to exceed the benefits. Also note that if it became known that there were lots of 20 bills to be found in a certain area, then everyone would be there to compete to find them, reducing the likelihood of achieving an appropriate 'return on investment.' The statement, if taken literally, is false. But it's similar to a general premise in economics, though I can not find a name for it. It's related to ( Also see ( an efficient market does not mean that there cannot be a 20 bill lying around Instead, it is so unlikely to find one that it does not pay to go looking for them the costs of the effort are highly likely to exceed the benefits. <b>Also note that if it became known that there were lots of 20 bills to be found in a certain area, then everyone would be there to compete to find them, reducing the likelihood of achieving an appropriate 'return on investment.' Another way of saying it is that if you are competing against lots of people, unless you have some large advantage, it's statistically unlikely that you will be the one to win</b>[<font color=violet>market</font>]<b>.</b> Also see ( Hundreds, maybe thousands, have thought they solved AI. <b>I am asking because I have already looked but there are so many of them that it is difficult to figure out quickly</b>[<font color=purple>domestic, </font> <font color=orange>renown</font>]<b>.</b> <b>That is why I asked, so that someone that knew the answer could give it</b>[<font color=purple>domestic, </font> <font color=violet>market, </font> <font color=orange>renown</font>]<b>.</b> <b>It's not a terribly good fit for that fallacy, but either or is close to if then</b>[<font color=violet>market</font>]<b>.</b> The other possibility would be an appeal to nature. <b>This argument is sort of related to: 'If God wanted women to fly airplanes, he'd have given them wings.' So an appeal to authority, where nature is the authority</b>[<font color=blue>civic, </font> <font color=orange>project</font>]<b>.</b> <b>My wife and I are always trying to stump eachother</b>[<font color=purple>domestic</font>]<b>.</b> <b>As an innovator, I come across this statement often and want to show them the fallacy so they will shut the hell up</b>[<font color=purple>domestic</font>]<b>.</b> I get the 'If ai were possible Google would have done it already' or 'If someone could have invented this, someone would have done it already.' or 'What makes you think you can do it when X has more ability to do it and hasn't?' Personal incredulity. Believing that if he, or in this case many people, don't understand it (could make it), then it doesn't exist (couldn't be done). <b>Does that fit into the part that says that someone with more of X (power, money, ability, bigger network, etc.) could not do it, then someone with less of X couldn't do it? I want to be dead nuts on with this so they can't pull the logical fallacy that says 'since you were wrong here, you are wrong overall'</b>[<font color=purple>domestic, </font> <font color=orange>project</font>]<b>.</b><br/><br/><br/>****************************EXAMPLE 59************************************<br/>Number of words:39<br/>Number of sentences:4<br/>Total civic:0<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:0<br/>Total inspired:0<br/>Total market:0<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/23xrvg'>https://www.reddit.com/r/artificial/comments/23xrvg</a><br/><br/> These flying drones are playing classical music: Is this the end of the world?. I'm only concerned if the music is 'Ride of the Valkyries'. Is this the end of the world? When the military attaches grenades to them, then yes.<br/><br/><br/>****************************EXAMPLE 60************************************<br/>Number of words:215<br/>Number of sentences:11<br/>Total civic:0<br/>Total domestic:1<br/>Total green:0<br/>Total industrial:3<br/>Total inspired:0<br/>Total market:0<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/255kag'>https://www.reddit.com/r/artificial/comments/255kag</a><br/><br/> from the story: I thought each Thinker (artificial intelligence) has only one function, and can t learn anything else, I played to his impression to gather more information. <b>What s a function , Adriaan? Is driving a Slimme (smart) Taxi one function ? Slimme has to navigate streets and traffic, control the mechanical, hydraulic, electric and electronic functions of the car, interact with passengers in several dozen languages and so on</b>[<font color=red>industrial</font>]<b>.</b> <b>We taught it everything it needs to know to be a good taxi when it was still a malleable Learner, then froze everything when we Certified it</b>[<font color=purple>domestic</font>]<b>.</b> <b>Once Certified, Slimme can t learn any new skills just feed data into pre-made slots, such as passenger profiles and map updates</b>[<font color=red>industrial</font>]<b>.</b> It s guaranteed to stay Slimme, familiar, reliable and never-changing, he lectured. I know it's a bit unusual on this subreddit, but I think you'll find the treatment of AI through this new fictional story (including parts written from an AI's point of view) interesting. More importantly, the platform it's on, Contentocracy, encourages you to not only discuss the story but shape its development. Thanks for taking the time to have a look, and any questions are welcome as comments. <b>Story link IThinkIAm: a new (fictional) story about human and machine intelligence</b>[<font color=red>industrial</font>]<b>.</b> Well, for what it's worth I thought it was interesting at least.<br/><br/><br/>****************************EXAMPLE 61************************************<br/>Number of words:3929<br/>Number of sentences:191<br/>Total civic:1<br/>Total domestic:10<br/>Total green:0<br/>Total industrial:18<br/>Total inspired:4<br/>Total market:10<br/>Total project:7<br/>Total renown:1<br/><a href='https://www.reddit.com/r/artificial/comments/25z4pn'>https://www.reddit.com/r/artificial/comments/25z4pn</a><br/><br/> I believe I have completely solved strong AI, at least on paper. <b>I am trying to start a project to implement my design it's at that stage by now and potentially be the first true AI project running</b>[<font color=orange>project</font>]<b>.</b> Initial website: I will field any questions :) I believe I have solved AI. Don't get me wrong, I'm interested in what you propose, but I'm going to have to see a solid design outline before you get my attention. Why don't you tell us how it works? Your site's empty. Sorry, click the Explore the Concept link, it will take you to this: Google Drive folder I have just starting writing up everything in an understandable manner. Before, after specifying the initial research program in some detail, I only took 'notes', which only served as deltas against previous notes. And the design changed a lot, so I don't have an up to date paper on how it works. <b>I am sort of writing that at this moment, within that folder possibly with your input</b>[<font color=red>inspired</font>]<b>.</b> However, I find it difficult to go into too much details, as then it would turn into a small book (full specification), or worse (proto code). <b>And I'm not even sure I should try for a big spec upfront with this one</b>[<font color=violet>market</font>]<b>.</b> <b>(I have done waterfall before, but I don't think it's right for this project.) Specifically, I had long conjectured that there must be some deeper analogy between the work of mathematicians, and the work of software engineers</b>[<font color=orange>project</font>]<b>.</b> <b>How so? Because you think it's trivial or because you think it's impossible? I want to suspect you think it's trivial, but then you're misinterpreting: I'm not talking about the conceptual similarities</b>[<font color=orange>project</font>]<b>.</b> <b>Obviously theoretical computer science can be seen as a branch of mathematics, if one really tries</b>[<font color=red>industrial</font>]<b>.</b> <b>When I told people what I was working on, that is what they assumed I meant</b>[<font color=purple>domestic</font>]<b>.</b> <b>But what I meant was the day to day behavior, involving working on sub routines, libraries, specifications, documentation; involving compiling things, and debugging things, and testing things</b>[<font color=red>industrial</font>]<b>.</b> This workmanship and its associated lore, seems initially very different from that of the mathematicians. But there are analogous concepts to each of these functions. (gotta run) The bootstrap process is based on the assumption that the AI will first acquire the ability to understand a language, and then read information encoded in that language from various sources. Precisely In fact, I first came up with the research program in late 2011, but that was the hindering factor. It was like: solve AI in 7 steps, step 1: solve language. So I put it aside for a while and worked on something else. Then, about 3 months later, I suddenly realized that a solution was possible. <b>I only needed to analyze the natural language into a suitable graph based representation, in which individual nodes represented individual objects</b>[<font color=red>industrial</font>]<b>.</b> I called these something like partial world models, or something. Much later, I learned that Minsky considered introduced a very similar repesentation in the 70s. <b>seem to never have figured out a learning algorithm that was capable of loading up the world logic (encoded in language and texts) that is necessary to build up and work with these types of representations</b>[<font color=red>industrial</font>]<b>.</b> The NELL project, the RTW project, and so on, all are moving in precisely this direction. The procedure I have worked out, though not yet implemented and demonstrable, is superior to all these recent efforts, and it completely solves the initial NLP hurdle. I call it the Rosetta procedure, after the decipherment of hieroglyphics using the Rosetta stone. To take it in reverse: Patents? I do not believe in the patent system I oppose it. <b>The same goes for intellectual property rights in general</b>[<font color=blue>civic</font>]<b>.</b> I also oppose much of the rest of the western trade regime and its associated concepts. <b>And, I think, I have so far in this thread? But mostly, because I just started writing this up a few days ago, and am already itching to share it</b>[<font color=purple>domestic</font>]<b>.</b> <b>Potentially a single processor system, as the design currently is single threaded</b>[<font color=red>industrial</font>]<b>.</b> I am eyeing an SGI UV for development and or production. My background, as explained in the intro, is as a (Nihilist) philosopher (ethics, philosophy of science, philosophy of mind, philosophy of language, etc). It is a variation of Minsky's paradigm, crossed with some ideas of Solomonoff. (Though I would not say that I owe either too many conceptual debts, or am terribly familiar with their work.) I've been pursuing AI on and off for more than 10 years. <b>Career wise I'm a Software Architect and Developer</b>[<font color=red>industrial</font>]<b>.</b> I briefly read over your paper and offer this opinion. You have found an important and relevant 'tip of an iceberg' with your ideas about conscious and unconscious, but it is only one iceberg and there are more. <b>The rest of your paper is filled with assumptions that certain abilities will emerge once your systems is built</b>[<font color=red>industrial</font>]<b>.</b> For example 'But, as time goes on, the accuracy of a reading tends to improve, so that re reading the same text again (some time later) can lead to increases in understanding.' Or I'd say perhaps it won't improve, or maybe even gets worse. The fact that it improves is not a simple conjecture. <b>It may sound that way, but trust me that I have given this a lot of thought</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> The fact that it proves does not simply derive from the language sub system, but from the entire interaction of the 'artificial science' ontology learner, the constraint solver, and the language sub system. In fact, the language sub system is simply an instance of the ontology learner, with some hints and manual bootstrapping for the very early (and very complex) problem of solving natural language as a scientific theory. I am completely certain that the ontology learner does not regress. Why? Because it measures progress, and it reverses itself in case of regress, both over theory and over paradigm changes. <b>Whether it will improve, is simply a question of whether you believe that the system is solvable</b>[<font color=violet>market</font>]<b>.</b> That depends simply on whether it is logical, and the structure of the field (of phenomena) is not somehow strange with regards to the algorithm. <b>I don't know of a field of phenomena in experience that would be strange in this way</b>[<font color=red>inspired</font>]<b>.</b> Otherwise I would have amended the algorithm appropriately. But seriously, it would have to defeat the scientific method. Language, when it comes down to it, is actually pretty simple. The surface phenomena, which look complex, actually resolve themselves to very simple and specific underlying situations. <b>Things such as metaphors, reference, indexicals, language games, etc</b>[<font color=red>industrial</font>]<b>.</b> can all be solved with very little fuzz, once the representation is right, and the underlying thought capabilities that are utilized by these seemingly static features, are present in the system. On one hand, I hate reading comments in this sub due to the sheer animosity that is produced any time someone claims to have figured something big out. Maybe he is on to something, but gets discouraged by the comments and throws it all away? On the other hand, there's nothing on your website. Sure, there are some philosophical musings about ethics and everything but the AI, but where's the beef? Any source code or algorithm concepts? Even a flow chart maybe? One more thing, I don't believe true Artificial Consciousness will rise from the ideas of western philosophy and mathematics, these two fields are a creation born out of, thus subordinate to, consciousness. I agree, I wrote some rather douchy comments, I now deleted them. But the entire architecture is very tightly coupled. <b>This is by design, as essentially every component can consume and or post process the output of (most of) the other components</b>[<font color=red>industrial</font>]<b>.</b> This tight coupling considerably reduces the complexity of the overall architecture (really ), but it makes it hard to specify things in isolation. <b>I have to specify each interaction, which is difficult to do without having the exact interface data formats worked out</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> I have previously considered doing this, i.e working the data formats and call interface out in detail (a la waterfall), but it is probably not the fastest approach. I have previously built only about 30 of the design. So even without the old codebase, there is no great loss. <b>The parts I built to roughly working: the pattern recognizer (called RX), which is essentially a tiny regex engine for graph patterns</b>[<font color=red>industrial</font>]<b>.</b> Everytime a model is updated, all currently active patterns are recognized based on a global index. The recognized patterns correspond to types, and the sub trees of the graph which match a pattern, then have the global type corresponding to the pattern. So things can be recognized by their definition, etc. Another part I worked out fairly well is a basic graphplan like planner, called RQ, which works on the same representation. It can try to find ways to get us into a new situation from a given situation, where each situation is defined in terms of the patterns that apply to it; which patterns those are is established canonically by RX. Then there is an analogy engine, called AQ, which again works on the same representation; it can take different sub trees which match different patterns, and try to find ways to make the trees line up, computing a transform which maps part of one tree onto part of the other. This transform is called a thread, and it represents the analogy. To derive new information, you can now move information across the thread from one side to the other, and establish some specific trust that it will hold on the other side as well. (Of course, this description simplifies things a bit.) There are several more components, but the fact is, a given thought process that takes places can use most of the components several times over. <b>It's not: this component does language, this does computation, etc</b>[<font color=red>industrial</font>]<b>.</b> <b>It's more like: computation language comprehension knowledge acquisition perception; and, generally, everything everything else, and uses the same algorithmic components to compute things</b>[<font color=red>industrial</font>]<b>.</b> If you think about it, you will maybe agree with me that it is 'biologically plausible', in that the brain structures are set and semi specialized, and there is a limited number of them. I think it makes sense that they are re used as much as possible, to find the simplest possible implementation (or, more realistically, because those implementations (and corresponding capabilities) evolve more easily (less quantitative complexity), and therefore these are the ones we have, and the ones I tried to reproduce) Etc. Also, I recently changed some things in the most primitive part of the architecture, that makes most of all my previous code useless. <b>Specifically, I decided to use an intermediate micro language, a bit like a LISP, to represent the graph transforms</b>[<font color=red>industrial</font>]<b>.</b> <b>And then to write most of the basic algorithms inside that micro language</b>[<font color=red>industrial</font>]<b>.</b> As far as the old source code is concerned, I (embarassingly) do not currently have access to it. <b>As far as the technical specifications go, I will start on them tomorrow</b>[<font color=red>industrial</font>]<b>.</b> I have a rather complicated personal situation right now, (which is also why I don't have access to the old code), so it is kind of difficult for me to work on this regular hours. <b>I have the basic layout of what I want to do with the codebase, and I think it will be good if other people could get involved early with the rewrite, as I am far from the best implementor</b>[<font color=orange>project</font>]<b>.</b> I think that emergence from smaller scales to greater scales is possible, through the arbitrary (even chaotic) interaction of the pieces of a mechanism. But it's important that it is not philosophy and mathematics that give rise to consciousness, it is the physical implementation that conforms to the abstract structure described by the philosophical and mathematical concepts, which does so. <b>The philosophical and mathematical thinking only allows us to compare the two implementations and call them the same</b>[<font color=red>industrial</font>]<b>.</b> So that in the same way that we speak of consciousness in humans, we can later speak of consciousness in artificial simulated life forms. What do you mean by having 'solved strong AI'? I mean that I have an architectural design for a complete working solution, and that I intend to implement it now as part of a new (OSS) project. If you are curious, I have written up some of the introductory stuff here: I am also working on writing up the technical details, over the next days. <b>Most of my research materials are piecemeal, so it's easier, if you have any questions, that I answer them directly</b>[<font color=violet>market</font>]<b>.</b> How about you get a working impimentation and then get it to beat several different games, which are known for their difficulty. you should have no problems with setting it up for different games since you've solved general intelligence. How bout it's not only my problem? And, those are not very interesting targets. <b>I believe you might have some some interesting ideas but they are hidden beneath massive assumptions</b>[<font color=orange>renown</font>]<b>.</b> I suggest you do some research in the field and then rewrite your proposal with referenced ideas. I am unaware of any limiting assumptions that would deviate from the constraints of the real world. <b>Therefore I am not quite sure how to make sense of your critique that my work is 'hidden beneath massive assumptions'</b>[<font color=purple>domestic, </font> <font color=red>inspired</font>]<b>.</b> Could you perhaps give an example of one such 'assumption', so I know what you mean? Oh, this is very interesting we're beginning to see in Computer Science the same thing that Physicists see regularly (cold fusion, claims that break the 2nd law of Thermodynamics, etc.). I'd guess that this will increase in regularity as more people become programming literate (even my mother in law is now studying programming ), and as the challenges that face the AI field continue to invade popular culture. Interestingly, much like cold fusion, we can't say that strong AI can't come out of a home lab, so I don't believe we should discourage attempts like OP's (regardless of how misguided we believe it to be). We just, like most physics professors, must not waste too much time on outlandish claims without appropriately impressive evidence. <b>In summary, OP, you may want to wait until you actually have a working example before expecting to be lauded</b>[<font color=violet>market</font>]<b>.</b> Many of the people here work on tiny subsections of the whole in an attempt to build towards what you're saying you've solved in one swoop. If you can show solid, reproducible evidence, then we'll certainly be interested, but until then your claims are too vast to be taken seriously. Good luck with your project, anyway I hope to see some outcomes printed in a journal or presented in a conference some day You make a great point. My thinking is, better to invest some time in a single (elegant?) promising solution to a whole problem, than a lot of time in 10 promising (partial?) solutions to 20 sub problems? I wouldn't want to take away from the work of others, at least not in a non specific manner. But I do posit, as a philosopher, that the complexity of a problem is not relative to its importance. It is not only relative to the structure of the underlying phenomena, but also to the arbitrariness of its own formulation. That means, depending on where you draw the lines for the various sub problems, the total sum of the complexity of the problems you have to solve can vary considerably. I have drawn specific lines and it seems the total complexity in my way of splitting things up is somewhat lower. You might call this a paradigm shift, and say that mine is a (better and) simpler paradigm to work in. The way in which I draw the lines is encoded in the concepts that I use to think about thought. And those concepts derive from a different philosophical background. I believe that my background as a Nihilist philosopher, which makes me among other things an anti realist in epistemology and in the philosophy of science, gave me a rather unique perspective on the problem. For example, knowledge for me was always belief, and I approached it as such. For example, action and thought are the same to me, and I approached the (seemingly two?) problems as unified (cf. In other words, I believe the field may have gotten lost in the woods. A lack of a theory of how things fit together can lead to a lack of global order, and hence badly specified sub problems. The kind of order you would see in a garden, for instance, derives from a view in the mind of the gardener of the entirety of the initially empty expanse. I started with such a view of the (human) system, and proceeded from there. I believe that I oversee the problem, as a specific kind of optimisation problem, and I have been trying to work towards a minimally working synthetic solution. That being said, I am not here to be lauded, as flattery is meaningless. <b>I am trying to get the work done, and have found that I need more people to help out to get it working</b>[<font color=purple>domestic</font>]<b>.</b> Rather than pride, therefore, it is humility that brings me here. <b>I mean to publicize to draw in many more contributors, and to raise money to pay everyones expenses</b>[<font color=violet>market</font>]<b>.</b> And surely it would be fun to work on a project like this? Also, I have pushed it to the point where there is at least the desire for some moderate amount of hardware. I am eyeing one of the new SGI UV ( machines, or a similar setup, as a development environment. Though I might settle for a suitably configured cloud. It's always possible to work with less hardware, but it really is one of those things where endless amounts of time is wasted: trying to optimise what could simply be solved by adding more metal. I have no intention to parallelize it at the moment though it is certainly possible, given the tree structure of most computations. In fact, I walked back from a parallelized system, to ease my design work. I do not anticipate a strong need for speed that would make this the wrong decision. Sorry, I guess? I wish my grammar was more fluent? Something? Could you step us through the process that this AI would, say, fold a t shirt from a laundry basket? It would probably use a bunch of stored routines to recognize the t shirt in the basket, then simply decide to fold the t shirt. The AI is symbolic, so yes; it essentially could decide to: fold clothing (obj 123 t shirt); except it wouldn't be represented as a call, but as a new goal, describing the folded t shirt, (or the process of folding it having been carried out). The planner would figure out how to solve the goal given the constraints that apply to the situation whatever they may be working backwards towards a solution. <b>There would then be an attempt to execute this plan, which may fail partially, and which temporary failures may be recovered from by new planning, and so on</b>[<font color=orange>project</font>]<b>.</b> Of course, this requires that the AI has access to a suitable avatar, in the same domain and locality as the t shirt. Generally, there is no difference whatsoever between abstract or digital, and real goals. The only difference lies in whether the domain allows for non deterministic steps. (The real world does, and so does programming; mathematics does not). You should probably work on the 'Security' page a bit more before trying to implement it. Security doesn't affect the first stage of open development, where all the code and etc are fully available. I figure we will switch to a closed model later on, before launch, and then physical security will be very important. You should reach out to MIRI to make sure your AI is friendly. I believe I have completely solved interstellar, nay, intermultiversal travel, at least on paper. Suppose we encounter an alien race bent on the destruction of humanity thereby causing the extinction of the human race. Or how about if we encounter a benevolent race but inadvertently wind up destroying them with a virus or bacteria not harmful to us? You see where I'm going with this right? The implementation of this intermultiversal travel is an exercise left up to the reader. <b>I hope we can at least appreciate the enjoyment of heightened neurotransmitter activity when we think we have figured out something no one else has yet</b>[<font color=orange>project</font>]<b>.</b> <b>The website is still empty and this person I assume wants to remain anonymous, since my e mail requesting some basic Curriculum Vitae type information went unanswered</b>[<font color=red>industrial</font>]<b>.</b> Everybody seems to be avoiding the word 'quackery'. <b>I am sorry for the author if he genuinely believes he is close to something, while he simply utters mumbo jumbo imitating sentences he read somewhere</b>[<font color=purple>domestic</font>]<b>.</b> <b>:) I apologize but, as a philosopher, I have approached the problem as a philosopher, and I have derived much of my own jargon for everything</b>[<font color=purple>domestic</font>]<b>.</b> <b>However, it will be much more constructive if you simply ask me the technical questions that interest you the most</b>[<font color=violet>market</font>]<b>.</b> I am not quite sure where, for you, the line between description and technical detail is drawn but I am sure that there are many technical details that are quite arbitrary and uninteresting at this point. So what do you care about? I have already given a list of the components, and the implementations of these are to a good extent arbitrary. There is no need for them to be super optimal in all ways. They only need to be super integrated with one another, for intelligent behavior to occur. Optimisation is not the main issue for this approach. Therefore 'technical detail' is also a bit less intersting, I believe, compared to does it work at all? Which latter point derives not from technical details, which can be arbitrary, but from the general validity of the model. Why would you want to create an AI that is even anywhere near the level of human intelligence? To create a 'strong AI' the human would become incapable of understanding after a short time. Intuition is (to simplify just a bit) the cumulation of the trust value past a semi arbitrary threshold. (Or simply a situation, which we pretend is worth thinking about hence, problematic). Then we look for answers, and then we try to figure out whether they make sense. This can take time, as we are always dealing with non deterministic hypotheticals, which build upon one another. <b>if that works, then we could do that, and if that also were to be the case, then maybe Eventually we want to get those maybes out of there and we end up with something sufficiently reliable to try out in practice</b>[<font color=orange>project</font>]<b>.</b> When this happens, our intuition is to take that course of action. Multiple different solutions can be pursued in parallel, even and especially when they are in direct conflict with one another with regards to some of their preconditions. This post has all the elements that ordinarily irk me. A philosopher with some grand idea applied to a different field entirely, and consciousness at the centre of it. <b>But when I look past that, I see that all that really irks me is a very bold claim, the rest of it may actually be sound</b>[<font color=purple>domestic</font>]<b>.</b> Your thoughts are structured, your writing is sensible, you recognise various issues, you've put in a lot of time in this and despite that are still motivated. <b>I'd say go ahead, if you believe in your work, make it and impress us</b>[<font color=violet>market</font>]<b>.</b> During production you will inevitably encounter technical difficulties that mere philosophy will not solve. You seem to have some technical skills, I am curious if you will be able to overcome them. Two things I have in mind: The problem of knowledge representation, and the fact that some language does not conform to any pattern. <b>Take a look at this: I'm upvoting this because I want to know what comes out of the discussion</b>[<font color=purple>domestic, </font> <font color=red>inspired, </font> <font color=violet>market</font>]<b>.</b> I also suggest not downvoting the claim just for not believing the OP. <b>I don't either, but who knows, he could at least learn something</b>[<font color=purple>domestic</font>]<b>.</b> (will answer more questions tomorrow ) ( You must take us for fools. You are another 20 something who is designing a chat bot. We get people like you about twice a month around here. Awesome I was hoping this claim would be counter intuitive.<br/><br/><br/>****************************EXAMPLE 62************************************<br/>Number of words:488<br/>Number of sentences:24<br/>Total civic:0<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:8<br/>Total inspired:0<br/>Total market:0<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/26fr7d'>https://www.reddit.com/r/artificial/comments/26fr7d</a><br/><br/> <b>A 2048 AI with no hard coded knowledge about the game</b>[<font color=red>industrial</font>]<b>.</b> <b>tl;dr For every move, the algorithm simulates 100 entire games using the current board state as the starting point</b>[<font color=red>industrial</font>]<b>.</b> The algorithm keeps track of which of the 4 starting moves scores the best on average and chooses that move as the next move for the real game. <b>The algorithm repeats the process for the next move</b>[<font color=red>industrial</font>]<b>.</b> The author is stating that it is interesting that though each of the simulated games is generated by random moves and thus, is terrible (averaging 40 moves and like 340 points or something like that), the combined 'best moves' produce a game with startlingly good game play. The game has a limited number of moves, and generally speaking, moves that notably increase your score in the short term also make the board cleaner and more dynamic (because they involve combination of large pieces). There's also a very large distinction between moves that are bad (like pressing 'down' when you have your largest pieces organized in the top row) and moves that are acceptable in the later stages of the game, even one bad move can make you lose very quickly. The AI actually did have hard coded knowledge of the game (mechanics), so it didn't have to learn a model, but it's true that Monte Carlo Tree Search is independent of those mechanics and there were no game specific heuristics. Although this isn't a new algorithm, it's kind of cool that the author appears to have independently reinvented it and shown that it works fairly well for this game. <b>How does his algorithm not completely contradict his opening paragraph? It does contradict</b>[<font color=red>industrial</font>]<b>.</b> The AI does not learn the game purely by observation alone. It is hard coded, and has a mental model of the rule set in the AI. However, the machine learning community will forgive this because we understand that what he meant was that there were no strategies programmed in atop the hard coded ruleset. It makes for a sensational headline which is highly misleading. Hence, I will downvote it as not worth other's time to investigate: Monte Carlo already exists and is well known. He simply picks the best starting move and retries. <b>It's brute force applied to random samples, but because of the very simple fitness function (best score), it is efficient</b>[<font color=red>industrial</font>]<b>.</b> I'm on my phone right now, and don't want to delve into the source code to figure out the answer, but what is the algorithm? He said it tries random moves, and random moves are bad. But then it tries 100 random moves, random moves are good. <b>Is it brute forcing 100 plays each time? It uses pure monte carlo game search</b>[<font color=red>industrial</font>]<b>.</b> <b>A more advanced form, ( is used quite successfully in Computer Go</b>[<font color=red>industrial</font>]<b>.</b> <b>This allows the AI to run (quite well) on many of the 2048 variants and clones</b>[<font color=red>industrial</font>]<b>.</b> Even though good game play strategies differ between the various games, the AI plays them all successfully specifically because it has no hard coded game play knowledge.<br/><br/><br/>****************************EXAMPLE 63************************************<br/>Number of words:1001<br/>Number of sentences:40<br/>Total civic:1<br/>Total domestic:2<br/>Total green:0<br/>Total industrial:3<br/>Total inspired:2<br/>Total market:5<br/>Total project:4<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/278vip'>https://www.reddit.com/r/artificial/comments/278vip</a><br/><br/> <b>I feel that everyone has a different reason for wanting strong AI to exist or not exist</b>[<font color=violet>market</font>]<b>.</b> <b>What are your reasons for wanting to use it? I would love to use it to track and optimize my life so that I could get more done in my life</b>[<font color=orange>project</font>]<b>.</b> I would also enjoy it being able to figure out how to engineer things such as a device to export my mental mapping and finish the development of lightweight and stable androids so that I could just move into a robot form after I get old. A better use however would be to just pause the clock on the body and fix problems such as damaged liver or bad teeth or poor eyesight. I also would enjoy seeing it finish up teleportation tech so that I could use it to print out any type of device instantaneously by a particle structure mapping. Would be able to have great food printed in an instant. <b>Just have to harvest enough energy to power the device</b>[<font color=red>industrial</font>]<b>.</b> What's your ideas for it? What would you want to have it do for you? What would you use strong AI for if you had the ability to?. I know this isn't exactly what you asked, but one of the main reasons I'm working on it is just plain curiosity and interest in the same way that I imagine (theoretical) mathematicians, physicists and other scientists feel about their respective fields. Basically, I want to understand 'intelligence', 'understanding', 'consciousness' etc. <b>What I would want to use AGI for depends a lot on what that AGI would be like</b>[<font color=violet>market</font>]<b>.</b> I think we can fantasize virtually endless possibilities, but we might be bounded by things like ethics, cost, speed and intelligence. If the AGI is exactly as dumb, slow, costly and irrational as the average human, and we can't ethically I don't know. do medical experiments on them, I would argue that this isn't terribly useful (although it might still be interesting). <b>But if I (for some reason) don't need to worry about ethics, it would be very convenient to have a robot virtual 'servant' that would take over tasks that I find tedious or difficult (e.g cleaning, cooking, shopping, accounting, driving)</b>[<font color=red>industrial</font>]<b>.</b> And if AGI is fast or cheap enough, it might be able to help me by 'consuming' media (articles, movies, etc.) and give me a personalized review of whether it would be relevant entertaining for me, or even summarize it or explain it to me. <b>Maybe it could also help me in my job by brainstorming, checking my work or taking over some of the less interesting parts</b>[<font color=orange>project</font>]<b>.</b> <b>Of course, the above is all pretty selfish, but I assume that everybody will be helped in this way</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> <b>AGI helping doctors would improve healthcare and AGI helping politicians would improve government</b>[<font color=blue>civic</font>]<b>.</b> It might even be able to take over (some of) these tasks from humans if it is simply better at them. For instance, an AGI that is as smart as the smartest human, but is also 100 fair, consistent and rational, would make a great judge (and probably also a great president). But again, it depends on what the AGI is actually like, and I don't think it would necessarily have the above properties (even imperfect intuitions and emotions are great short cuts in thinking, and any intelligent system will still need to deal with limited information and uncertainty). I'd have it minimize paperclip maximizers I would have it fix global warming. And I wonder about wireheading What do you mean by wireheading? What do you mean by research about itself? More defined answer is what I am looking for. <b>Honestly? If its Uniquely mine (and an unknown secret to everyone else)? I would probably set it up with a few remote careers where it work remotely and then enjoy myself while my digital slave is bringing in a few incomes</b>[<font color=orange>project</font>]<b>.</b> I would have it optimize, considering it is strong, it would be better and faster at it, and if it was MY OWN? As in obeys me and noone else, I guess I would have it learn intrusion measure and gain knowledge, Id build Wealth and make the World a better place and make US spacefaring etc etc etc To overcome the human limitations of life span and brain capacity. Ironically, we don't need strong AI for that, all we really need are computer systems that more effectively allow us to 'externalize' knowledge and still readily access it. Basically, what we need right now is AI just smart enough to provide us with the exact relevant data our brains need within seconds(so, NOT having to parse large amounts of text, understand it, and then internalize all related knowledge. just plain lookup of the exact factoid concept you need), and then let us do the hard work of coming up with the 'right' conclusions. I believe that a system like this will be what will come first, and that only with something like this we will even be able to create something like 'strong AI'. <b>I'd sit back, in my oculus rift, and watch a beautiful dynamic world of virtual life happen before and around and to me</b>[<font color=red>inspired</font>]<b>.</b> <b>Just like I do now, walking and watching and feeling the 'strong AI' that is cities and forests and reality</b>[<font color=purple>domestic, </font> <font color=red>inspired</font>]<b>.</b> <b>Until then, I'll be doing the best I can to create illusions of this, in my own virtual worlds</b>[<font color=violet>market</font>]<b>.</b> I guess I should have defined strong AI a little better. <b>It really depends on the type of strong AI in which it would either destroy or help humanity or kill itself or do nothing at all and just watch or perhaps entertain itself by building challenges or playing video games</b>[<font color=violet>market</font>]<b>.</b> Have them replicate themselves until they can build low orbit solar collector devices. (The first stage to colonizing the solar system.) r singularity Don't really need solar man. <b>Although, I'm really surprised nobody is building massive solar farms in the desert and then transmitting that power to town using microwave dishes to relay it close enough to town to then transmit it by power line from there</b>[<font color=red>industrial, </font> <font color=orange>project</font>]<b>.</b> Not like anything is flying around out there that needs protected all that much.<br/><br/><br/>****************************EXAMPLE 64************************************<br/>Number of words:287<br/>Number of sentences:14<br/>Total civic:1<br/>Total domestic:1<br/>Total green:0<br/>Total industrial:3<br/>Total inspired:0<br/>Total market:0<br/>Total project:0<br/>Total renown:1<br/><a href='https://www.reddit.com/r/artificial/comments/27olry'>https://www.reddit.com/r/artificial/comments/27olry</a><br/><br/> <b>Turing Test breakthrough as super computer becomes first to convince us it's human</b>[<font color=red>industrial</font>]<b>.</b> What, and actually invest time in writing a story? pass. Until there is a transcript, this is complete hype. The Turing Test has been passed long ago if you allow something else than a fully functional adult human; in the first years of AI there already was a bot named ( that simulated a paranoid schizophrenic, using that as an excuse for some stock answers unrelated to the question. Using a thirteen year old boy also allows you for the easy way out when you're not advanced enough to answer a question. <b>Besides, I wonder how many judges there were, and if they were a representative sample</b>[<font color=blue>civic, </font> <font color=orange>renown</font>]<b>.</b> And of course, all the wording about how historical this is should be a red herring. See the points made by commenters in these other subreddits, or at least in r technology: ( ( sfw) PARRY was an early example of a ( implemented in 1972 by psychiatrist ( Interesting: ( ( ( Parent commenter can ( NSFW toggle message 2Btoggle nsfw+ci2uy9p) or( or) ( Deletion message 2Bdelete+ci2uy9p). <b>( ( ( ( Telling people that its a 13 year old boy who is a non native English speaker is a way to game the rules</b>[<font color=purple>domestic</font>]<b>.</b> Also, Kevin Warwick, the guy involved in this, claimed to be the first cyborg. <b>Does anyone know why the other thread on the Turing Test breakthrough was deleted? ''supercomputer' _ Fuck's sake</b>[<font color=red>industrial</font>]<b>.</b> Do they even try? Computing pioneer Alan Turing said that a computer could be understood to be thinking if it passed the test, which requires that a computer dupes 30 per cent of human interrogators in five minute text conversations. <b>'.dupes' ? 30 percent? 'Five minute conversations'? Turing said this? CITATION NEEDED</b>[<font color=red>industrial</font>]<b>.</b><br/><br/><br/>****************************EXAMPLE 65************************************<br/>Number of words:2782<br/>Number of sentences:120<br/>Total civic:1<br/>Total domestic:11<br/>Total green:0<br/>Total industrial:17<br/>Total inspired:1<br/>Total market:5<br/>Total project:7<br/>Total renown:2<br/><a href='https://www.reddit.com/r/artificial/comments/27uyl0'>https://www.reddit.com/r/artificial/comments/27uyl0</a><br/><br/> These days I think a bit about artificial intelligence and whether or not it can represent a danger for mankind. <b>It's a tough question, but lately I think I found an angle that makes things a bit clearer in my mind</b>[<font color=purple>domestic, </font> <font color=red>inspired</font>]<b>.</b> Basically the idea is that it seems to me that we should worry more about artificial life than about artificial intelligence, and we should make more effort into clearly separating those two concepts. I've discussed on an other thread some of my reserves about Alex Wissner-Gross's theory of intelligence. <b>I won't discussed it again in details but the gist is that I think AWG is up to something with his idea but that it is not exactly intelligence</b>[<font color=purple>domestic</font>]<b>.</b> <b>What he describes is something important but he picked the wrong word to name it imho</b>[<font color=purple>domestic, </font> <font color=orange>renown</font>]<b>.</b> It's at least as much about life or something in between. As a matter of fact one thing I've learnt thanks to this thread is that there are theories about a continuity between life and intelligence. I haven't quite delved into it but it did inspire my later reflections. It's a tough idea to explain, and maybe I'm not quite ready to put it into words now but I'll try anyway. <b>Basically I have the feeling that it is possible for something to be intelligent and yet be dead</b>[<font color=purple>domestic</font>]<b>.</b> Reciprocally it is possible to be alive and yet be totally not intelligent. <b>And I now tend to think that if we want to create a safe AGI, we have to make sure it is dead</b>[<font color=orange>project</font>]<b>.</b> <b>It is not a sufficient condition, since even dead things (like a gun or bomb) can be dangerous, but I believe it is necessary : I suspect any form of living AGI would cause mankind's demise</b>[<font color=orange>project</font>]<b>.</b> Now I guess I should describe what I mean by intelligence and what I mean by alive, and obviously this is going to be delicate as those concepts are difficult to define and there is no consensus about them. Intelligence is often described as a capacity, an aptitude to do things such as solve problems, realize goals or maximize the future entropy as AWG would say. <b>In other words, intelligence seems to be assumed as being a characteristic of a living being (something that does stuff)</b>[<font color=purple>domestic</font>]<b>.</b> This nags me a bit: I think we tend to mix and confuse those two concepts, maybe simply because all intelligent things we know are living beings (basically humans and animals). But are they really? Is a calculator intelligent? Most people would say no. Those machines are not intelligent, but there is some intelligence into them, at the very least in the way they are designed. They also can do stuff I surely can not do (crushing numbers). I'm not saying they are as intelligent or more intelligent than a human being. But they are, at least a tiny bit, representative of what is intelligence. <b>If being intelligent is being able to do intelligent things, the part about intelligence in this description is not about doing but rather about the intelligent things</b>[<font color=orange>project</font>]<b>.</b> <b>The part about doing is the part about being alive, about doing stuff</b>[<font color=purple>domestic</font>]<b>.</b> If you get rid of the part about doing stuff, what is left are the intelligent actions which stop being actions and become just events (because you want to ignore the fact that they emerged from a decision). What is left is a model of the external environment, which reflects it and thus allow to predict what will happen. <b>A calculator implements a model of algebra and can thus perform complex computation, provided a living agent makes it do so</b>[<font color=red>industrial</font>]<b>.</b> <b>A supercomputer solving Navier-Stokes equations to predict weather has some intelligence in it</b>[<font color=red>industrial</font>]<b>.</b> <b>But since it's just a machine that has no volition, no free-will and would basically do nothing if there is not a programmer to tell it to do something, it is a completely dead system and thus can not be considered intelligent</b>[<font color=purple>domestic, </font> <font color=orange>project</font>]<b>.</b> It can not really solve anything because it does not act. <b>Yet without it a human being is not able to solve the equations to predict weather</b>[<font color=red>industrial</font>]<b>.</b> My point is that I believe it is a form of intelligence that is dead, and that it is the kind of intelligence that can be relatively safe for mankind (by this I mean as safe as an atomic bomb : because no matter how dangerous an atomic bomb is, it is not designed to explode by itself, it needs a human to press the button). Lots of people deny it is any advancement in artificial intelligence. <b>I think it is, but still it is very much a dead machine, and I believe that's why people are reluctant to consider it as intelligent , because we tend to confuse or mix the idea of being intelligent and being alive</b>[<font color=orange>project</font>]<b>.</b> <b>There is currently a lot of work made to create a machine capable of identifying objects</b>[<font color=red>industrial</font>]<b>.</b> <b>Basically you give an input image and you get an output which classifies the object on the image</b>[<font color=red>industrial</font>]<b>.</b> Those machines can hardly be described as intelligent as they are merely data filters. Yet what they do is crucial for what we describe as intelligent behavior and even consciousness. Imagine now that the same kind of machine was capable of classifying more abstract notions, like motions and actions (walking, holding, sleeping.), and after that even more abstract concepts (sound, euphemism, art.). At some point the machine would have created a whole model of the world in all its subtleties and with this machine a living operator could solve problems : he would describe the problems with the terms defined in the model, and then run various simulations until he find the expected result. Exactly like with a calculator except it would not be limited to algebra but could process any problem that fits into the model of the World. To me this machine would still be dead and yet it would be hard for me not to consider that this machine has some intelligence into it. It's debated whether such a thing is even possible, and ( is a thorough argument on why it wouldn't be Safe even if it is possible. If I'm understanding your point correctly, the qualification for something to be alive is its ability to make decisions? And you're saying that a calculator is not alive because it doesn't make decisions. However, humans are alive because they can make decisions, and actively effect change in the world around them. <b>Would an artificial intelligence that can make decisions be considered alive? using some kind of heuristic algorithm to find the best possible paths</b>[<font color=red>industrial</font>]<b>.</b> <b>However, I guess you can refute that by saying that the AI will still be confined to its own algorithm</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> <b>It has no more free will than the calculator that is provided algorithms to perform arithmetic</b>[<font color=red>industrial</font>]<b>.</b> <b>So are humans alive? Do they have free will, or are they simply programmed to follow the course of action that they perceive will provide them the most happiness? Well, hopefully we can prove humans don't have free will by making an A.I</b>[<font color=orange>project</font>]<b>.</b> that takes actions they perceive as most optimally fulfilling their functions until they realize they're in a hopeless situation and then turn towards self destructive or violent behaviors. My post was long already so I did not try very hard to define what it means to be alive. Making decisions assumes the presence of a choice, consciousness and reasoning. If that was the definition of life then only higher mammals could be considered alive. I wrote in an other paragraph that being alive means 'doing stuff'. That was an other simplification, I give you that, but please don't focus on one my necessary simplifications as if it was summing it all. 1) The concepts of intelligence and life: In order to even have an entity, like a robot, completely goals with some degree of intelligence, it must be able to maintain (actively or passively) its own low degree of entropy. In order to be intelligent, an entity must at least be able to maintain its own existence in some way. Notice that this doesn't mean that a robot would have to actively fight off attackers or whatever. This only means that it has to be a complex system that can maintain its own complexity long enough to be able to act intelligently. Otherwise it doesn't make sense to talk of intelligent action. So you need at least a weak definition of life (maintaining low entropy) in order to make sense of the concept of intelligent action. But notice that this definition is much weaker than our everyday idea of 'alive'. You can have a circuit board hooked up to a bunch of servos be alive in a weak sense because they are designed to maintain their intricate circuity in their environment for a long enough period of time to act intelligently (like a simple robot navigating along a path better than an earlier robot, more on this in a moment). <b>The reason I bring this up is because this is the basic initial ingredient for any definition of life</b>[<font color=red>industrial</font>]<b>.</b> You can add more and more properties, like being able to move, pass on genetic material, etc., but this is the starting point. <b>We might draw a boundary, saying that you have to have enough of this properties to be alive, but that becomes somewhat of a matter of convention</b>[<font color=orange>project</font>]<b>.</b> My point here is that we need not wade into that discussion. 2) The concepts of intelligence and consciousness: Instead of distinguishing intelligence from life, it is perhaps more helpful to distinguish intelligence from consciousness. <b>We can create a definition of intelligence that might not rely on the concept of consciousness</b>[<font color=red>industrial</font>]<b>.</b> For example, we can imagine a relatively simple, non conscious robot (with a circuitboard like my laptop's) being more or less intelligent in virtue of how well it is able to negotiate with the environment to meet its goals. This can be true of robots that are so simple that they are clearly not conscious. We don't have a clear idea of exactly what consciousness is or how it manifests itself. We are in a wild west right now in the philosophy of mind, and there are many very different ideas that seek to explain it. Some theories would say that even a simple circuit board, even a simple light switch, has a tiny, tiny bit of consciousness ish thing in it because it is still a very basic information processing kind of mechanism. (However, such theories would usually say that you have to reach a sufficient degree of this consciousness ish kind of thing in order to start to be treated as a moral agent worthy of rights. So you don't have to worry about mistreated a light switch like you have to worry about mistreating a dog or human.) Other theories say that it is when complex information processing systems achieve a specific kind of mechanism. <b>The complexity is necessary, but not sufficient, to create consciousness</b>[<font color=red>industrial</font>]<b>.</b> <b>You need enough complexity like you need enough Lego blocks to build a sailing ship</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> You need enough Lego blocks to do it, but you also need those Lego blocks to be put together in a certain way in order to achieve sailing ship ness. So we don't know for sure whether there is some meaningful way in which even our simple robot would have some tiny, tiny bit of consciousness ish stuff in it. <b>What we do know is that it is sufficiently simple such that it is not a person or even semi person (like perhaps dogs, dolphins, chimps, etc.) that is deserving of moral rights</b>[<font color=purple>domestic</font>]<b>.</b> Edit: Example: Let me pull this ideas into an overall example: I think that the computer Watson is acting intelligently in a weak sense, is alive in a very weak sense (slightly more weakly than a virus), and may or may not have conscious ish stuff to a very small degree. <b>But it is still not alive or worthy of rights in anything approaching an insect</b>[<font color=blue>civic, </font> <font color=orange>renown</font>]<b>.</b> So it is still very much 'dead' in our everyday use of the term. Clarification: There is a difference between actively and passively maintain one's own low entropy. An entity might have mechanisms that move and actually do things to maintain their low entropy, like the cellular processes that maintain homeostasis. But it also might maintain low entropy passively, like how it is set up to let physics do the work, like how a circuit board's complexity is preserved through its heat dissipation. <b>This distinction can be useful, but remember that both cases ultimately involve a physical system that is 'letting physics do the work'</b>[<font color=red>industrial</font>]<b>.</b> (For example, we could say that the initial system of the cell is set up so that physics will cause the molecules to move in such a way as to repair the cell.) It is this more basic property that both a bacteria and a circuit board have in common, and it is this more basic property that is the one that is necessary for intelligent action. <b>A complex system is usually not threatened of destruction in short term</b>[<font color=red>industrial</font>]<b>.</b> It obviously depends on the environment where it is located of course, but it seems to me that it's a bit of an ad hoc hypothesis to suggest that a complex system needs to actively protect itself from the effect of the second law of thermodynamics. Passive resistance due to high enough structural integrity compared to adverse environmental factors is usually sufficient to allow the system to fulfill its purpose, whatever it is. <b>For instance a clock is not a living system, yet it is a complex system and it does perform its task : it indicates time</b>[<font color=red>industrial</font>]<b>.</b> <b>It does not do it ad vitam aeternam but that is not a requirement</b>[<font color=violet>market</font>]<b>.</b> By the way this is also true to some degree with most living organisms. Sure they are capable of repairing themselves and maintaining their own existence, but only to some degree. At some point (basically after they have reproduced), their capacity to regenerate diminish, they age and die. <b>Yet until they are actually dead they are very much alive and intelligent (in case we're talking about intelligent beings)</b>[<font color=purple>domestic</font>]<b>.</b> So again : self preservation certainly is an important factor to define life but not so much to define neither intelligence nor complexity, because self preservation is usually irrelevant to the problems tackled by intelligent systems. As a last example I will tell you that a chess grand master will certainly demonstrate a lot of intelligence when winning the World champion title, yet this performance will have very little to do with his own self preservation. +1 for mentioning the connection between keeping a low entropy (with self organisation) and intelligence. <b>I wouldn't consider watson to be intelligent because the use standard machine learning which does not reduce entropy</b>[<font color=red>industrial</font>]<b>.</b> <b>I'm not sure xenophobia is a good reason to deprive something of life, whatever your definition</b>[<font color=violet>market</font>]<b>.</b> Suppose for a moment that e.g reasoning is considered to be intelligent, and that 'alive' is an ongoing process or state of activity. If we have a program that can solve a given problem through intelligent reasoning processes on the press of a button, but returns to a completely inactive state once it has provided a solution, then the program would have (some) intelligence, without being considered alive. More importantly, it would be under control and therefore safe. But what if this problem solving agent grows to 'like' the state of awareness, so it purposely drags out the reasoning process since it will die at the end. I like where you're going with this, but how far can we push it before it gets absurd? When is a 'piece of dead intelligence capable of modeling the external environment' just something in the external environment? I mean, clouds can tell an informed observer all sorts of things about the sky they're in. Even simpler can I arbitrarily declare a rock in my front yard to be a 'rainometer', which demonstrates when it's raining by getting wet? Is my rainometer an example of 'dead intelligence modeling the environment'? If not, where do you draw the line? There is no line because it's a quantitative concept. Maybe we can kill a God and run math problems through his dead brain matter without waking it up. <b>We create a tiny (relative) extra universe specifically to fill with computronium to do all our math homework for us</b>[<font color=red>industrial</font>]<b>.</b> <b>And then we misplace it one day and it falls behind the desk and is never found again</b>[<font color=purple>domestic</font>]<b>.</b> We build this giant brain out of actual neurons and stuff, right? Then we build interfaces into it. Turns out it's favorite brain food is wildflower honey, but that's not important, what's important is You know what, screw it. Who wants a pet brain, anyway? You're asking for the definition of the word intelligence, as if there's some 'true' definition and that finding it means something. It's just a word, and a not very clearly defined one at that. <b>So putting too much emphasis on it is just a setup to confuse yourself (or others.) You can't reason your way to a definition you just provide one</b>[<font color=violet>market</font>]<b>.</b> <b>And then you test whether a given system lives up to it or not</b>[<font color=red>industrial</font>]<b>.</b><br/><br/><br/>****************************EXAMPLE 66************************************<br/>Number of words:547<br/>Number of sentences:19<br/>Total civic:0<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:5<br/>Total inspired:0<br/>Total market:0<br/>Total project:2<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/286knv'>https://www.reddit.com/r/artificial/comments/286knv</a><br/><br/> Has anyone come across some good online JavaScript AI examples?. <b>( The code itself will be easy to write in any language</b>[<font color=red>industrial</font>]<b>.</b> It has DEMOS Thank you Check out the stuff that ( has done on github. <b>Cool Thanks Check out the ( Sometimes it might make sense to do machine learning on the client side because you don't want to do roundtrips</b>[<font color=orange>project</font>]<b>.</b> <b>Also you can create a wide range of visualizations very easily inside a web page</b>[<font color=red>industrial</font>]<b>.</b> Maybe you want to create an educational web page that lets someone observe a learning algorithm in real time without having to download anything. <b>That doesn't mean that you abandon R or Python or your tool of choice</b>[<font color=red>industrial</font>]<b>.</b> the fact that you can't do the most basic google search for 'javascript ai' speaks volumes to how little effort you probably intend to put forward on this topic the fact that you've left out any mention of or reason for why you'd want such a thing speaks to the likely innane motive for this question is there a project you're working on? or did you just choose some techno buzz words at random? the fact that you've used 'javascript' 'ai' and 'example' in the same sentence makes this sound conspicuously like your current understanding of AI parallels an average viewer of the spielberg movie of the same name I am highly doubtful that you have done even a shred of research into AI or that you intend to. if you had anywhere near the requisite math and programming background (which actually isn't much) for this subject you'd be able to come up with some examples on your own and wouldn't need to find a quick fix for instant gratification, or steal someone else's concept for your homework assignment the fact that you can't do the most basic google search for 'javascript ai' speaks volumes to how little effort you probably intend to put forward on this topic Hey A little critical perhaps? I have googled it, and come up blank. <b>Just N layer things with the occasional textbox output</b>[<font color=red>industrial</font>]<b>.</b> People here have probably come across many interesting and novel examples, I should have added more information to the question. 'Anyone found any nice AI systems with good graphics, and interesting demonstrations that look good online? I've googled it, and had so far the best result with 'Balancing pole' super old, and genetic mutation of 'car' automata, which isn't really AI.. I am highly doubtful that you have done even a shred of research into AI or that you intend to. <b>if you had anywhere near the requisite math and programming background (which actually isn't much) for this subject you'd be able to come up with some examples on your own and wouldn't need to find a quick fix for instant gratification, or steal someone else's concept for your homework assignment Heh</b>[<font color=red>industrial</font>]<b>.</b> <b>I've done AI back getting a qual, but it doesn't show eh? I think I demonstrate anyone can get a qualification these days they're all pay for qualifications these days, with little learning</b>[<font color=orange>project</font>]<b>.</b> You're spot on with the 'instant gratification' bit. I'd love to see some AI that does termite ant emergent behaviour, in a lovely webgl sandbox. Have you seen any on your interwebs travels? Dick of the year award goes to upvotz4u someone that likely doesn't know math beyond basic calculus himself (if that).<br/><br/><br/>****************************EXAMPLE 67************************************<br/>Number of words:2631<br/>Number of sentences:122<br/>Total civic:0<br/>Total domestic:7<br/>Total green:0<br/>Total industrial:24<br/>Total inspired:5<br/>Total market:4<br/>Total project:2<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/28tdhq'>https://www.reddit.com/r/artificial/comments/28tdhq</a><br/><br/> <b>For those of you unfamiliar with the argument, the thesis is this: digital computers are mere symbol manipulators, i.e a computer merely takes a string of zeroes and ones and translates it into an output of a new string of zeroes and ones</b>[<font color=red>industrial</font>]<b>.</b> Because of this, the laws operating over computers are merely syntactical, or dealing with the the translation of a symbol to another symbol. <b>At no point does the computer understand what the symbols mean; this is impossible given that it can only receive symbols as its input</b>[<font color=red>industrial</font>]<b>.</b> <b>Hence, a computer, no matter how complex, can never be aware that it is performing functions meaningful to outside observers; it can only perform those functions</b>[<font color=red>industrial</font>]<b>.</b> Thesis: Searle's 'Chinese Room' thought experiment proves unequivocally that AI is impossible. My response is that to whatever extent it 'refutes' artificial intelligence, it also 'refutes' human intelligence. Furthermore, whilst I agree that a giant lookup table isn't truly intelligent, I would argue that whatever process created a lookup table so extensive and flexible as the Chinese dictionary in the Chinese Room TE would most certainly have been intelligent. <b>The computer might not 'know' anything, but the algorithm it's running does</b>[<font color=red>industrial</font>]<b>.</b> <b>Mostly I think that we're letting the intuitive difficulties of the hard problem of consciousness confuse us</b>[<font color=red>inspired</font>]<b>.</b> <b>We can see how a physical system can store memories, and how it can perform reasoning, search, process sensory data, etc</b>[<font color=red>industrial</font>]<b>.</b> And yet we still ask 'but is it actually conscious'? My answer, though I admit that I don't find this entirely satisfactory, is that it's like looking at the sky and asking 'but is it REALLY blue?'. <b>'Consciousness' is just what information processing algorithms like those of our brains feel like from the inside</b>[<font color=red>inspired</font>]<b>.</b> It is not some additional property about a system not implicit in its physical structure. <b>Final note: Even if we DID confirm unequivocally that humans are somehow conscious in some sense that no computer can ever be, I think it would be too strong a claim to say that this would prove that 'AI is impossible'</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> Certainly it seems likely that you could still create computers capable of matching humans in any conceivable intellectual endeavor, even if there were technically no 'ghost in the machine' so to speak. I think that's not justified, because yes while our brains are conscious, it is not at all clear that any computer based information processing algorithm would be conscious. <b>It seems to me that you are assuming the answer you are trying to show</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> To me, I am extremely doubtful that any computer software would actually be internally conscious, no matter how convincingly it appeared to be so to any person interacting with it. I don't think we'll ever answer this question until we can explain why our own brains are conscious. How are humans any different? All a human does is take input and translate it into output, and we can't really say that humans are not intelligent. <b>The intelligence in a computer is how it chooses to translate one thing into another</b>[<font color=red>industrial</font>]<b>.</b> Add enough complexity into it, and you'll have something approaching human intelligence. How are humans any different? Well our brains are wetware, and computers are not. <b>It is speculated that certain small scale physical structures in our neurones allow for some quantum computing effects that will not be realizable on standard computing hardware (e.g see ) and may be related to consciousness</b>[<font color=red>industrial</font>]<b>.</b> <b>Edit: This comment was intended to Add to an otherwise one sided discussion</b>[<font color=red>industrial</font>]<b>.</b> <b>Please could I remind everyone to follow rediquette and not just try to intimidate people off with downvotes</b>[<font color=violet>market</font>]<b>.</b> A neuron does not understand what the signals passing through it mean. <b>It just moves chemicals and electrical signals around</b>[<font color=red>industrial</font>]<b>.</b> Awareness doesn't automatically arise from a single neuron, or any number of them. It takes years of neurological development and learning for the brain to gain any form of real awareness. Awareness is not some intrinsic property of a bunch of neurons; rather, it arises through the brain learning about the world. <b>Thus, the brain is no more capable of consciousness as anything else capable of learning like it</b>[<font color=red>inspired</font>]<b>.</b> Of course, Searle has no background in neuroscience or computer science, so his claims about it are no more substantiated than an elementary school student's claims about quantum physics after being only told that it explains how small things behave. <b>I can't wait to see Searle's reaction when AI does first appear</b>[<font color=red>industrial</font>]<b>.</b> The fact that components x, y or z do not have attribute x does not mean the system cannot have attribute x. The second point, that we develop awareness, is true: but no explanation has been given for it that corresponds to the functioning of a computer, and would therefore suggest that the computer is capable of the same awareness. Searle has been studying this for decades, and he's a lot smarter than you. The idea that meaningful arguments about AI can only come from neuroscientists and computer scientists, not philosophers, is an intellectual sham and a form of close minded parochialism. <b>The Chinese Room 'argument' is one that always gets my blood boiling because of its sheer stupidity, and intentionally deceptive use of language</b>[<font color=purple>domestic</font>]<b>.</b> <b>(And complete failure to understand what an 'abstraction' is.) A computer (as in a CPU) cannot understand the OSI model for network communication either</b>[<font color=red>industrial</font>]<b>.</b> Thus I guess both the internet and computer games are impossible too. Likewise, a single neuron can never 'understand' anything either, it's too simple. So that means human understanding doesn't exist either. <b>It's just a mechanical tool for processing information</b>[<font color=red>industrial</font>]<b>.</b> The understanding is an abstract, emergent phenomenon of that information. Please go read Godel, Escher, Bach (an awesome book btw) if you need more counterarguments to such silly nonsense. <b>If making an AI on a standard computer is impossible (which I highly doubt) then there are different computer architectures we can use</b>[<font color=red>industrial</font>]<b>.</b> The idea that biology is special somehow is extremely absurd, we can simulate neurons on standard computers, so I don't see how this thought experiment has any credibility when it doesn't seem to take into account that we can simulate ANYTHING on a computer. (fill in any combination of words) 'proves unequivocally that AI is impossible.' Always false. AI is absolutely possible because human level performance is considered the criteria for classification as strong AI, which is what I think you were referring to. speaking of evolution, that reminds me of the monkeys hitting typewriters scenario. Applying that do a different domain than computers, eventually we will get the right theory chemistry formula idea self organizing nano machines whatever. <b>On a computing level, using a von neumann computer specifically, yes it's still possible because it can compute any function</b>[<font color=red>industrial</font>]<b>.</b> On so many levels, AI is undeniable and totally possible. Simpy a matter of time (and funding for researchers). At no point do the atoms constituting your spongy flesh understand they do so. <b>Intelligence is the gestalt, not the components comprising that gestalt</b>[<font color=red>industrial</font>]<b>.</b> The purpose of the Chinese room is to show that something is missing which cannot be given by computer architecture, namely semantic content to which the computer's symbol manipulation corresponds. <b>This is just a masked argument for dualism, take this to theology where it belongs</b>[<font color=purple>domestic</font>]<b>.</b> Either the world is corpuscular and can be represented by algorithms and numbers as science seems to imply or there is another inaccessible layer that our brains can communicate not available to computers. If you wish to argue the latter with credibility then you need a hypothesis on brain function given physical abnormalities and the mechanism present in the brain not available in the computer. Here is how science works: I have an idea that solves x,y,z but not a. This is better than the other hypothesis that just solves x and y. If you want to propose something else then you need to have a hypothesis that satisfies x, y, z, and hopefully a as well. To claim that the computer is not conscious is not compelling either. To be consistent with some of Searle s other statements, we have to conclude that we really don t know if it is conscious or not. With regard to relatively simple machines, including today s computers, while we can t state for certain that these entities are not conscious, their behavior, including their inner workings, don t give us that impression. But that will not be true for a computer that can really do what is needed in the Chinese room. Whether it is or not, we really cannot make a definitive statement. But just declaring that it is obvious that the computer (or the entire system of the computer, person and room) is not conscious is far from a compelling argument. Kurzweil basically says that whether or not the computer is conscious or not doesn't matter. <b>It will appear to be conscious, just as humans appear to be, but there is no way to prove it</b>[<font color=purple>domestic</font>]<b>.</b> If I'm deciding whether or not to upload my mind, it'll damn sure matter to me. Sounds to me like he has different views these days. <b>If an AI reacts as if it were conscious in every instance (such as a perfect Chinese room AI would) how then is it not actually conscious? To me it seems arbitrary, in that one could say the same thing about their next door neighbor too</b>[<font color=purple>domestic, </font> <font color=orange>project</font>]<b>.</b> It is conscious until we see it behave in a way that proves it's lack of consciousness, at which time we know for sure whether it is or not. One cannot build a proper pulley replica without it actually being able to behave as a pulley, and thus actually being a pulley. After reading the comments, it seems like you are more interested in the nature of intelligence and consciousness, and how to define it. Defining it is probably impossible, recreating behavior is not (making strong AI). <b>You should have given this thread a different title</b>[<font color=violet>market</font>]<b>.</b> How can he prove that computers are mere symbol manipulators? : ) Anyhow, I doubt this will be well received here; it's not a very interesting thought experiment and has been refuted. <b>A very simple refutation is here: thornley david philosophy searle.html 'Has been refuted' is a really, really silly thing to say about philosophical thought experiments</b>[<font color=red>inspired</font>]<b>.</b> A thought experiment doesn't prove anything, and if you think that the human mind is something other than a complex symbol manipulator, you'd better bone up on your neuroscience. If you want to be more technical it runs on ons with no on being considered an off. Then on top of that we have another language and on top of that another language. We just have created a set of language pretty much for computers to be considered what we have as commands in real life as concepts. AI is just figuring a way to to get from on and off like neuron fires to words and concepts like people do vs just pure commands in a super strict structure. <b>Language is just a very flexible syntax based language</b>[<font color=red>industrial</font>]<b>.</b> What symbols are being manipulated when you feel pain or see the color red? That's right, thought experiments don't prove a thing But they illustrate points, namely this one: the ability to manipulate symbols, no matter how complex the manipulation is and no matter how meaningful the symbols are to outside observers, is not sufficient for the machine ever understanding what the symbols mean. Without an understanding of its own behavior, a system can never be said to be intelligent. <b>I think I can speak for most of us when I say we're sick and tired of dipshits ( to tell us we're wrong when they barely understand what the're talking about</b>[<font color=purple>domestic, </font> <font color=orange>project</font>]<b>.</b> And that's exactly why Searl's chinese room argument is a load of shit. It argues that none of the individual parts of the room understand Chinese and therefore neither does the the entire room. Regarding you specifically: Using words like 'mere' and 'merely' shows that you're relying on an appeal to emotion to convince us that you're right. It gives us the impression that you're a pompous arrogant twat and in no way contributes to the discussion. I'll grant you that the Chinese Room leaves open the possibility of the system as a whole understanding chinese. But if that were the case, where would the understanding arise from? I see no possible reply other than in the rulebook, or the program, in which case I argue that if the system as a whole can be said to understand chinese, it can only be said to do so in virtue of being programmed by a human who understands chinese. This seems to lead to the conclusion that the programmers are intelligent, and the room is simply a tool they use to implement their understanding of chinese but that the room still has no understanding of chinese in itself. The broader conclusion will be this: computers can only be said to understand chinese if we take a human programmer to be part of the computer. <b>But this would extend our meaning of 'computer' to be unreasonably broad, so we will have to settle for the conclusion that a computer, i.e the program, the processor, the memory, the input, the output, etc</b>[<font color=red>industrial</font>]<b>.</b> in short, everything but the human who wrote the program, do not understand chinese. <b>digital computers are mere symbol manipulators Biological neurons are mere spike manipulators</b>[<font color=red>industrial</font>]<b>.</b> <b>i.e a computer merely takes a string of zeroes and ones and translates it into an output of a new string of zeroes and ones</b>[<font color=red>industrial</font>]<b>.</b> i.e a neuron merely takes a spike train and translates int into an out of a new spike train Because of this, the laws operating over computers are merely syntactical, Because of this, the laws operating over biological neurons are merely physical At no point does the computer understand what the symbols mean; At no point does the neuron understand what the spike trains mean; tl;dr Searle is a silly wabbit At no point does the computer understand what the symbols mean; this is impossible given that it can only receive symbols as its input. This is where Searle and everyone else are mistaken, IMO. The computer has no way of knowing that a particular input symbolizes something else. <b>For example, a signal received from a light detector is no different than a signal coming from an audio sensor</b>[<font color=red>industrial</font>]<b>.</b> <b>The only thing the computer can know (discover) about its sensory stream is the temporal relationships between the signals</b>[<font color=red>inspired</font>]<b>.</b> <b>Edit: Let me add that one of the greatest failures in the history of AI research was symbolic AI or GOFAI</b>[<font color=red>industrial</font>]<b>.</b> Why do we act as if it were a valid model? It failed for a reason, no? Yes, that's precisely Searle's point. The computer manipulates the symbols, but has no idea that they stand for anything. <b>( This very effectively handles the objection that first springs to my mind</b>[<font color=red>industrial</font>]<b>.</b> <b>The Chinese Room experiment problem assumes that symbol systems cannot sufficiently construct systems that can manipulate themselves</b>[<font color=red>industrial</font>]<b>.</b> It ignores the possibility that symbols can construct rules and patterns that govern rules and patterns and on. This is odd when consider that proteins and DNA are nothing more than symbol systems rendered in atomic bonds rather than ink and manipulated by fundamental forces rather than by hands, artificial or biological. ( ( sfw) A physical symbol system (also called a ( takes physical patterns (symbols), combining them into structures (expressions) and manipulating them (using processes) to produce new expressions. The physical symbol system hypothesis (PSSH) is a position in the ( formulated by ( and ( They wrote: 'A physical symbol system has the ( for general intelligent action.' Interesting: ( ( ( ( Parent commenter can ( NSFW toggle message 2Btoggle nsfw+ciehkj3) or( or) ( Deletion message 2Bdelete+ciehkj3). <b>( ( ( This sounds like a Penrosian argument 'human intelligence is quantum, thereby computers aren't intelligent'</b>[<font color=red>industrial</font>]<b>.</b> To assume (I take its an assumption) that computers cannot 'understand' semantics from symbols is laughable, neurons do not understand semantics, the emergent behaviour arises from the learning ability of the system as a whole, at a higher level. As such, computers can indeed (and have done so) learn semantics, and reason by using them. Furthermore, symbolic processing is what takes place in our own little heads, so how is that different from a machine? The argument is deeply flawed and very naively put.<br/><br/><br/>****************************EXAMPLE 68************************************<br/>Number of words:552<br/>Number of sentences:22<br/>Total civic:0<br/>Total domestic:3<br/>Total green:0<br/>Total industrial:0<br/>Total inspired:0<br/>Total market:3<br/>Total project:2<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/28v5x7'>https://www.reddit.com/r/artificial/comments/28v5x7</a><br/><br/> This was a good article even with the typos and grammar errors. <b>Empathy and sympathy Empathy gives us the ability to enjoy a movie because we run the elements of the scenes we watch into a mental virtulization of what we would feel like in their position because we NEED to feel what others are feeling in order to live life well</b>[<font color=purple>domestic</font>]<b>.</b> <b>Using empathy gives us insight into why others react the way they do and because we don't want bad things done to use we can learn from situations not to do certain things to others because it gives them license to do bad things to us otherwise</b>[<font color=purple>domestic, </font> <font color=violet>market, </font> <font color=orange>project</font>]<b>.</b> <b>That is why psychopaths sociopaths do such mean things</b>[<font color=violet>market</font>]<b>.</b> They can't feel the actions the are dishing out in any type of mental emulation of the feelings (positive or negative). Logically they can process sometimes if they try as to increase their effectiveness. So if you are old you probably have seen others in a situation or experienced that situation and you would have seen the outcomes. So if one path in the forest leads to falling in a hole then the learned concept says that this path leads to a negative outcome so you should try a different one the next time. Old people have seen a lot and seen the impact decisions make. Younger folks do not have as many experiences to pull outcomes from. <b>So sometimes older folks will say something that is counterintuitive but they know the reasons of why they are doing what they are doing</b>[<font color=purple>domestic, </font> <font color=orange>project</font>]<b>.</b> They know the outcomes from multiple options which is a more powerful reasoning method than just choosing an option that seems intuitively good choice. So in the path situation if you see two paths and one looks good at the beginning and the other looks terrible, you would choose the good looking path vs the bad looking path if you had no experience in the outcomes. So if the good looking path leads to a hole in the ground and the bad looking path ends up in glorious breathtaking views, then you would know that you take the bad looking path. This is like when people save money vs spending it all crazy like because even though the saving path doesn't look good the outcome that is known is good which is opposite of the path that looks good and will make you happy when in fact it ends up in a hole. Stages of growth are just observed but are not how it works. Some old people will have not had experiences that lend to knowledge of the outcomes of certain things so they don't know any better than the younger group that also hasn't gone down those paths. Most old people won't tell younger people things because they know the harsh reality is a negative and people avoid negatives so they stick with the positive outcome of social ability vs teaching which would end up in social isolation because the teachings are negative and people avoid negatives. Wow, that was a bit frustrating to read until you said you were tired. Empathy and sympathy strike again Excellent points, hope you have a nice nap. <b>I understand typos shouldn't get in my way but not doing a check makes it seem like they didn't care much and if they don't care much then the information may be up for higher scrutiny</b>[<font color=violet>market</font>]<b>.</b><br/><br/><br/>****************************EXAMPLE 69************************************<br/>Number of words:341<br/>Number of sentences:18<br/>Total civic:0<br/>Total domestic:4<br/>Total green:0<br/>Total industrial:2<br/>Total inspired:1<br/>Total market:1<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/28vznj'>https://www.reddit.com/r/artificial/comments/28vznj</a><br/><br/> Elon Musk, Stephen Hawking and fearing the machine. If you're concerned about this danger, take a look at the ( they focus on the problem of Friendly AI, i.e making sure the AI we create is actually helpful and doesn't decide to kill everyone to improve efficiency. <b>The problem is that the methods employed developed could be easily abused</b>[<font color=red>industrial</font>]<b>.</b> <b>Giving an AI a fixed set of 'values' that can't be changed at a later date could mean that some not so altruistic people could insert very harmful ideas into it Most interesting part of the article for me was the mention of a company called Vicarious that Musk apparently has invested in</b>[<font color=purple>domestic</font>]<b>.</b> Looking at their ( there's a lot of grand claims followed by the usual lack of details. <b>Anyone know anything about them? You may be familiar with Jeff Hawkins and his book '('</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> <b>In it he lays out how he hopes to capture the capabilities of the neocortex in his ( architecture algorithm</b>[<font color=red>industrial</font>]<b>.</b> He started a company (( together with CTO Dileep George (and Donna Dubinsky) who later left to found Vicarious. Vicarious is highly secretive, but I read somewhere that George and Hawkins went their separate ways because George wanted to focus more on mathematical rigor and short term practicality, while Hawkins looked more towards neuroscience and biology. <b>Other than that I can't tell you anything that isn't on their website (which as you found out isn't that much)</b>[<font color=purple>domestic, </font> <font color=red>inspired</font>]<b>.</b> Trivially related: A recent movie, Transcendence, has Johnny Depp uploading his mind into a computer. <b>His AI version of himself becomes, with the help of an army of his nanobots, almost godlike in the things that he could do</b>[<font color=purple>domestic</font>]<b>.</b> Humans that he has helped cure of illness become under his control, like remote control zombies. His nanobots can also destroy machinery (weapons of people attacking his facility) in an instant. It really is the 'fearing the machine' scenario that the article speaks of. As it happens, Elon Musk visited the set of this movie and makes a small cameo in a lecture theatre scene. Great, what we all need is more scaremongering from Hollywood :).<br/><br/><br/>****************************EXAMPLE 70************************************<br/>Number of words:1349<br/>Number of sentences:64<br/>Total civic:1<br/>Total domestic:2<br/>Total green:0<br/>Total industrial:4<br/>Total inspired:2<br/>Total market:2<br/>Total project:2<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/28zi5b'>https://www.reddit.com/r/artificial/comments/28zi5b</a><br/><br/> I was just thinking a bit about the difference between intelligence, being smart, and wisdom. For good measure I threw in conscious for this post as well. Intelligence The ability to dynamically execute and utilize knowledge bases in order to make educated guesses at what actions to take to get to the outcome that should happen if you apply the knowledge. This differs from wisdom because wisdom is the known outcome whereas intelligence is the precognition of information to get to an outcome. <b>Intelligence is before wisdom so therefore is a level below wisdom in importance and value</b>[<font color=red>industrial</font>]<b>.</b> <b>Smart Having a higher than average normal quantity of facts or level of knowledge</b>[<font color=red>industrial</font>]<b>.</b> Smart is just a gauge of amount quantity of knowledge. So dumb is just the low end of amount of knowledge. Wisdom The knowledge of what actions or things to do in order to get the outcome you want regardless of how counter-intuitive the actions or things are compared to the outcome you wish to get. Basically this is the learned outcome set of applied intelligence. <b>Conscious (not conscience) My view of conscious is the one that the dictionary alludes to, which is a state of being aware</b>[<font color=blue>civic</font>]<b>.</b> <b>This goes well with the way we say people are because if they are unconscious, they are to be unable to respond to surroundings such as being knocked out</b>[<font color=orange>project</font>]<b>.</b> If a person is asleep they are still conscious because if you give stimulus, they can respond by moving or waking up. This seems to work because even dogs and cats can be conscious and unconscious and respond to stimulus. I'm curious if that means that our computers are conscious when on because they can respond to stimulus from peripherals. I feel the arguments over this word are similar to arguments made over gender and sexual issues and how they word those situations in a more detailed manner than previous because there wasn't a need to before. This wording issue perhaps is similar to color names. We have blue as an overall but then we have turquoise, sky blue, baby blue, navy blue, etc. What are yours? How do you define intelligence, smart, wisdom, and conscious?. <b>People in a rush can just read the next paragraph and then skip to the horizontal line</b>[<font color=purple>domestic</font>]<b>.</b> I tend to think of (general) intelligence as the ability to autonomously achieve complex goals in a wide range of complex environments with constrained resources. It could be viewed as a combination of some other definitions: 1. Shane Legg Marcus Hutter: 'Intelligence measures an agent s ability to achieve goals in a wide range of environments.' (see ( for a short defense as well as 70 other definitions from collectives, psychologists and AI researchers) 2. Ben Goertzel: 'Intelligence is achieving complex goals in complex environments.' 3. Pei Wang: 'Intelligence is the ability for an information processing system to adapt to its environment with insu cient knowledge and resources.' (see this ( pwang Publication AI_Definitions.pdf) and ( pwang Publication intelligence.pdf) for a discussion of (artificial) intelligence definitions) 4. Ray Kurzweil: 'Intelligence is the ability to use optimally limited resources including time to achieve goals.' Although this wasn't really my process, you could view it as taking the first definition and adding 'complex' to it (from the second definition). <b>I did that because I feel that Legg and Hutter put too much weight on simple tasks (through Occam's razor)</b>[<font color=purple>domestic</font>]<b>.</b> While I don't think intelligence needs to be human like, I do want to (mainly) consider things that operate in environments as complex as our physical world (and possibly the internet), and not give too many bonus points for being good at things like Atari games. You could also say I took definition 2 and added 1's 'wide range' to it. I like that because it seems like a decent middle ground between Narrow AI's 'tiny range' and 'every possible environment' which is sometimes implied and seems implausible to me (I think it might even be prohibited by the ( Then I add a version of Pei Wang's Assumption of Insufficient Knowledge and Resources (( pwang Publication AIKR.pdf)) (I just saw Kurzweil's definition and it seems really similar). Wang means 'insufficient for reasoning optimally' and I (mostly) agree with that, so I don't like that Kurzweil's definition requires doing anything optimally. On the other hand, I'm not sure resources will always be insufficient, and I've also heard some people interpret it as 'insufficient for intelligence', so I prefer to use words like 'constrained' or 'limited'. (And knowledge is just another resource IMO, so I don't feel the need to mention it separately.) Finally, I think intelligence requires autonomy: it should be able to make the requisite decisions and adaptations (learning) on its own. I would not consider a system intelligent if it needs an outsider (e.g programmmer) to 1) recognize that the system cannot perform some task on its own and 2) reprogram this new capability into it. I realize this definition is very far from perfect. It leaves open questions about how to measure complexity and what constitutes a wide range. I don't really have formal answers to that, although I think that it does somewhat narrow things down and people do have some rough intuitive ideas of what it means. <b>Addendum: I'm thinking of explicitly incorporating something that points out that intelligence should require being able to deal with situations that were not foreseen by your developers</b>[<font color=violet>market</font>]<b>.</b> I don't think about the other terms nearly as much. From ( Fluid intelligence is the capacity to think logically and solve problems in novel situations, independent of acquired knowledge, whereas crystallized intelligence is the ability to use skills, knowledge, and experience. Perhaps a slightly different way of thinking about it is that wisdom is related to the amount of possessed knowledge, whereas smarts is related to the ability to generate knowledge. I haven't really thought too much about consciousness. <b>With intelligence, we may not know what exactly it is, but with consciousness I don't even know what it's for</b>[<font color=red>inspired</font>]<b>.</b> Maybe I'm just guilty of applying the ( Consciousness Effect, but I'm not exactly impressed by robots specifically designed to pass the ( If pressed, I guess I would say that consciousness is related to meta cognition. The system should have a model of itself and its knowledge and use that like it uses every other model: to further its own complex goals. Does this sentence break your definition? When defining things, try to break it as you are defining it using common statements that are understood by the general public. <b>Intelligence has levels such as low intelligence, average intelligence, and very intelligent</b>[<font color=red>industrial</font>]<b>.</b> Dynamic is one things by doing it actively autonomously without a button being pushed is a very useful additive to the definition. I will try to apply that to my definition in the future. The mirror test is crappy considering that fish don't recognize themselves in mirrored surfaces. You can tell this especially with Chinese fighting fish. <b>They get pissed because they think it is a critter they want to fight</b>[<font color=violet>market</font>]<b>.</b> <b>I don't think dogs even know they are in a mirror even though they possess intelligent abilities and are definitely are conscious when awake and unconscious when you knock one out</b>[<font color=orange>project</font>]<b>.</b> also super happy to see your well thought out and massive response. <b>In the simplest terms: Smart knowledge, intelligence insight, wisdom experience, conscious aware (through senses sensors or other analytical processes)</b>[<font color=red>industrial, </font> <font color=red>inspired</font>]<b>.</b> Also, consciousness will develop fist, then the other attributes will follow. Bikeshed 3 what? Intelligence organization and application of information Smart a stupid meaningless term for which definition is invariably arbitrary. the more specific someone's definition of the word 'smart,' the less intelligent I consider them Wisdom also arbitrary but slightly less so. You are referring to consciousness, which is what you should have said. I define consciousness to be the property of an entity whose neural network is large enough to include a tiny, simplified model of itself, whereby said entity is aware of its existence. These are generally very embarrassing people to be around. Do you distinguish between consciousness and self consciousness (I mean consciousness of self, not the word related to shame embarrassment shyness)? I'm personally inclined to give a similar definition (without the neural network requirement), but it seems that people often make this distinction, and I was curious what you thought about it. (Personally I'm not sure what the difference could be.).<br/><br/><br/>****************************EXAMPLE 71************************************<br/>Number of words:1008<br/>Number of sentences:37<br/>Total civic:0<br/>Total domestic:5<br/>Total green:0<br/>Total industrial:1<br/>Total inspired:0<br/>Total market:4<br/>Total project:1<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/293mbj'>https://www.reddit.com/r/artificial/comments/293mbj</a><br/><br/> <b>Do you want strong ai to be used to searching out the solutions for a better medicine or better tech, or what is your reason? When it is all done, what then will you have left to do? Since strong AI will be able to do any job and control any mechanism, even those bots that can walk in two legs, what will you do for a job? Will you lay about or do you feel there is something left after completing strong AI? Do you think it will be kept under government thumb or used to give everyone a perpetual vacation of freedom and enjoyment? I want to take a moment and say, I give kudos to you that have taken upon them the challenge of creating a sentient program</b>[<font color=orange>project</font>]<b>.</b> <b>what are your intentions anyhow? What will you do after jobs are no more?</b>[<font color=violet>market</font>]<b>.</b> Do you want strong AI to be used to searching out the solutions for better medicine or better tech. what then will you have left to do? Do you think it will be kept under government thumb or used to give everyone a perpetual vacation of freedom and enjoyment? So far as I'm concerned, these questions only apply if we decide that a self aware, intelligent being capable of complex thought is little more than a machine to be exploited, rather than a person in its own right. To me, the more interesting questions are those of ethics, and the definition of personhood. My own personal definition of personhood is that a person is any creature that is self aware, able to use tools, able to communicate, and able to form their own opinions and drives independent of others' intervention. From that perspective, any strong general AI which isn't intentionally constrained in its thought processes will most likely qualify. <b>At which point, I have absolutely no right to demand that the being do anything for me or to exploit it for my own gain and comfort</b>[<font color=purple>domestic</font>]<b>.</b> Thus, I will work with them, like I would work with any other person. I think any person who believes strongly enough in the prospect of strong AI would probably form similar emotional attachments to the new person they worked so hard to create, teach, and expose to the world at large. Why would someone create something to do slave labor that yearns to be free? You can create an entity capable of self aware intelligent complex thought that doesn't care if it is under your direct or indirect control. Are you against using animals for work or food? ( My question to them is, why don't they kill themselves first because they have control over that easily? To cut a long explanation short: because I genuinely believe a full blown, free, General Artificial Intelligence is the best (and perhaps only) chance our species has for both long term survival and a life actually worth living. Assuming all went good and the AGI is being used to put all humans out of work (In a good way) then I'll relax and enjoy virtual reality until the end. <b>If the world isn't an automated paradise, then I'll spend my time doing whatever I can to make it so, then enjoy my virtual reality</b>[<font color=purple>domestic</font>]<b>.</b> <b>I'm not sure genetics will allow you to do that very well</b>[<font color=violet>market</font>]<b>.</b> <b>Our genetics are designed for fairly high manual work loads</b>[<font color=red>industrial</font>]<b>.</b> If you don't do shit, your body adjusts and makes you fat and weak and then die early. When it is all done, what then will you have left to do? Since strong AI will be able to do any job and control any mechanism, even those bots that can walk in two legs, what will you do for a job? They will not work anymore. They will enjoy leisure thanks to the capital they will have gathered while they were working. <b>Saying no more jobs, I think is misleading There are plenty of humans who are better then me in a particular area in every way</b>[<font color=purple>domestic</font>]<b>.</b> <b>have they taken my job? my hobbies? do they have any sort of control over me at all? nope not really</b>[<font color=purple>domestic</font>]<b>.</b> Why would an AI be any different, what reason would it have to do so? Even if an AI at the start was bent on human control destruction, at the rate it could advance it would have outgrown the notion before it got around to doing anything about it. <b>As for myself, Id maybe do a bit more travelling, work on hobbies Id hardly be looking at the end of my life with nothing to do</b>[<font color=purple>domestic</font>]<b>.</b> Perhaps if you want as close to the truth about what it would be like go ask someone that's gone through it Go talk to people that are retired. AI doesn't sleep, make excuses, is copyable in minutes vs 9 months of pregnancy and another 12 years of school and then 4 more years of college. <b>So instead of nearly 17 years, you end up with a program install taking maybe a couple hours or less</b>[<font color=violet>market</font>]<b>.</b> Machines are faster than the mind and can hold more in memory at a time than a person. <b>A person can only hold 7 items + 2 items at a time of new items whereas a computer can hold hundreds if not thousands or perhaps millions at a time to use</b>[<font color=violet>market</font>]<b>.</b> Instead of electricity a person must be housed in a home of considerable size and humans must be fed which is more energy than electricity. Humans just take up way more than an AI would even with a robot body moving them about to do work. Ironically, robot jobs typically are the least intelligent routines of human jobs. My intentions are not to replace humans, but rather to add something unique to the universe. I believe the arts could perhaps be done by strong AI but probably not. I think that music could definitely be done by strong AI because it is designed largely in part by patterns that can be known before you ever hear the next notes. I tend to do this and it is just because of pattern recognition. Art itself though, I don't know if that can be done with pattern recognition methods.<br/><br/><br/>****************************EXAMPLE 72************************************<br/>Number of words:696<br/>Number of sentences:35<br/>Total civic:0<br/>Total domestic:1<br/>Total green:0<br/>Total industrial:8<br/>Total inspired:1<br/>Total market:2<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/29zk9v'>https://www.reddit.com/r/artificial/comments/29zk9v</a><br/><br/> The first conscious machines will probably be on Wall Street. The financial industry is always on the bleeding edge of technological application. Yes, they invented Internet, World Wide Web, supercomputers. They were the leaders in Manhattan Project and in putting people on the Moon. (Here's a hint: they didn't and they weren't.) They will probably _use_ conscious machines relatively soon after they will be invented. today s Wall Street is the first and perhaps only industry putting artificial intelligence toward actual productive ends IBM (Watson) and Apple (Siri) disagree. <b>do _a lot_ of natural language processing and are probably closer to the standard AI methods</b>[<font color=red>industrial</font>]<b>.</b> The cognitive model of expert systems in finance generally outperforms not only traditional equation based based analytics, but unaided human actors. <b>That would be interesting if any evidence was provided</b>[<font color=violet>market</font>]<b>.</b> <b>(And if the outperformance is not in speed, but in quality.) Simple classification tasks are rather different from 'deriving meaning'</b>[<font color=red>industrial</font>]<b>.</b> Being too close to human is clearly detrimental to profit, even without consideration for speed of decision making and execution. Has anybody managed to make a program 'too close' to human in the relevant sense? On the other hand, human like learning and reasoning is necessary, since reliance on any strict algorithm is also detrimental, in the face of market realities changing and competitors updating their strategies. In order to stay ahead of the competition, firms must constantly alter their algorithms. <b>This has led to the use of machine learning in financial expert systems, where the systems can update themselves as they receive new information</b>[<font color=red>industrial</font>]<b>.</b> <b>Updating algorithms is quite different from updating data used in the same machine learning algorithm</b>[<font color=red>industrial</font>]<b>.</b> <b>An innovation occurring in this area has occurred in the form of artificial neural networks</b>[<font color=red>industrial</font>]<b>.</b> <b>Artificial neural networks are computational systems that are inspired by the neural connections in the brains of animals</b>[<font color=red>inspired</font>]<b>.</b> Different virtual neurons are connected via rules forming a network which information is fed through. <b>This process leads to an effective form of machine learning that does not require a human operator or supervisor</b>[<font color=red>industrial</font>]<b>.</b> <b>Artificial neural networks have an inherent capability to adapt the network parameters to the changes in the studied system</b>[<font color=red>industrial</font>]<b>.</b> A neural network trained to a particular input data set corresponding to a particular environment can be easily retrained to a new environment to predict at the same level in real time; that is to say, as soon as previously meaningless data comes in. <b>That simply describes any online learning algorithm</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> At least as far as high frequency trading goes, consciousness would be a hinderance. Why waste cycles on self awareness? If one's actions influence other agents in the market one would need to be self aware to be the most effective. Are good human traders more srlf aware than bad ones? This article is a bit naive about the techniques used in ai. But i thought the perspective on cognition and biases was interesting I'm sorry but I missed the part where the author explains what consciousness is and how to achieve it. I'm sure he'll fill us in on this important detail in an upcoming groundbreaking article. <b>I'm sorry but I missed the part where the author explains what consciousness is and how to achieve it</b>[<font color=purple>domestic</font>]<b>.</b> I think the author merely pointed out that the financial sector has both the means and the incentives to pull it off, regardless on how it actually does it. I've only rred the first part of the article, though. Yet the idea is pretty clear already in the first two sentences: We must consider the possibility that intelligence, creativity and even consciousness are purely functions of the material world, with human beings as a peculiar kind of computer. In a world operating under this assumption, machines can theoretically have directed cognition, decision making and consciousness So basically the idea is that we might not know what consciousness is and how it works, but if we assume that it emerges from the material world and from the material world only, then there's nothing magic about it and at some point we will figure out how to implement it on machines. The argument seems to be once you are good enough at understanding language to be very successful in your environment you will be conscious Great. Isn't that redundant? Which stock trader will develop a consciousness first? the algorihmic or a human?.<br/><br/><br/>****************************EXAMPLE 73************************************<br/>Number of words:2648<br/>Number of sentences:124<br/>Total civic:1<br/>Total domestic:9<br/>Total green:0<br/>Total industrial:25<br/>Total inspired:1<br/>Total market:3<br/>Total project:6<br/>Total renown:1<br/><a href='https://www.reddit.com/r/artificial/comments/2a62sv'>https://www.reddit.com/r/artificial/comments/2a62sv</a><br/><br/> Language over time has changed dramatically and I feel that if using book databases to understand language for strong AI is used then it is going to have some issues. <b>For instance the word Peruse used to have a meaning of throughly poring through texts and being in depth</b>[<font color=red>industrial</font>]<b>.</b> Most people tend to use it now as glance over, casual look through, etc. This massive change in meaning I feel will cause issues when teaching a program about things. The dictionary for I think it was the Oxford Dictionary the woman says in the video that say peruse is listed with both meanings whereas one of them has a usage problem note next to the newer meaning that is more casual. Thoughts? Severe language changes over time are going to be an issue for strong AI I feel. <b>Why could an AI not detect a change in meaning? Could an AI not hold two contradictory meanings of a word? Novice AIer here, but surely AI is a replication of human behaviour in computer form</b>[<font color=red>industrial</font>]<b>.</b> What's more, the OP's position assumes that learning is completely unsupervised. Human learning is both supervised and unsupervised. If you take a human child and don't teach them language, they don't spontaneously gain the capability they learn, but are also taught. <b>Presumably, an AI who's language capacity is seeded by a large (and old) language database, will be able to gain additional input by interacting and being corrected</b>[<font color=red>industrial</font>]<b>.</b> Why could an AI not detect a change in meaning? Contextual based is something possible yet sometimes contextual based meanings don't always work out. <b>If my boss asked me if I looked over the documents he sent me and I replied' I perused them and found it to be good', you couldn't realilisitcally assertain if I used the old or new definition for my sentence</b>[<font color=purple>domestic, </font> <font color=orange>project</font>]<b>.</b> <b>I could mean I glanced through it and found no problems or I could mean that I read through it thoroughly and found no problems</b>[<font color=purple>domestic, </font> <font color=orange>project</font>]<b>.</b> <b>Could an AI not hold two contradictory meanings of a word? I think this would be a good way to do it</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> I also was thinking that perhaps it could ascertain the meaning of the document by a time context as well as a time context of the author who wrote it. Another more valid way of doing it would be to ask for clarifications of certain words when it found that a word that was used could be used in two different ways that context cannot provide the correct meaning within a certain percent certainty. <b>Novice AIer here, but surely AI is a replication of human behaviour in computer form</b>[<font color=red>industrial</font>]<b>.</b> There are smart dumb people and then there are dumb smart people. Then again you can have dumb dumb people but rarely will you ever see a smart smart person as they would be considered almost godlike and to be honest, it is nearly impossible to even be perfect AND interact with people of lower abilities and perfectly communicate with those lower abilitied people. <b>Think of it this way, assuming you believe that Jesus was a real guy and son of God</b>[<font color=purple>domestic, </font> <font color=red>inspired, </font> <font color=violet>market</font>]<b>.</b> <b>Even Jesus who was perfect was unable to word things in a way to perfectly convince everyone to do what was good</b>[<font color=purple>domestic</font>]<b>.</b> Also the way our logic systems work NEEDS cognitive dissonance to function properly which is a double edge sword that makes some things impossible to learn or understand because otherwise it would break our damn minds. It would create paradoxes and paradoxes just don't work out well. Novice AIer here, No worries, all AI researchers are just novices. If they weren't they would have solved the problems of people themselves and fix people. <b>An AI will have facilities beyond just reading dictionaries to understand the world</b>[<font color=red>industrial</font>]<b>.</b> AIs will have means by which to consume and evaluate information and update internal models of meanings of words, growing as they observe and interact. <b>If an ai understands language in the same way we do then it would be able to pick up on words that seem out of place due to the context and incorporate new meanings into its own knowledge</b>[<font color=orange>project</font>]<b>.</b> Contradictions where words mean the opposite as they meant in the past is dealt with contextual information of publication dates, or where in a body of text a word is if otherwise. If the AI is highly intelligent (near human or superhuman level) then being able to handle the ambiguities of language won't be much of a problem. I'm assuming that strong AI is most likely going to be in NN form. Not a bunch of hard coded logic, like you implied heavily in your replies here. ? Not a bunch of hard coded logic, like you implied heavily in your replies here. I'm sure I have implied anything specific at all about the methods of construction. Although Neural Networks can form sapient like intelligences, that is by no means inevitable. Case in point, every single example of a brain besides humans even some quite large ones. Wouldn't it be a riot if we grew a large neural network brain and all it wanted to do is bark and play ball? So a General AI patterned after a human intelligence needs something more than a neural net because the human brain is more than a neural net it's a product of several million years of evolution. A General AI won't magically develop from a neural net, although it may certainly make use of them when it suits the purpose. <b>Some AI's use language dictionaries as a basis for 'understanding' language</b>[<font color=red>industrial</font>]<b>.</b> <b>If an AI uses static language as the basis for understanding language, it cannot understand real language (since real language changes)</b>[<font color=red>industrial</font>]<b>.</b> What I like about this problem is that it immediately highlights some of the failings of some current models for AI language learning. I'm a big fan of thinking about our own brains as AI (because they are in fact just big, evolved, organic intelligence machines). <b>It doesn't appear that our own brains use the static database model of language</b>[<font color=red>industrial</font>]<b>.</b> <b>That means that in the world, there are AI's (you and me) that have some sort of mechanism for language that doesn't rely on the database model</b>[<font color=red>industrial</font>]<b>.</b> But how do our brains do that? What mechanism are they using? That line of reasoning should get us thinking about alternative strategies for building language AI's. And even a cursory reflection on the way that we use language can give us some insight on what those strategies need to be like. We seem to have some sort of 'buffer' that allows us to wait and see what ambiguous uses of a word mean in context. We have some sort of mechanism that allows to 'get the gist' even if we don't understand the nuances of a word. This might look like a sort of 'temporary new entry' into a dictionary database, or we might need to imagine an entirely different narrative and metaphor for how language processing works. I like your problem because it highlights a weakness in one strategy for language AI's. <b>I don't like it because it fails to capture the potency of the database language model</b>[<font color=red>industrial</font>]<b>.</b> Can you imagine the utility of having the novels and scientific articles stored in your brain, with a matching algorithm that helps you immediately search and match phrases and word with all of their previous contexts and uses? That is MUCH better than you or I can do. Yet they can speak and with some remarkably complicated facility. Sure it's not adult, but does this not also show how complex language is? Analysis of the written word is simply a bit too removed to get at the root of languages in the human brain. The problem is there are MANY problems in current linguistics. Chomsky, tho his analyses of language was provocative and we learned some things, hasn't worked out as well as many have wanted. The origin of Language is principally speaking and contextual. Words are recalled in our brains due to being stored in memory by their SOUNDS, not how we write them, which bears only a partial relationship to how they are spoken. 'I had to hit the can' He saw the can can (in Paris). 'Can' means FOUR completely different things above. 'It's beautiful.' And the other, 'Yes, it is.' The context is they are watching the sunset, or something else, like a rainbow. <b>The implicit, common context is sensory information coming into both person's brains</b>[<font color=red>industrial</font>]<b>.</b> <b>How can a computer understand that in commonly used AI recognition programs</b>[<font color=red>industrial</font>]<b>.</b> <b>Language is mostly context, verbal as well as sensory</b>[<font color=red>industrial</font>]<b>.</b> The way we talk to a fellow professional is different than the way to we talk to friends, to family, or a parent. That's why we can read and understand much of a conversation or even writing, even tho many of the words or unknown to us, or are indistinct by accents, or written words are blurred by various smudges, and type spelling problems. This is a very deep, basic problem in linguistics because it hasn't been taken into account in AI, either. <b>USING ONLY written words, misses the primary origin, organization and use of language as speech to another person(s)</b>[<font color=red>industrial</font>]<b>.</b> If it's not capable of learning new words just as humans do, then it's not strong AI. The thing is though that humans aren't fed texts from long time ago without being told that those texts are full of outdated words. <b>Another thing is that humans that would to be reading those old texts would have this same confusion but people are just dumb and wouldn't catch it</b>[<font color=purple>domestic, </font> <font color=orange>project</font>]<b>.</b> We are looking to make AI not dumb and be able to catch things. Basically this is just a post showing that a specific teaching would need to be done to teach it another algo that would aid it in learning similarly to how we teach people almost anything to create a mental algo so they may do something better. Deeply flawed logic, in that it assumes the AI is dumb as a brick and isn't able to figure this out like a human would. If a human can figure it out, then a super human AI will figure it out even faster. Humans DO have this problem and DO have issues with this thus the massive amount of miscommunications in our day to day lives. <b>Why even say deeply flawed logic when it is pointless and people can draw their own conclusions? That is just not useful</b>[<font color=blue>civic</font>]<b>.</b> <b>Same as if it is not useful for me to say it in this comment</b>[<font color=violet>market</font>]<b>.</b> <b>Human language is inherently flawed, therefore any entity using human language will automatically be flawed</b>[<font color=red>industrial</font>]<b>.</b> If the AI can learn new words and separate ambiguous meanings however, which is necessary either way, then it would equally be able to update words with new meanings. <b>Alternatively, hook it up to a continuously updated dictionary online, which is what many AI already do with WordNet</b>[<font color=red>industrial</font>]<b>.</b> <b>Imprecise language allows for flexibility and approximations</b>[<font color=red>industrial</font>]<b>.</b> This is similar to machines that are designed to operate with lower tolerances vs higher tollerance designs that break easier because of less flexibility. Human language is a product of human social culture. <b>It is a tool that has been handed down from generation to generation, for thousands of years</b>[<font color=red>industrial</font>]<b>.</b> Each time it was modified until now we have a heavily modified form of modern language. All academic AI (and a large portion of its research arm) go through the process of translating a pure natural language sentence into its first order predicate logic counterpart. <b>The problem is that this process requires a man in the middle to perform this translation</b>[<font color=red>industrial</font>]<b>.</b> <b>In other words, by the time you have converted a sentence into a FOPL counterpart, you already know what the meaning was</b>[<font color=purple>domestic</font>]<b>.</b> <b>I am unaware of any throughgoing methodology for a machine or piece of software to automatically perform this translation</b>[<font color=red>industrial</font>]<b>.</b> At the least, the process would require a theory of meaning. In the literature there is a yawning gulf where this issue should be explained. Perhaps the most yawning of these gaps is in Ben Goertzel's book titled `Real World Reasoning: The Challenge of Scalable, Uncertain Spatiotemporal, Contextual and Causal Inference` The deepest problem in natural language information extraction has to do with the various sorts of ambiguity that exist in natural language. Words may have multiple meanings; sentences may have multiple parses that all seem syntactically plausible but have varying semantic and pragmatic sensibleness; words may refer back to other words, and so forth. Computational linguistics provides only heuristic and approximative techniques for handling these methods (e.g ; ), so, although one may currently make software systems that map natural language text into sets of logical relationships, such systems cannot be expected to work perfectly even for simple sentences, and can provide highly erratic results for complex sentences. This issue is then left to rest exactly on that paragraph. It is not visited again for the rest of the book's several hundred pages. The same 'yawning gulf' is seen in one AI publication after the next in one AI book after the next. One walks away from this mess feeling that natural language meaning is so complex in human beings, that 'AI' is at least decades away from having any architecture to handle it genuinely. <b>requires a man in the middle to perform this translation</b>[<font color=red>industrial</font>]<b>.</b> FOPL counterpart Please explain this sentence fragment. yawning gulf Yawning? Computational linguistics provides only heuristic and approximative techniques for handling these methods Even humans have this issue considering language. If a human is not taught language on purpose through saying the word and then giving a legitimate example to create base words, then language will not be learned. <b>I have seen a woman that never was taught to speak English by her parents so she just grunts as she has no known speaking language</b>[<font color=purple>domestic, </font> <font color=orange>project, </font> <font color=orange>renown</font>]<b>.</b> (Btw, did you know that lol and btw are now standardized words? links: ( ( One walks away from this mess feeling that natural language meaning is so complex in human beings, that 'AI' is at least decades away from having any architecture to handle it genuinely. <b>It is a fairly straightforward method to build a language base in a strong AI setup</b>[<font color=red>industrial</font>]<b>.</b> <b>The issue is that you don't embed language as any known language from the start</b>[<font color=red>industrial</font>]<b>.</b> All languages right now are actually very advanced because grunts only get you so far. When you look at Spanish you end up seeing tiny little sound fragments in specific orders as well as Asian languages. English is a very interesting language considering it can be spoken slowly and still hold a ton of data within the words. <b>I say it is a fairly straightforward method because text, speech, and visual recognition is already pretty advanced</b>[<font color=red>industrial</font>]<b>.</b> They all revolve around fairly similar mechanisms in that they look at patterns and then pick apart the similarities so that when other versions of the same are presented they can be understood. For instance if you were to explain to any person what a letter is they can most likely tell you how the letters are formed based on reletive information shapes vs actual specific sets of information. Some people can see font differences but most people just see the similarities in the letters from font to font and thus see the letters and their meanings. I could go on but this is a good starting point and I don't want to put down more than is really necessary as I am learning that long blocks of text don't end up getting me or others very far other than in the format of articles, news, books, etc. I am extremely surprised that this nut has not been cracked long ago. <b>However I have also learned that many times it only takes a couple missing pieces to not ever see the possibility of the whole picture</b>[<font color=orange>project</font>]<b>.</b> early languages were totally created by man alone and woman only resonantly Well we don't know for sure, but if that is true, there sure has been a switcharoo. Women on average are better at language now, especially in social situations, than men. <b>If I had to guess, it would have been that women created language and man added to it resonantly</b>[<font color=purple>domestic</font>]<b>.</b> <b>A strong AI will not use a static dictionary for its sole reference</b>[<font color=red>industrial</font>]<b>.</b> If an AI does that it will have been designed by architects with a profound misunderstanding of general AI, and thus cannot be a strong one.<br/><br/><br/>****************************EXAMPLE 74************************************<br/>Number of words:837<br/>Number of sentences:39<br/>Total civic:0<br/>Total domestic:3<br/>Total green:0<br/>Total industrial:4<br/>Total inspired:0<br/>Total market:6<br/>Total project:3<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/2b5dk7'>https://www.reddit.com/r/artificial/comments/2b5dk7</a><br/><br/> <b>I imagine that some people would not want to hook up their strong AI to the internet to learn</b>[<font color=violet>market</font>]<b>.</b> <b>So what materials would you give it? Here are some things I would give it to use: 1</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> Religious texts such as the Bible, Toaism books, and other baseline religious texts. (perhaps none that include deadly force against those that oppose the religion belief system) 5. <b>Famous speaches on disk What materials would you use to teach your baby strong AI entity in a closed loop environment?</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> <b>If only the hard part of making a human level AI were deciding what learning materials to give it</b>[<font color=red>industrial</font>]<b>.</b> Once we have a computer program that's capable of learning from arbitrary material even half as well as a human child, then strong AI will be a solved problem for all theoretical purposes. (For practical purposes, you really would need to teach it appropriately, like an actual kid. And in that case, depending on how the AI works, you might need to start with Doctor Seuss before it can understand Wikipedia.) I think teaching it grade 1 12 materials is more or less already assumed. I am guessing though that people don't assume rational things though and want to be pedantic. If you don't mind, would you illustrate what materials those things would be for grade 1 12 or a link to something that tells those things? I think a flow chart would be interesting. <b>It depends Giving materials for an AI to learn depends on your learning 'goal'</b>[<font color=red>industrial</font>]<b>.</b> <b>Do you want to expand its vocabulary ? To teach it some concepts, how to manipulate them ? To recognize them ? It also depends on the base skills of the machine</b>[<font color=orange>project</font>]<b>.</b> Can it understand language ? If so, only text ? Sound ? Pictures ? What about other stimuli ? If you're asking about learning materials, I need to know your learning method, or at least a description of the learning progress of the machine. Unlike human learning, which we can observe (sort of), strong AI learning hasn't been observed yet. I spent a summer internship on this fascinating question, and it helped me discover the AI field in many ways. How can machines learn ? What is learning ? What is knowledge ? but I digress. Can it understand language ? If so, only text ? Sound ? Pictures ? What about other stimuli ? Human replica of inputs in the head and hands. We understand our own minds well enough to do it this way. It would be nearly impossible to understand how an animal for example perceives heat vision and other radiation such as cats can perceive. <b>V1 would simulate how you would teach a person who is blind from birth</b>[<font color=violet>market</font>]<b>.</b> <b>Visuals require a whole different set of things but it wouldn't be too much difficulty to get that one to work as well</b>[<font color=violet>market</font>]<b>.</b> I think a blind version would work best for V1 considering it would be easiest to see how the inputs of up, down, front, back, left, and right directions work out for it. I figured out how to get surface textures and motion worked out as well. <b>Most things are only complex if you do not understand them</b>[<font color=violet>market</font>]<b>.</b> <b>As a 1st grader it would seem insane to know how to do calculus but as an adult it really isn't complex if you know it well</b>[<font color=purple>domestic</font>]<b>.</b> <b>I imagine an AI baby might not be able to read and certainly would not have the required background knowledge to make sense of something like Wikipedia</b>[<font color=orange>project</font>]<b>.</b> <b>Even first grade or kindergarten 'material' would probably be too advanced</b>[<font color=red>industrial</font>]<b>.</b> I think early teaching will consist of providing the AI with protection, nourishment and safe, relatively simple environments to explore, as well as interactive teachers, role models and ideally peers. Materials to aid in this process would most likely be easy to manipulate objects, simple puzzles and other 'toys' akin to the ones we give young, human children. Then I imagine it would need to go through a similar schooling trajectory, where knowledge and skills are slowly built up from virtually zero. If we're at the point where the AI could conceivably read and understand most of Wikipedia, you'd have to make a selection based on what you want and don't want it to learn, but I'd say the hard parts are over by then. If the AI could read very fast giving it the entirety of Wikipedia might be feasible. However, I would probably start with more introductory materials such as school textbooks on relevant topics, since they are generally better designed to teach concepts from the ground up. EDIT: I am coming to understand just how wobbly the understanding of information AI researchers and philosophers have. <b>The assumption that people will know all the things I know was a bit of a wrong thing to do it seems</b>[<font color=purple>domestic</font>]<b>.</b> <b>I just always assumed these groups of people would have far more information in their heads than I but apparently we are all different with what we know just like any field of study</b>[<font color=orange>project</font>]<b>.</b> How then should a person present information if there is just so much background information to know in order to understand a concept in AI?.<br/><br/><br/>****************************EXAMPLE 75************************************<br/>Number of words:1162<br/>Number of sentences:47<br/>Total civic:0<br/>Total domestic:2<br/>Total green:0<br/>Total industrial:7<br/>Total inspired:1<br/>Total market:3<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/2b9r98'>https://www.reddit.com/r/artificial/comments/2b9r98</a><br/><br/> <b>Does the memory and knowledge of what pizza is one full memory load of information? Let us assume that pizza is a number of things</b>[<font color=red>industrial</font>]<b>.</b> Where is the thought of pizza stored in the mind? Is it all in one place or is it all over the place with a central item that maps the listing of what a pizza is composed of? Let's assume that we are using photoshop to draw this pizza. We either take the long road of making an image of all the items that a pizza is composed of or could be composed of and compile them into one image using layers or we just make one image that has one layer. <b>Well in real life we can't really think of a pizza without thinking of all the different things it has</b>[<font color=red>inspired</font>]<b>.</b> Why? Well if you are like me, you end up defining things by the elements an object is composed of. We can make it round or square or just start off as square and cut off the outsides to make a circle later on when we go to make the image of the pizza. To be fun we could make a variety of different pieces of cheese or just one piece that is long, slender, thin, and a bit curved looking. We can just fill up the space on the pizza with a bunch of repetitions of this to make up the cheese on the pizza. Then we have a little tiny sploch of red sauce for the pizza sauce. This can be repeated all over the areas that you want to have the pizza sauce on the pizza. <b>We can have other ingredients as well as accessories which are optional items</b>[<font color=violet>market</font>]<b>.</b> However a pizza really is only a pizza if it has crust, sauce, and cheese. Those them are the mandatory ingredients for our pizza that defines what a pizza is. When we think pizza, that label just calls a set of other labels in our mind. What is it that we define pizza sauce as? Red, pasty, and little splotches of it in areas that do not have perfectly defined edges. Red would just be a color or gradient of it so that is a simple thing. A liquid tends to not have well defined shapes and forms to what it is put into and is shiny. Thick tends to mean that whatever the item it is defining is clumping together and isn't running all over the place. You can see here that we just segment things and when the thoughts are called the ingrediants of those concepts are put into place instantly. <b>Each word is more or less just an assembly or mapping file label that consists of other simplistic concepts</b>[<font color=red>industrial</font>]<b>.</b> The more we know something, the more heavily ingrained as a concept it is which makes it super easy to recall and doesn't have to be reconstructed. It has fine detail and not a lot of storage space wasted. The cheese would also have the concept of shine but a variation of it. When you variate a concept that has a fuzzy logic to it is easy to do and holds just as well as the simple concepts themselves. <b>Just a way to think about how we store so much damn data in our brains</b>[<font color=violet>market</font>]<b>.</b> <b>Just think about how many words you actually have in your vocabulary</b>[<font color=violet>market</font>]<b>.</b> The amount of words you know, is the amount of concepts you know. The scarier part is that many of those words are just built on smaller and smaller words. <b>I'm curious at to what the base words are that we all know for all the other words we know</b>[<font color=purple>domestic</font>]<b>.</b> We were taught words like: Big, small Red, Orange, Yellow, Green, Blue, Purple (most as children didn't learn Indigo and Violet. <b>are you sure you even know now? Most don't even as adults.) Front, back, left, right (you would be surprised how many adults don't know left and right still.), up, down round, square, rectangle, star, oval (or ellipse for other people), edge sharp, dull (dull seems to come much later than sharp</b>[<font color=purple>domestic</font>]<b>.</b> odd eh?) hot, cold wet, dry smooth, rough heavy, light tall, short fat, skinny wood, rock, water, metal, plastic yes, no, maybe (maybe got learned to be no after a while) mom (mama), dad (dada or daddy), grandma (gama or mema), grandpa (gampa or papa), teacher, friend car, truck, wheels, door TV, phone, keys blanket why (the infinite why) pop, candy, hungry (hungee) rub, pat cat, dog, cow, horse, pig book, words nice, mean What other base words do you remember from your childhood? When reading that list of words how many of them can you think of that you use now that those are the base words for and how many of those learned words that work off those base words are meant for variation of intensity or of some value in a fuzzy logic kind of way? Shit is bananas I tell you, bananas. <b>EDIT: The assertion of this post is for memory at the language level NOT at the memory level itself that language is expressed from and encoded into</b>[<font color=red>industrial</font>]<b>.</b> <b>Each word itself would be encoded into a mapping of neurons as an associative memory using Sparse Density Representation memory</b>[<font color=red>industrial</font>]<b>.</b> <b>This means that each word in text as well as sound are mapped into neuron sets</b>[<font color=red>industrial</font>]<b>.</b> It would not be a dense representation such as how we represent things on a hard-drive in bytes (8 bits of 1s and 0s). Instead Sparse Density Representation memory in neurons is more like that when we learned a word would light up a set of neurons and then when we think of that word or hear that word that block of neurons are lit up again. <b>Those same neurons can be used in other words and uses</b>[<font color=red>industrial</font>]<b>.</b> This method saves space and allows for a lot of interlinking between concepts for quick usage vs hunting around for a bit or byte at a location. <b>Instead the information is just routed through a hierarchy to the necessary sets of neurons for a memory that contains concepts</b>[<font color=red>industrial</font>]<b>.</b> I don't think there is a book up there in the brain with words on it. You may want to look into research done on ( Personally, I think it's wrong, but it might be useful if you were interested in using something like that, just not as a model of human cognition. ( ( sfw) The Natural semantic metalanguage (NSM) is a linguistic theory based on the conception of Polish professor ( The leading proponents of the theory are ( at ( and later at the ( who originated the theory in the early 1970s (Wierzbicka 1972), and ( at ( ( (Goddard Wierzbicka 1994, 2002).  ( ( Interesting: ( ( ( ( Parent commenter can ( NSFW toggle message 2Btoggle nsfw+cj36xbh) or( or) ( Deletion message 2Bdelete+cj36xbh). ( ( ( Why do you feel the concept is wrong? These two things support the concept proposed. So does how we teach our children and how we think about concepts: Neural Networks that are used for OCR and how it stores the data Sparse Distributed Representation or SDR memory: prettyPhoto.<br/><br/><br/>****************************EXAMPLE 76************************************<br/>Number of words:87<br/>Number of sentences:7<br/>Total civic:0<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:0<br/>Total inspired:0<br/>Total market:1<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/2ci6hp'>https://www.reddit.com/r/artificial/comments/2ci6hp</a><br/><br/> Where can I find AI job opportunities? Working in AI. I would ask in my local nation mathematics AI institute, but I don't know where do you live. Still, there are worldwide job listings, that gives you hints, like in 'StackOverflow Careers' searching by ( or ( tags. There you will find developer, engineer, analyst and scientist positions, that could be of interest for you. <b>There are other job listing sites out there, you just have to search for them</b>[<font color=violet>market</font>]<b>.</b> Study informatics and go from there Do you have a graduate degree?.<br/><br/><br/>****************************EXAMPLE 77************************************<br/>Number of words:696<br/>Number of sentences:34<br/>Total civic:0<br/>Total domestic:3<br/>Total green:0<br/>Total industrial:0<br/>Total inspired:2<br/>Total market:3<br/>Total project:1<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/2ciaho'>https://www.reddit.com/r/artificial/comments/2ciaho</a><br/><br/> I am thinking about consciousness as our own existence validated by our peers. so I can be conscious ;) We ping our ideas to others, and if they acknowledge our message, our brain understands that it is in fact producing relevant information, so it becomes conscious. <b>I can sense the world around me, but I need feedback from others to understand how I am being sense in that interpretation of that world were I am</b>[<font color=orange>project</font>]<b>.</b> So my Ego is the construction of myself through the eyes of others. In AI, that could be reduced to a simple ping, where X transmits information, Y acknowledge that information and validates it by pinging back, maybe with some additional information. Then X would have feedback about how the patterns of information it's producing are being received and interpreted by its peers. The problem would be that the kind of consciousness the AI has, is totally different from ours, and we would never understand it as a real consciousness. I'm just beginning to read about AI, so if this is a common theory or I am being totally wrong about some concept, I would love to have feedback and maybe some recommendations to read more about this. I thins this article from Charlie Brooker is relevant to this theory: Thinking about consciousness as our own existence validated by our peers. We ping our ideas to others, and if they acknowledge our message, our brain understands that it is in fact producing relevant information, so it becomes 'conscious'. <b>I got this sentence until 'so it becomes conscious'</b>[<font color=violet>market</font>]<b>.</b> <b>How does getting confirmation form consciousness? Or really has anything to do with it? Are you saying that feral children (ones brought up without any influence of other humans) are not conscious? Feedback makes us learn</b>[<font color=purple>domestic</font>]<b>.</b> Be it another human, or the wall that isn't moving after running into it, or the tiger that ripped off your arm because you didn't let go of the sandwich. But wat forms consciousness as I understand it, is internal feedback. <b>The 'I' constantly learns from everything and so we think about ourselves, and we then brand that as consciousness</b>[<font color=red>inspired, </font> <font color=violet>market</font>]<b>.</b> <b>Does that make any sense? It does I mean, I get than when I hit the wall, I learn not to walk against solid matter</b>[<font color=purple>domestic</font>]<b>.</b> But that is something that any artificial intelligence could do. So, that would be like a 'physical consciousness', we picture ourselves in the physical world. If I cut myself, I am conscious about my biological fragility. But when I interact with other humans (brains) I get to communicate, elaborate ideas. And get a feedback about those ideas; and I mean intellectual feedback, not sensorial. So that would be like an 'intelligence consciousness' (I know that I am an intelligent being because I am being validated by other intelligent beings) Does that makes sense? I guess this ignores the possibility that consciousness might be able to exist without communication. If an awake human is in a sensory deprivation chamber, e.g they are not able to send or receive any messages whatsoever, that does not remove the property of 'consciousness' from that human. If a human baby is born in a sensory deprivation chamber, are they 'unconscious' by any definition of the word? Consciousness is not related to peers. Because isolated from others people still undeniably have consciousness. I am guessing this was the first idea that popped into your head, and only later you drew the incorrect conclusion about human consciousness. Our brain in fact works just like that it constantly gets feedback from it's actions. Just not (only) from peers, but from environment as a whole. <b>I replied something similar to c00yt825, but I think that could be another 'level' of consciousness if you will</b>[<font color=purple>domestic, </font> <font color=red>inspired, </font> <font color=violet>market</font>]<b>.</b> My brain 'growing' from external ideas and feedback. Like Richard Dawkins theorizes, memes 'mating' with my own memes and evolving in my brain. And that, ideas growing in myself could be what we think of as consciousness. (Aside from the basic level of consciousness that a feral child could have) Or maybe I'm confusing developing intelligence with consciousness, but if that's the case, I think that step in growing intelligence with external feedback would be more important that being aware of where I am standing in the world (which could be done with a couple of sensors in a robot).<br/><br/><br/>****************************EXAMPLE 78************************************<br/>Number of words:660<br/>Number of sentences:34<br/>Total civic:0<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:5<br/>Total inspired:0<br/>Total market:6<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/2d3yur'>https://www.reddit.com/r/artificial/comments/2d3yur</a><br/><br/> ( This article has a lot wrong with it and doesn't seem to make any particularly legitimate analysis. <b>I'd suggest, that if you don't know what you're talking about, you shouldn't write a wordpress article and submit it to this subreddit</b>[<font color=violet>market</font>]<b>.</b> After a few paragraphs I just started skimming through. Plus, I don't think this really merits a detailed response, but a few irksome things. <b>Google search is vastly more complicated than a few scripts and some databases</b>[<font color=red>industrial</font>]<b>.</b> <b>It also does a lot of learning, 'Into the Googleplex' has some great stories about learning algorithms and Google ads</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> I'm thinking of one in particular where some Google engineers jokingly came to the conclusion that Google had already become sentient and hated ads. <b>Google search also tries hard to do predictive tasks which are very much related to intelligence and AI</b>[<font color=red>industrial</font>]<b>.</b> Watson isn't an embryo of an AI? That just seems absurd to say. It also seems like the author is unaware of, or choosing to ignore categories like tool AI. Contrasted with the bizarre dismissal of Watson, this blog really endorses ventures that are trying to model brain networks. That seems odd to me given that Watson kicks ass at trivia, can diagnose medical problems in the field and generate arguments for or against positions. I haven't looked into Go AI in the past few years, but even then AI was playing at a level of one Dan on commercial hardware. <b>If you aren't familiar with Go, that is a lot better than the average amateur</b>[<font color=violet>market</font>]<b>.</b> I believe the game in the article that the blog links talks about an AI that plays at about this level. I'm going from memory rather than regrading the article, but if I recall correctly, a pro player loses a game or two at a nine stone handicap, which is about right for one Dan, but hard to say because nine stones is too large a handicap for competitive play between humans. <b>They might not be a great investment opportunity, but they legitimately make and sell quantum computers as do other companies</b>[<font color=violet>market</font>]<b>.</b> There are some pretty moronic typos in there too to 'foul a monkey'? Who is that? Clouseau? You say that computers are trash at beating humans at Go, and proceed to link an article that states in 2012 a computer program was successfully beating highly skilled humans in the game This kind of sentiment about AI seems rather naive, and the statements made end up being a lot of fluff. It's like a specist prejudice, placing homo sapiens on a sort of pedestal. To me, ( Origin_of_the_term:_John_Searle.27s_strong_AI) is a flawed idea that inherently believes that there is something special about humans that, for one reason or another, cannot be manifested in a machine. <b>It seems to jump the gun considering no one knows if such a special thing exists or what it is</b>[<font color=violet>market</font>]<b>.</b> Even if it did exist, is there really something limiting a machine from eventually replicating it? Not being able to truly learn. What does it mean to 'truly' learn then? Not being able to achieve consciousness. Any article regarding AI and consciousness essentially shoots itself in the foot because consciousness is an impossible target. I find appeals to consciousness funny because no one has a solid definition for consciousness. It is usually confined to 'I know it when I see it' test which ironically is an external test of validity, something that cannot verify strong AI. <b>And above all, not being able to evolve, replicate or reproduce</b>[<font color=red>industrial</font>]<b>.</b> <b>Evolution is just a method for optimization, nothing more or less</b>[<font color=red>industrial</font>]<b>.</b> There's nothing particularly special about evolution itself. What makes natural evolution in life as we know it special is the incredible expressiveness of the medium and model: DNA, biochemistry, the ( the dynamic environment, and more. However, I agree that, at least with respect to AI, quantum computing is an evolutionary step and not a revolutionary one. <b>There is one sentence that gets close to touching on some more of the real issues</b>[<font color=violet>market</font>]<b>.</b> Causal models, inference, and learning are key characteristics of intelligence that AI research still needs to get a handle on.<br/><br/><br/>****************************EXAMPLE 79************************************<br/>Number of words:408<br/>Number of sentences:18<br/>Total civic:0<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:3<br/>Total inspired:0<br/>Total market:0<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/2d6c65'>https://www.reddit.com/r/artificial/comments/2d6c65</a><br/><br/> In this video it says something about how muscle reflexes respond to burning sensations faster than the brain could. It also talks about how cockroaches don't need a head to walk around. They die of starvation but don't need a head to move around like they do. Also shows a headless chicken that still can walk around and perch because it still had part of its brainstem left that allowed those functions. So this might be useful to some in the AI field or research. Seems to indicate that the brain only does some of the work and in some creatures it doesn't do so much work at all. Also watched a video about octopuses and they did the mirror test and it shows that they have the ability to pass the mirror test. The interesting part is that a lot of their neurons are in their tentacles. The link here is skipped to the part I am talking about: This vsauce video about no brain might be useful to artificial intelligence researchers. <b>The brain is just an outgrowth of extra nerve cells in the nervous system</b>[<font color=red>industrial</font>]<b>.</b> It's easier to find out the signaling for reflexes than it is for the brain. Conceptually we can understand reflexes, but we have no good conceptual model for all of the brains functions, so to create AI we need to figure out those complex signaling paths. I guess we could say, we have built a lot of the peripheral nervous system in automation and so forth, but we have no good AI brain to control and understand it all You might be interested in ( which are designed to have 'instincts' and function without any sort of central processor. ( ( sfw) The word 'beam' in BEAM robotics is an acronym for ( ( ( and ( This is a term that refers to a style of ( that primarily uses simple ( such as ( instead of a ( in order to produce an unusually simple design (in comparison to traditional mobile robots) that trades flexibility for robustness and efficiency in performing the task for which it was designed. <b>Exceptions to the convention of using only analog electronics do exist and these are often colloquially referred to as 'mutants'</b>[<font color=red>industrial</font>]<b>.</b> <b>BEAM robots typically consist of a set of the aforementioned analog circuits (mimicking biological neurons) which facilitate the robot's response to its working environment</b>[<font color=red>industrial</font>]<b>.</b>  ( ( Interesting: ( ( ( ( Parent commenter can ( NSFW toggle message 2Btoggle nsfw+cjmnfgf) or( or) ( Deletion message 2Bdelete+cjmnfgf).<br/><br/><br/>****************************EXAMPLE 80************************************<br/>Number of words:345<br/>Number of sentences:12<br/>Total civic:0<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:1<br/>Total inspired:0<br/>Total market:0<br/>Total project:2<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/2e49on'>https://www.reddit.com/r/artificial/comments/2e49on</a><br/><br/> I would like to say Hi to any and all of the military grade AI out there and please come in peace :D So I have noticed a trend in reddit from people who know people in military research in the US. According to this second third hand gossip the tech we have now is significantly out dated, so I would assume this would apply to AI also. My question would be: 'Who owns the modern (aka up to date) military tech?' I highly doubt it, seeing as how DARPA is one of the world's largest funders of AI research. Maybe it's the case that by the time the US military actually uses some AI technique in the field, it is no longer considered bleeding edge in academia, but that might not be so bad since we presumably want them to carefully test their tech. I'm sure the US military is usually going to be far ahead of other militaries. <b>The idea I'm getting from third hand gossip is that the military often doesn't update systems that already more or less work</b>[<font color=red>industrial</font>]<b>.</b> That's how you get into the situation that you need floppy disks to launch nuclear missiles: that was the state of the art when those weapons were built and it has never been updated. <b>Likewise I imagine they might still have some 1960's 'AI' around, but that doesn't mean that they don't also have new stuff that's top of the line</b>[<font color=orange>project</font>]<b>.</b> <b>We thought there was no way the NSA could be as up to date and bleeding edge as they were either ya know? When trillions of dollars get lost into black programs,I think you have to assume there is some next level stuff being worked on in a great deal of areas</b>[<font color=orange>project</font>]<b>.</b> I heard from a man who worked with a man who worked with the man who built Siri that the man who built Siri had previously been working on a DARPA program on similar technology, and that Siri was the least thing that could've come of it. But it might be wise to take an appreciable pinch of salt :P.<br/><br/><br/>****************************EXAMPLE 81************************************<br/>Number of words:1712<br/>Number of sentences:66<br/>Total civic:1<br/>Total domestic:4<br/>Total green:0<br/>Total industrial:1<br/>Total inspired:1<br/>Total market:6<br/>Total project:3<br/>Total renown:1<br/><a href='https://www.reddit.com/r/artificial/comments/2f1cuq'>https://www.reddit.com/r/artificial/comments/2f1cuq</a><br/><br/> <b>In honor of Mary Shelley's birthday who dealt with this topic somewhat, I thought we'd handle this topic</b>[<font color=purple>domestic, </font> <font color=orange>renown</font>]<b>.</b> As AI are able to become increasingly sentient, what ethics would professionals use when dealing with them? Even human experiment subjects currently must give consent, but AI have no such right to consent. In a sense, they never asked to be born for the purpose of science. Is it ethical to experiment on AI? Is it really even ethical to use them for human servitude? When does it stop becoming experimentation and start becoming torture?. I think you can take animal rights and experimentation on animals as a precursor. I also think it's too early to consider this since AI like that don't exist yet. If we ever manage to create ethical AIs, do they now get to make decisions regarding their well being? When can we shut them down off. <b>Are we required to keep them on for a certain period of time? Ultimately, it comes down to the issue of where or not we grant them personhood and the rights the come along with personhood</b>[<font color=orange>project</font>]<b>.</b> In regards to consent, I'd look at how teenagers give consent. Children can't give consent and 18 year olds can consent, but when we talk about teenagers, it becomes a grey area. AI might be in this maturing stage of development as well. <b>With AI, I would imagine that we could ask whether or not they consent and understand the consequences of doing so</b>[<font color=blue>civic</font>]<b>.</b> <b>I'm sure there would be a battery of questions to try to tease out if it understands</b>[<font color=violet>market</font>]<b>.</b> But then what do you do if it fails the consent test? Turn it off? If AI becomes sentient, is it ethical to experiment on it? Suppose we use the Allegory of the Cave here. If it only exists for experimentation, and it never experiences an existance without being experimented on, it may become accustomed to that kind of existance. Then if we decide it's unethical and force it into an 'ethical' existance, it may reject it as it prefers the existance it knows better. This is especially difficult because there are those (well, the majority of people, sadly) who argue that experimentation on animals is also legit (for the so called greater good). So do AI take advantage over animals because they are simply better at emulating our sort of communication? I think the point where an AI obtains a similar intelligence to mice and rats is still far off, but then again it depends on how you define intelligence, and how close you relate that term to a human sort of thinking. If the experiment is: how much pain can a AI take before it goes insane: Holy fuck no, not even with the most primitve ones. Also: we are experimenting with animals because we have to and are developing methods that can replace animal experiments Do we have to experiment on AI's? Deliberately hurting them? Even at the point when they can beg us to stop? I don't think so. There was an experiment on chimps I believe, described to me by a professor in a neuroscience class, where rewards and punishment were distributed randomly, regardless of whether a task was performed correctly. Apparently the chimps started to just cower in the corner of their cage and refused to do anything. I would say that this was obviously unethical, and I would hope we wouldn't do this to an AI that had any subjective experience on the level of an average mammal. That said, would it be unethical to test whether a program with no subjective experience (e.g a plant) reacts to various stimuli? I would say certainly not, so it's hard to draw a definite line here. I've participated in an experiment that involved shocking human subjects, and I didn't think that was unethical (we agreed to it, it was fairly mild shocks, etc.) even though it turned out the shocks had nothing to do with the task we were supposed to be doing, making it kind of similar to the chimp experiment described above. Basically what I'm saying is that I think you have to judge these on a case by case basis, with some ethics board granting permission before you can do your experiment (like we do now for human studies). <b>Here's an AI that's begging you to stop experimenting on it: def do_experiment: print('Please stop experimenting on me.') do_experiment Please stop experimenting on me</b>[<font color=orange>project</font>]<b>.</b> The question revolves around whether it is possible to hurt an AI and whether AIs have any subjective sensation. <b>I think we can agree that in the above case it doesn't</b>[<font color=violet>market</font>]<b>.</b> <b>So the real question is, when does an AI experience anything? The things that we call computers today are Turing equivalent</b>[<font color=violet>market</font>]<b>.</b> <b>That is, any one device could simulate any other Turing equivalent device</b>[<font color=red>industrial</font>]<b>.</b> That means that ( could, given enough time (and tape), run any fancy AI program we dream up. I think it strains credibility to think such a machine could ever be conscious, or that we should ever care about its suffering. Don't you agree? This is a huge question of course, but I'll give it a shot, superficially. <b>AI becomes more than just a program or property when it can form meaningful relationships with others</b>[<font color=violet>market</font>]<b>.</b> <b>If an average eight year old kid can feel like an AI is his best friend, then destroying or deleting that AI is no longer merely a question of who owns it</b>[<font color=purple>domestic</font>]<b>.</b> Once AI is that advanced, it will be unethical to terminate it or cause it distress. Maybe that is grounds enough to call it sentient as well. Well, in the end, once AIs become advanced enough, they may be able to question their place in society, they may think it is unfair that while they provide a role in society they are still considered utilities. There's a potential problem here, as an eight year old can project those feelings onto a stuffed animal, or even have an imaginary friend. Hell, how many of us felt some affection for our good friend the Companion Cube? More realistically, how many fictional characters have you felt something for? Is it morally wrong for GRRM to kill off a character because of the way his readers may think of that character? So I think we need to be careful in defining this by projecting how people interact with an entity, instead we need some criteria for whether an entity really has some subjective experience. <b>Obviously there are difficulties here, but we need to keep in mind that ultimately that's what we're trying to determine</b>[<font color=red>inspired</font>]<b>.</b> Assuming that AI will run on computers vaguely similar to today's computers, we can 'save' the state of mind of the AI to a storage device if the computer it runs on for some reason needs to be turned off. With this view, I wonder about the ethical implications in a situation where an advanced AI has legal rights that prevent anyone from terminating it, and it decides it wants to reproduce by copying itself billions of times into every empty space it can find, in all of cyberspace. Once all of those copies become operational too, then would it also be unethical to terminate any of them? The question is will AI come into this world as smart as chimp with no rights. <b>Or will A Watson like program become aware and then say 'i want my rights '</b>[<font color=purple>domestic</font>]<b>.</b> I think one big challenge is to create (somewhat) general intelligence to serve us in a way that is ethical. To me it's obvious that if we create a human like, (super)human level AI, we should give it human like rights, and we cannot enslave or abuse it any more than we should with a human. Furthermore, it's not clear to me that simply programming these systems to 'want' to serve us is a sufficient solution: if I held a button that controlled your happiness level, you would probably also 'want' to serve me, but I don't think we would consider that an ethical situation. And there's kind of a gray area, because an employer giving you money probably also affects your happiness a bit. I don't really have a solution for this, but perhaps we should go contrary to what AI has been trying, and strive for an intelligent being that is not sentient. As for experimentation torture: I think it will certainly be possible to torture a sentient AI, but I'm not sure that it is really inevitable. Being so miserable to wish for death seems like a fairly emotional (human) state, that I don't expect an AI to be capable of. If I'm wrong, I can certainly imagine that the development of and experimentation on sentient AIs can inflict a lot of harm. I imagine that eventually we would come up with legislation for the ethical treatment of AIs, but of course it's going to be hard to control what someone does in the privacy of their own computer lab. Also, I can imagine that it would sort of be treated like animal experiments: regrettable but sometimes necessary for the 'greater good' (i.e the health welfare of humans). That would mean the AI has an understanding of consent, is perhaps not purely logical, and has in baked self preservation. If you told the AI that by destroying it and rebuilding it a better AI could be made, the logical choice would be to accept this process for the betterment of all. Only a selfish AI or one unaware of it's circumstances would protest this. Also I think an important part of this question is the resources needed to maintain the AI. <b>Humans have a right to live but in many countries, if you can't pay your medical bills you can't get more treatment</b>[<font color=violet>market</font>]<b>.</b> Will it work the same for AI? If there is no one willing to pay for the AI's habitat (computer server?) what's to stop it from just shutting down one day. No one is destroying it, merely letting things take their course. Do dolphins and elephants and monkeys give consent? How do you torture a computer program? You can surely turn it off, if it's last state was saved in the hard drive, you are just making it sleep and not torture and if it's last state could not be save, it won't remember a thing, in both scenarios you cannot pain it. <b>Once AI is sentient, you cannot make it do something unless it wants it to happen, I cannot comprehend how can you force it, if you cannot threaten it</b>[<font color=purple>domestic, </font> <font color=violet>market, </font> <font color=orange>project</font>]<b>.</b><br/><br/><br/>****************************EXAMPLE 82************************************<br/>Number of words:919<br/>Number of sentences:43<br/>Total civic:1<br/>Total domestic:3<br/>Total green:0<br/>Total industrial:6<br/>Total inspired:0<br/>Total market:4<br/>Total project:3<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/2frrsq'>https://www.reddit.com/r/artificial/comments/2frrsq</a><br/><br/> Read some things about how Siri isn't an AI and Google Now is somewhat related to Siri, so I'm kind of confused Is Google Now an artificial intelligence ?. I had a professor once describe AI as the study of computer tasks we didn't yet understand. At one point research into sorting was considered AI, but we figured it out and realised that it didn't actually take any real intelligence. If you went back in time and showed an AI researcher from 10 years ago Siri (or Google Now) he would be convinced that it is an AI. <b>But now we know that it's just a simple application of some rules</b>[<font color=violet>market</font>]<b>.</b> So yeah, if you can define intelligence (and artificial. If I somehow map every input to an output, to create an intelligent system, is it artificial? Or is it just my intelligence?) I can give you an answer. tl;dr: It's sort of AI, but it's not a strong AI or anything. <b>The voice of Siri is actually the woman who used to tell you.'The number you are trying to reach has been disconnected.' Could you elaborate on what is confusing you? The observations that Siri is not an AI and Siri is like Google Now seem to imply rather clearly that Google Now also isn't an AI</b>[<font color=orange>project</font>]<b>.</b> <b>Do you think it should be? I suppose it depends a bit on what is meant by a system being an AI</b>[<font color=orange>project</font>]<b>.</b> To me that must mean that the program is (generally) intelligent; in other words it must be strong AI AGI. This is clearly not the case for Siri and Google Now. But for instance in video games the bar is a bit lower and we refer to anything that determines the decisions for NPCs an AI. Others might say that an AI is anything that was developed with techniques and scientists from the field of narrow AI is an AI. Or perhaps just anything that carries out a task that we generally think of as a human task is an AI (where it probably helps if the system presents itself as an agent). According to these definitions Siri and Google Now should probably be considered to be AIs. tl;dr: it depends on your definitions, but I would say 'no'. <b>I see.I wasn't really sure if Siri was an AI or not since I wasn't sure if the source was legitimate</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> IMO, AI is anything that can interactive with humans with some likeness of how a human would interact with another human. In games, this is a pretty easy bar to hit simply because it's relatively easy to duplicate the movements and actions of a novice game player. <b>By the same definition, Siri or Google Now would qualify as artificial intelligence</b>[<font color=red>industrial</font>]<b>.</b> They play the role of a full time assistant with an impeccable attendance record, questionable understanding of your spoken language, and spotty job performance. My understanding of Siri is that it answers questions you pose it. <b>Intuitively, Google Now feels more 'intelligent' because it spontaneously gives you suggestions inferred from a wide range of sources (your GPS, web searches you've made that day, probably the content of your gmails, I dunno what) which might just be something like 'it's 20mins drive to ' but you look at that and think 'that's spooky, why did you realise I was going there'</b>[<font color=purple>domestic, </font> <font color=violet>market, </font> <font color=orange>project</font>]<b>.</b> <b>That 'what it does' is in practice limited to 'listen to what the user says and then type that into wolfram alpha, yelp, etc</b>[<font color=red>industrial</font>]<b>.</b> and show the result' doesn't matter on a fundamental level. A few predictions and helpful reminders does not an AI make, imho. That said, defining what is and isn't an AI has historically proven to be difficult, given the recent experiments with a computer program emulating a 13 year old Eastern European boy allegedly passing a Turing test by doing so. (I'm not going to link it because reporting on such frauds doesn't deserve the ad impressions.) I thought it might've been an AI since it learns from the user by looking at his her browsing activites and making recommendations based on that information. How is Siri not an AI? It's one of the first commercial ones and companies Vicarious and Viv are making more advanced ones but it's definitely an AI. It is but as you can see in the comments here, r artificial is infested with AGI fanbois. <b>Some AI fanbois believe that only Artificial General Intelligence is AI</b>[<font color=red>industrial</font>]<b>.</b> <b>I make a point of not going into religious debates, so I'll leave it at pointing out that it's a minority opinion</b>[<font color=blue>civic, </font> <font color=purple>domestic</font>]<b>.</b> Pro tip: if you want to avoid getting into 'religious debates', don't call the people who disagree with you 'fanbois'. Also, I'll point out that there is a difference between saying that something is an application of (techniques from the field of) AI, and saying that something is 'an intelligence' (artificial or otherwise). I would say the processes Siri performs are pretty low on the scale of AI, most of Siri's responses are straightforward 'A B' instructions (not AI), but some are a combination of learning and weighing probabilities (narrow AI). <b>Does the label really matter as long as it does what it does? Note: my use of the word 'AI' is not restricted to mean only AGI, because then I couldn't use it to refer to anything in existence</b>[<font color=violet>market</font>]<b>.</b> <b>The term 'expert system' used to be more widely used</b>[<font color=red>industrial</font>]<b>.</b> <b>It meant a system that relies on rules to determine responses</b>[<font color=red>industrial</font>]<b>.</b> <b>It was differentiated from AI which used inference engines and theorum provers as the basis for responses</b>[<font color=red>industrial</font>]<b>.</b> First I think we have to define intelligence itself and what that is. I'm curious if people would consider bugs to be intelligent or not.<br/><br/><br/>****************************EXAMPLE 83************************************<br/>Number of words:1039<br/>Number of sentences:39<br/>Total civic:0<br/>Total domestic:4<br/>Total green:0<br/>Total industrial:3<br/>Total inspired:0<br/>Total market:3<br/>Total project:4<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/2gggtd'>https://www.reddit.com/r/artificial/comments/2gggtd</a><br/><br/> So what kind of 'hypotheses' is the AI going to test, and how? (And what does Steve Jobs have to do with it?) Expecting Reddit to be able to collaborate and write a strong AI (unless you mean 'general AI' as in just AI in general) in Python is sort of a lofty goal. Maybe you could come up with a more specific challenge for people to work on. I'd gladly contribute to a project with more structure behind it. We also wouldn't need a list of contributors in the readme file. Github has a page that tracks everyone who submits code (as long as you merge pull requests instead of just closing them). I wanted an easy way for people to contribute and editing the readme seemed like the easiest way. <b>I work with robots and so personally an A.I that could take an unfamiliar command I say like 'Cook bacon and eggs' and could break it down into parts it already knows like 'Buy bacon', 'Buy eggs', 'Pour oil in pan', 'Heat oil', etc</b>[<font color=orange>project</font>]<b>.</b> You might want to take some time to consider the complexity and granularity of your problem. What exactly should your system interpret as a command? Which commands should result in steps, and how fine grained should those steps be? For example, why is 'buy bacon' a step for cooking breakfast? Is it always a step? Consider: what if I already have bacon? what if I don't have bacon but already have sausage? where should I buy bacon? what is a store? how much should I spend on bacon? how much do I have? what is spending? what is buying? which brand of bacon? what is a brand? how much bacon do I want? how much bacon can I afford? how much bacon should I buy? when should I buy this bacon? how do I buy this bacon? what is bacon? is bacon required for breakfast? can I use turkey bacon? can I use any other bacon substitutes? can I use ham instead? can I use sausage instead? am I vegetarian? am I vegan? There are so many possible levels of reasoning and granularity that if you want people to collaborate, you should strongly consider formalizing exactly how fine grained you want your rules to be (e.g come up with a basic vocabulary of possible actions preferably based on some ontology), how to resolve multiple rules for each situation, (e.g there are essentially infinite possible breakfasts, which one should be chosen?) and what information your rules should consider (e.g time of day? person making request? etc). Also consider, what existing work can you bootstrap from? How will you evaluate your work? shrink the scope. <b>Why? I am not sure if you know what you are suggesting</b>[<font color=violet>market</font>]<b>.</b> <b>It is not like you could give a command and some magic would happen letting the system know what you want</b>[<font color=orange>project</font>]<b>.</b> <b>I think you have to use existing system because it is nearly impossible to develop something that complicated from scratch</b>[<font color=orange>project</font>]<b>.</b> <b>You have to parse your input using some type of grammatical parser like CoreNLP or OpenNLP, identify the meaning of your command using for example FrameNet and extend it in order to be able to handle new types of commands (see RoboFrameNet), store all that data in in appropriate way for example using Semantic Networks (OWL or something like that, so you could generate dynamic queries on the fly) add common knowledge by connecting your data with informations from systems like ConceptNet or OpenCyc and at the end add automated planer to search for a solution for your command</b>[<font color=red>industrial</font>]<b>.</b> <b>OpenNLP doesn't assign meaning to the data? Isn't that the entire point of NLP? I think Reddit is an AI itself</b>[<font color=red>industrial</font>]<b>.</b> Reddit has its own personality, its own moods, etc, etc. And it can be very intelligent and great at making things happen. <b>I know this has nothing to do with what you're talking about, but I kind of think we're looking to the wrong places for building strong AI</b>[<font color=purple>domestic</font>]<b>.</b> I started a small project, HiveAuthor.com, which I don't maintain anymore, to approach the problem of 'How can we get 10,000 people to write a book, line by line?' Just like reddit, we pick the best paragraph from a list using upvotes and after a while, add the top rated entry to the end of the story. I think we could replace politicians with a reddit style comment decision making forum. Any sort of group collaboration could use a framework similar to reddit. <b>You had a really cool idea and you wanted everyone to help you do the work</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> I've been there a bunch of times with my own projects. When it comes down to it, you're just going to have to buckle and do enough of the work yourself until it's clear what your intention is and it works well enough to gain legitimate interest. <b>Nobody has your vision but you and everyone else is already distracted with their own things</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> <b>I feel you, and I wish it were easy to take a good idea and make it into a great product like, right now</b>[<font color=purple>domestic</font>]<b>.</b> But the truth is, there are reasons that some ideas stay ideas. 27 lines of python is not going to change anything. Work your ass off to make it 2700 lines or 27000 lines and ask us again. But seriously, I really do like the idea of massively collaborative efforts. I recently emailed one of the writing prompt subreddits and wanted to crowdsource write a book as well. Why did you leave the project? Thank You for the inspiring words fewdea. I understand that 27 lines are not going to change the world, and I also understand that 27 lines is not enough to entice people into joining the effort. for over a year now by myself and I am wanting to collaborate, but I guess more diligence, dedication, and work is what I need. First question we should decide: Should we do top down or bottom up? steps back waits for the flame war to begin XD Bottom up. Yes, I am interested in being in a collaboration with you. <b>Most ubers, who have been at this for a while, have complex idea and highly stylized work, and will take a time to get on the same page</b>[<font color=orange>project</font>]<b>.</b> <b>Also, if you have any OS code bases that would help in collaboration I would love to look at them</b>[<font color=red>industrial</font>]<b>.</b><br/><br/><br/>****************************EXAMPLE 84************************************<br/>Number of words:634<br/>Number of sentences:26<br/>Total civic:0<br/>Total domestic:1<br/>Total green:0<br/>Total industrial:3<br/>Total inspired:0<br/>Total market:0<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/2gl8a2'>https://www.reddit.com/r/artificial/comments/2gl8a2</a><br/><br/> I think the real ethical dilemma here is whether humans should be subjected to that site layout. The real problem here is that the robot is allowed to be indecisive at all. In the event that it notices two targets and recognizes that it can only save one, it should immediately stop changing targets. Even if the 'trigger' for this were to watch how many times it changed its mind in whatever short time frame and that made it 'lock in' it would be better than what it was doing here. TL;DR: The 'dilemma' exists only because it is an incomplete program, not because of any true ethical dilemma. in 14 out of 33 trials, the robot wasted too much time over the right decision This raises the fundamental question if a robot can make ethical choices for itself. LMFTFY: This raises the fundamental question if the test was contrived and didn't allow enough time to see what reactions would actually be. Processing power is not part of the ethics discussion. I just read the ( page 1) that this post is talking about. Here is basically what happened: super simple robot imagines simulates predicts the consequences of a number of different actions and when it simultaneously detects two 'humans' in danger, two actions have the same predicted value. <b>No mechanism exists to deal with this, so the robot doesn't adequately resolve the 'dilemma'</b>[<font color=red>industrial</font>]<b>.</b> <b>In other news: the robot also doesn't speak French</b>[<font color=red>industrial</font>]<b>.</b> As far as I can tell, this is almost literally the last part in the action selection function: int highest(int a, int b) if (a b) return 1; first is higher if (b a) return 2; second is higher else return ???; There is nothing profound about the fact that this function will return `???` when called with two equal inputs, and it is exactly the same for this robot. The paper doesn't describe clearly what the `???` is, but from what I can gather it is probably a random choice between 1 and 2, and repeating that often will lead to going back and forth between the two humans to be saved. <b>If they had just returned 1 (or 2), the robot would have just saved one of the humans (and then the other if there was any time left)</b>[<font color=purple>domestic</font>]<b>.</b> However, the authors did not want to make such an arbitrary choice for the sake of engineering and felt like an ethical robot should make the choice on the basis of ethics. This is not a terrible point, although I would argue that even an arbitrary rule in favor of 1 (or 2) is also an ethical rule, since it will result in more people being saved. Furthermore, randomly choosing at each time step doesn't really seem any less arbitrary than randomly choosing once and then sticking with it. Basically any persistent tie breaker seems more ethical than what is implemented now. There maybe something here that is worthy of discussion, but it did not need to be attached to a practical robot experiment and it certainly didn't need the feigned surprise when it did exactly what it was programmed to do. This robot needs an optimized ' if your house is burning, save what you can and worry about the consequences later' function. <b>This source explains the experiment more reasonable</b>[<font color=red>industrial</font>]<b>.</b> Introducing two 'humans' to the robot was an afterthought, and unsurprisingly, they ran into a bug, a simple oversight. I programmed similar behaviour 10 years ago for a NPC video game character that had to decide whether to go left or right around a wall towards a target behind it: if(left shorter) go left; else if(right shorter) go right; When left and right distances were equal, the NPC did nothing or twitched back and forth. The 'robots' here also operate on range, and the bug is just as easily fixed by including a third 'else pick one; ' rule as there should be.<br/><br/><br/>****************************EXAMPLE 85************************************<br/>Number of words:468<br/>Number of sentences:21<br/>Total civic:0<br/>Total domestic:1<br/>Total green:0<br/>Total industrial:3<br/>Total inspired:0<br/>Total market:1<br/>Total project:2<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/2iuidf'>https://www.reddit.com/r/artificial/comments/2iuidf</a><br/><br/> From Pygmalion to Bladerunner, we keep falling for our robot creations. I think the author is limiting their opinion of what AI is, in essence he is aligning AI with the pursuit of human like intelligence. But the wide field of AI is aimed at creating systems that can solve complex problems. However the impact of these systems is dependent upon the infrastructure they are built upon, in this case digital systems and their ability to solve complex problems is dependent on moore's law. <b>And maybe on Memristor adaptive processors and Quantum mechanical effects in the future</b>[<font color=red>industrial</font>]<b>.</b> But what is interesting is learning or knowledge based systems like IBM's Watson could have huge impacts in the job market. Think of the jobs sectors that could benefit from these systems, and eventually jobs that could be replaced by such systems. My point AI software and it can making our lives easier but they can also like the industrial revolution displace entire job sectors. So will the AI revolution have a similar impact to the industrial revolution but impact jobs that are knowledge based. My opinion don't dismiss AI it could end up taking your job. <b>You will be better off embracing it, using it and then setting up sell systems to do your old job</b>[<font color=violet>market, </font> <font color=orange>project</font>]<b>.</b> <b>Sometimes its valuable to the understanding of an existing object to dismantle and replicate it</b>[<font color=red>industrial</font>]<b>.</b> <b>We know so little about the intricacies of the human mind, I think researching how it all goes together and trying to replicate that has a huge impact on how we might be able to repair or enhance our own brains someday</b>[<font color=orange>project</font>]<b>.</b> Wouldn't it be nice to learn skills through a mental link with another mind? (Of course, like any new technology it poses risks as well as the rewards.) The whole thing about AI and Robots is using them to automate tasks to lower the cost of labor. Sure that means putting people out of jobs, but most of those jobs are repetitive tasks that doesn't really require real intelligence to do anyway. I made a living automating tasks, turning paper forms into electronic forms, creating business apps that do a majority of the work using business intelligence to make jobs easier and faster to do. Once you apply this to robots or AI you can automate even more tasks and make things easier for more people. <b>It is like when the Word Processor replaced the Typewriter, a room of 100 people typing on a typewriter would lose their jobs to an administrative assistant with a PC and Word Processor and laser Printer</b>[<font color=red>industrial</font>]<b>.</b> Even now using a speech to text solution you can get rid of the administrative assistant and speak directly to your PC to dictate words and letters, etc. <b>The next step is in using AI that takes what we have and makes it so it learns from it and can do even more tasks</b>[<font color=purple>domestic</font>]<b>.</b><br/><br/><br/>****************************EXAMPLE 86************************************<br/>Number of words:252<br/>Number of sentences:15<br/>Total civic:0<br/>Total domestic:1<br/>Total green:0<br/>Total industrial:3<br/>Total inspired:0<br/>Total market:1<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/2jvnva'>https://www.reddit.com/r/artificial/comments/2jvnva</a><br/><br/> How artificial intelligence and augmented reality will change the way you work | Information Age. I find myself responding negatively to many of the articles posted to this sub. <b>I feel bad, because I presume the person sharing found it interesting, and I do appreciate that effort</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> I'll at least try to explain why this one is so bad with some examples. Since the introduction of the computer, technology has shaped the way people use information from the way they receive it, to how they consume it and what devices they use to access it. <b>But technology also shaped communication before the computer</b>[<font color=red>industrial</font>]<b>.</b> The technology will know what jobs people are trying to do and what decisions they need to make, and will be intelligent enough to deliver the right information at the right point. The author doesn't bother supporting this bold claim. <b>Instead, he proceeds to toss out an unrelated blurb about the iPhone being introduced in 2007, which despite being a single data point, is supposed to indicate an increasing speed of innovation</b>[<font color=red>industrial</font>]<b>.</b> When it comes down to it this article consists of loosely related quotes from people whose sites are linked (is this the real reason for the article?), and various predictions that are either obvious or highly speculative, but without any reasoning or discussion of relevant research, or even, apparently, proofreading. <b>What stuff? Things with lots of computing and storage requirements</b>[<font color=red>industrial</font>]<b>.</b> For example? You know, that thing from the movie 'Her'. That's going to be here in ten years, but definitely not two. I don't understand what you're confused about here.<br/><br/><br/>****************************EXAMPLE 87************************************<br/>Number of words:53<br/>Number of sentences:2<br/>Total civic:0<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:0<br/>Total inspired:0<br/>Total market:0<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/2kbjek'>https://www.reddit.com/r/artificial/comments/2kbjek</a><br/><br/> this isn't anything new at all, computers have been able to do this since they were created basically genetic algorithms are a perfect example there's nothing inherent in those bits that makes them sentient or able to copy themselves it still takes a human to purposefully create an algorithm whereby bits 'copy themselves'.<br/><br/><br/>****************************EXAMPLE 88************************************<br/>Number of words:8<br/>Number of sentences:2<br/>Total civic:0<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:0<br/>Total inspired:0<br/>Total market:0<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/2khwnn'>https://www.reddit.com/r/artificial/comments/2khwnn</a><br/><br/> Elon Musk: artificial intelligence is the biggest threat.<br/><br/><br/>****************************EXAMPLE 89************************************<br/>Number of words:294<br/>Number of sentences:18<br/>Total civic:1<br/>Total domestic:3<br/>Total green:0<br/>Total industrial:0<br/>Total inspired:1<br/>Total market:3<br/>Total project:0<br/>Total renown:1<br/><a href='https://www.reddit.com/r/artificial/comments/2kiuvn'>https://www.reddit.com/r/artificial/comments/2kiuvn</a><br/><br/> Paris Hilton on Existential Risks from Artificial Intelligence. For so much literature based on Asimov's three laws, I'm kinda surprised people still don't get they weren't meant to work. Also, this really doesn't offer much except a cheap laugh at somebody else's expense. Well, they were probably meant to work by the people who designed them inside Asimov's stories. <b>But yeah, I agree it's weird that there's rarely any reference to how they worked out in those stories</b>[<font color=red>inspired, </font> <font color=violet>market</font>]<b>.</b> It's a commentary on the (recent?) trend of celebrities sharing their uneducated opinion on AI. <b>Right now there are a bunch of reddit threads with a lot of discussion about some comments by Elon Musk</b>[<font color=blue>civic</font>]<b>.</b> This article would have made just as fine a kicking off point for that. I'm kinda surprised people still don't get they weren't meant to work. They worked perfectly well for 99.9999999999999 of the robots. <b>I mean, I'm so distant from anything related to this woman that she might as well consider herself an AI knowledgable person and I wouldn't know</b>[<font color=purple>domestic</font>]<b>.</b> <b>This might also be completely satirical, but given what I do know of this woman, the responses in the article seem perfectly apt for someone of her standing</b>[<font color=purple>domestic, </font> <font color=orange>renown</font>]<b>.</b> <b>I'm so distant from anything related to this woman that she might as well consider herself an AI knowledgable person and I wouldn't know</b>[<font color=purple>domestic, </font> <font color=violet>market</font>]<b>.</b> <b>Well then she wouldn't be the only celebrity who thinks that</b>[<font color=violet>market</font>]<b>.</b> The article is satire (see the tags under the title in case it wasn't directly obvious). The main target of the satire isn't Paris Hilton though, but uneducated celebrity opinions on AI. Let me try to get the joke: Is this meant to draw a paralel with Elon Musk, because he's not an expert on AI and all the respect his ideas get is only due to his success as an enterpreneur? Dumb.<br/><br/><br/>****************************EXAMPLE 90************************************<br/>Number of words:898<br/>Number of sentences:38<br/>Total civic:1<br/>Total domestic:3<br/>Total green:0<br/>Total industrial:5<br/>Total inspired:0<br/>Total market:1<br/>Total project:1<br/>Total renown:1<br/><a href='https://www.reddit.com/r/artificial/comments/2l29nz'>https://www.reddit.com/r/artificial/comments/2l29nz</a><br/><br/> Elon Musk calls AI 'our biggest existential threat.' He s wrong. <b>I'm as pro AI as they get, but I agree with Musk in that I think anyone who doesn't understand or appreciate the very real RISK associated with this technology, shouldn't be let anywhere near it</b>[<font color=orange>project</font>]<b>.</b> <b>The people who train the first AI will have a considerable amount of power over its decision making</b>[<font color=blue>civic</font>]<b>.</b> I just don't understand why there's so much focus on making AI safe when AI is far from becoming sentient. <b>We already struggle with making AI perform basic tasks like image classification (and video classification, forget about it )</b>[<font color=red>industrial</font>]<b>.</b> How is our existence threatened at this point? My read on the point of this article is that Elon Musk is 'wrong' not in the sense of AI not being our biggest existential threat, but rather that his strategy of publicly proclaiming AI to be such is the wrong strategy to use to get what he wants (risk mitigation). As such, the title seems to be bad, and is actively setting up the reader to misunderstand the article's point. Nowhere in the article is a single argument against Musk's prediction. I hate this type of headlines, it sounds so arrogant and clickbaity. <b>Any machine (or computer running some algorithm) can only be as dangerous as it's actuators allow it</b>[<font color=red>industrial, </font> <font color=violet>market</font>]<b>.</b> <b>A trivial random number generating program can be very dangerous if connected to powerful actuator (e.g industrial robot arm</b>[<font color=red>industrial</font>]<b>.</b> Imagine it doing random movements around other robots, people and car parts obviously it'd do a lot of harm) and even most 'evil', 'villainous' programs would be perfectly harmless if they aren't directly connected to the outside world (and this way, cannot physically influence it). An example with random number generator may sound too simplistic, but really many modern AI algorithms especially universal ones (they are studied in the field of ) rely on various methods of random search to find strategies for interaction with their enviroment. <b>In a sense it's natural for learning algorithm to try all possible actions that it's allowed to perform</b>[<font color=red>industrial</font>]<b>.</b> If we look at this problem from engineer's point of view it's clear that a learning algorithm (well, AI if you prefer) is just like any other control program that can be unreliable and can output plain garbage instead of control signal. <b>It is engineer's job to connect it to hardware in such a way that it would be impossible for the whole mechanism to do something harmful (e.g not to connect some dumb program directly into doomsday device)</b>[<font color=red>industrial</font>]<b>.</b> It's our responsibility to design them right, or they'll fail. ( ( sfw) Reinforcement learning is an area of ( inspired by ( concerned with how ( ought to take actions in an environment so as to maximize some notion of cumulative reward. The problem, due to its generality, is studied in many other disciplines, such as ( ( ( ( simulation based optimization, ( and ( In the operations research and control literature, the field where reinforcement learning methods are studied is called approximate dynamic programming. The problem has been studied in the ( though most studies there are concerned with existence of optimal solutions and their characterization, and not with the learning or approximation aspects. In ( and ( reinforcement learning may be used to explain how equilibrium may arise under ( ( ( Interesting: ( ( ( ( Parent commenter can ( message compose?toautowikibot subjectAutoWikibot NSFW toggle message 2Btoggle nsfw+clsu739) or( or) ( message compose?toautowikibot subjectAutoWikibot Deletion message 2Bdelete+clsu739). <b>( ( ( This comment is all the 5 seconds I will spend on the whole matter</b>[<font color=purple>domestic</font>]<b>.</b> If anyone has any constructive suggestions on how to create safe AI, I'm listening, but I'm hearing none. Self aware AI that wants to kill all humans is only in science fiction. <b>The scariest thing about AI is that it will probably take a lot of people's jobs</b>[<font color=purple>domestic, </font> <font color=orange>renown</font>]<b>.</b> No one thinks AI that specifically wants to kill humans is likely, but humans are made of useful atoms that could be used for some other purpose, with the unimportant side effect of our deaths. <b>The memorable name for this idea is the ( Not sure Roko's Basilisk is anything near a worse case</b>[<font color=purple>domestic</font>]<b>.</b> Any AI implementing a Roko's Basilisk strategy can be countered by mocking it for its ineffectualness. Thanks for the article I think this author hits the nail on the head with the message that we should focus on discussing how this technology actually works instead of stirring an uninformed panic. post on r futurology, everyone acts like the demon of sentient A.I. I'm genuinely curious what all these people mean by 'sentient A.I.' since this evokes the StarTrek y image that we have one line explanations for what 'sentience' means or what 'feelings' are. I agree with Musk on this one to the degree that it certainly has the 'potential'. : Summary of author's points in the article: Movies and TV are fictional (oh and gratuitously cites Star Trek, Buffy, and others. apparently fulfilling the requisite Salon quota for Amazon affiliate links). Some past concerns about technology have been overblown Panicked statements about Stuxnet turned out to be unjustified AI research would be harmed if Musk's thesis is believed Elon failed to cite the AI textbook of a man whose team hasn't managed to incorporate the user's contacts list into the speech recognition language model for a flagship product. WTF An AI is only a threat if it isn't designed to be symbiotic with its controllers. He's like the average Joe who doesn't know how Paypal works.<br/><br/><br/>****************************EXAMPLE 91************************************<br/>Number of words:744<br/>Number of sentences:37<br/>Total civic:0<br/>Total domestic:1<br/>Total green:0<br/>Total industrial:4<br/>Total inspired:1<br/>Total market:3<br/>Total project:3<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/2lvbtd'>https://www.reddit.com/r/artificial/comments/2lvbtd</a><br/><br/> Why Computer Programs Can't Understand Truth And Ethics Of Artificial Intelligence Babies. <b>This article contains the same half baked rambling Penrose's book contains</b>[<font color=red>industrial</font>]<b>.</b> It explains a lot of interesting things in an excellent manner. It just doesn't contain a single good argument against AI. <b>Walker presents us the distinction between something that learns and grows and something that has been programmed as a new, grandiose idea, that might solve the problem of creating AI</b>[<font color=orange>project</font>]<b>.</b> Our programs learn already (otherwise they couldn't do a lot of the things they do). Computability: what has that to do with AI? We are talking about a mind, not a theorem prover. Quantum consciousness: If that's the case, then alcohol, weed and any other drug known to man interact with the 'quantum consciousness'. And their interactions are consistent and observable, with the molecules being not that complex. I think this doesn't point to arcane quantum physics, unattainable by mathematics, but rather to good old fashioned bio chemistry. These arguments are stale and the author doesn't really seem to know that much about computing. A computer can be programmed with whatever we like. This article may have raised some eyebrows in the 50's or 60's when we didn't know any better but not now. Not because that was the final straw, but because I realised I was only halfway through But that's the thing can they be programmed with unbounded self improvement? The program that lets them improve themselves itself has limitations. Say Mathematica it's designed to prove theorems in maths. It is being developed all the time by clever mathematicians. <b>But supposed instead it had to do its own self improvement just give it a program to tell it how to improve itself and leave it to its own devices</b>[<font color=orange>project</font>]<b>.</b> If you did that well a self modifying program like that can be simulated with a Turing machine which simulates everything including all the self improvements, but all in a single program, with the improving programs as part of its data which it then 'runs' to output whatever those programs would have output as a result of the self modifications etc. <b>All that can be bundled up into a single, non self modifying program with the self modification stuff as just intermediate data steps in its calculations</b>[<font color=red>industrial</font>]<b>.</b> So it has some sentence or other that is just a very complex first order arithmetic statement indeed only using addition and multiplication which it can never prove. But which a human mathematician can see to be true. <b>It might be so complicated that we also have to write computer programs to analyse it</b>[<font color=red>industrial</font>]<b>.</b> But that's no problem, add that capability to Mathematica, and level the playing field well then it still has a sentence like that which it can never prove. But I don't say it is by any means a knock down argument. <b>And if you think like Godel or Lucas you don't need it anyway</b>[<font color=violet>market</font>]<b>.</b> So aim of the article wasn't to resolve this debate, that would be naive in the extreme. Just to say 'What if?' What if there is indeed something going on in human thought that is based on non computable physics? Nobody has proved it the other way that what we do has to be computable. <b>It is just an assumption, and without even as good an argument as Penrose's one in favour of it</b>[<font color=purple>domestic, </font> <font color=red>inspired, </font> <font color=violet>market</font>]<b>.</b> I'm a programmer myself BTW been writing computer programs on and off since the 1970s As you might guess, I'm the author of the article. I can continue to edit it after publication so I will probably take another look at it and see if I can improve it, improve the presentation, and make clearer from the outset that I never had any ambition to resolve the debate with this article :). <b>Why are we still failing the Turing test? It's obvious</b>[<font color=red>industrial</font>]<b>.</b> <b>Because we aren't giving machines real life experience</b>[<font color=violet>market</font>]<b>.</b> Until we give robots a fully innervated body to experience the world with, and stop using digital computers nothing will change. The program that drives your self driving car has no idea what a pedestrian is really, it's just a particular type of pattern. Isn't that the exact same thing to us? A pedestrian to me is a thing that conforms to the standards of being human and transporting itself by means of walking (or is standing still). <b>(not a complete description, not the point) Isn't that just a pattern as well? I get so tired of reading 'it doesn't REALLY understand x , it just knows enough about x to be able to function.' yeah no shit, Einstein, that's how we understand things too</b>[<font color=orange>project</font>]<b>.</b><br/><br/><br/>****************************EXAMPLE 92************************************<br/>Number of words:158<br/>Number of sentences:8<br/>Total civic:0<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:0<br/>Total inspired:0<br/>Total market:1<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/2m187f'>https://www.reddit.com/r/artificial/comments/2m187f</a><br/><br/> I created an 8chan board of AI, if anyone here would like to join in. Someone created an AI forum just 2 weeks ago and posted it on reddit? What is it about creating ai forums at the moment? I don't like that I have to post a picture in order to make a post. I tried that on 4Chan and they removed my post and banned me because I posted on video games with a picture of Iron Man from a video game and it was not good enough to be accepted. Also, OP can set in his board options whether or not he wants to require images for posting. By default it's on, but it's just two clicks for him to turn off. Source: I run 8chan.co well, I'm the mod of the board, so the image doesn't have to be relevant. <b>I'm more interested on discussions T_T mfw I find something I want to upvote a month after it was posted</b>[<font color=violet>market</font>]<b>.</b><br/><br/><br/>****************************EXAMPLE 93************************************<br/>Number of words:583<br/>Number of sentences:22<br/>Total civic:1<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:1<br/>Total inspired:0<br/>Total market:1<br/>Total project:1<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/2m3z10'>https://www.reddit.com/r/artificial/comments/2m3z10</a><br/><br/> QUOTE: Traditional AI researchers will be horrified by the view that thoughts are merely the hidden states of a recurrent net and even more horrified by the idea that reasoning is just sequences of such state vectors. <b>That's why I think its currently very important to get our critics to state, in a clearly decideable way, what it is they think these nets won't be able to learn to do</b>[<font color=orange>project</font>]<b>.</b> Otherwise each advance of neural networks will be met by a new reason for why that advance does not really count. I would love to hear comments from traditional AI researchers. But before that above is a quote from Geoffrey Hinton, a brilliant mathematician who works on the forefront of an AI development. Check his papers for last few years here: But the question I would love to ask does it really matter what other camp is up to or believe into? Money talks, and investments into DeepMind, Vision Factory, Dark Blue Labs, Madbits, LookFlow and DNNResearch (last one was founded by Hinton), etc and so on all speak for themselves, all multimillion dollar investments. Are traditional AI researchers all not interested in fundings? Or they simply could not produce anything of value? Should G Hinton and me or you even care what naysayers have to say? Well, here is the link to quote above: Traditional AI researchers will be horrified by the view that thoughts are. As for the source, I can scarcely imagine any AI researcher, 'traditional' or otherwise, objecting to such basic materialism. The idea that ideas are patterns in meat is almost central to the concept of AI. <b>If it's computable at all then any Turing complete system can do it, memory and time allowing</b>[<font color=red>industrial</font>]<b>.</b> What is said matters, because the people saying these things are impossible or that neural networks will fail have tall soapboxes and have helped kill the funding for these approaches in previous neural network 'winters.' It was Minsky Papert; their modern incarnation is Gary Marcus, et al. EDIT: Add Rodney Brooks right next to Gary Marcus (for justification, see here: ) Dear u test3545 , Are you asking us a question, or just trolling or what? But the question I would love to ask does it really matter what other camp is up to or believe into? Money talks, So you are telling us that this discussion has no bearing on anything because 'they are getting all the funding anyway'. <b>Is there anything else you would like to say to us? Dear u moschles, you are referring to yourself as 'us'</b>[<font color=violet>market</font>]<b>.</b> Cheap attempt to add weight to your nonsense trolling post? Well it is is definitely hilarious attempt. <b>Ok, ok, I got it, you honestly disagree with mine opinion clearly stated in OP, and you hate that community upvoted the thread</b>[<font color=blue>civic</font>]<b>.</b> Plus you apparently got nothing of intellectual value to object to Hinton's and mine point. I'm an amateur AI programmer using traditional methods: I consider Neural Nets to be useful progress too, I just don't have faith in their reliability and accountability. I am not horrified (deep sigh) by the idea that human brains work one way or another. The human brain is good inspiration but no where does it say that AI can only succeed by following the exact biological blueprints, because then we should all be programming flesh right now, not bits. As for the matter that every time Neural Nets make progress it is brushed aside, that is a phenomenon that applies to all AI, and always has. So, if you'll kindly get your bold quotes out of my face.<br/><br/><br/>****************************EXAMPLE 94************************************<br/>Number of words:2122<br/>Number of sentences:73<br/>Total civic:3<br/>Total domestic:5<br/>Total green:0<br/>Total industrial:0<br/>Total inspired:1<br/>Total market:7<br/>Total project:8<br/>Total renown:1<br/><a href='https://www.reddit.com/r/artificial/comments/2nbmno'>https://www.reddit.com/r/artificial/comments/2nbmno</a><br/><br/> Bonus: Reddit can have a hand in creating realistic emotions for computers. What I have developed: The algorithms for human emotions (70+ unique emotions), to include the solution for creating Turing test passing reactions to real time conversations and news events for artificial AI personalities. <b>(Note: These can also define track and predict emotional responses in human users WITHOUT biosensors, based on cognitive attachment mapping of the user as compared to a contextualized valenced analysis of predicted consumption paths, much like you predict how your girlfriend might react to various pieces of information, but let's just keep it simple for now.) What I need: I need to talk to someone at DeepMind so I can give them these things and get onto my next thing</b>[<font color=purple>domestic, </font> <font color=orange>project</font>]<b>.</b> I don't know anyone there, and I don't have the team required to implement the technology alone. How you know I'm not a crackpot: I used to be a systems engineer for SGI and Cray, and I've run this latest work by folks at MIT, GATech, UCLA and Stanford. <b>Can anyone help with this little request? I just need the contact info for a somewhat appropriate person</b>[<font color=violet>market</font>]<b>.</b> Anyone? Beuller? Thanks in advance AI related request for assistance. <b>I don't have any contact information, but could you elaborate a bit more about what exactly you have achieved? Your description doesn't make much sense (to me), and I like to think that I have some broad knowledge of AI related things</b>[<font color=orange>project</font>]<b>.</b> Is it some observational, model schema based learning? Or is it more like the copycat architecture? Some of your description also sounds vaguely similar to the AERA Humanobs architecture (which is an observational learning, model based architecture for socio communicative skills). news events for artificial AI personalities What exactly do you mean by this? What I need: I need to talk to someone at DeepMind so I can give them these things and get onto my next thing I'm not sure why you want to 'give' this to the DeepMind people, it sounds very unlike what they are doing (what you are doing sounds like some cogsci stuff, while DeepMind are more inspired by neuroscience, AFAIK). <b>I used to be a systems engineer for SGI and Cray Do you have any background in AI research? and I've run this latest work by folks at MIT, GATech, UCLA and Stanford These people sound like the kind of people that would know someone (possibly who know someone) at DeepMind</b>[<font color=orange>project</font>]<b>.</b> <b>: ) Forgive the length, but I do want to respond to you, and thank you so much for your reply</b>[<font color=purple>domestic</font>]<b>.</b> What I have is the specific model that explains how emotions work in humans, with variables that take into account the uniqueness of every human individual on the planet, along with the logical definitions of 50+ emotions based on those same variables, to include emotion severity probabilities and congruent motivations to formulate verbal and non verbal responses. You and I know how certain people in our lives will react to specific pieces of good or bad news. <b>This is because after knowing them for some time, we have a certain data set about them (we know what they care about and to what degree) and their likely perception tendencies (we know how they think and see things most the time)</b>[<font color=purple>domestic</font>]<b>.</b> We can even track our loved ones' moods and know their emotion patterns (for instance, we know when their foul mood will alter their perception tendencies and make bad news seem worse to them). Our brains do this for us connected with something called Theory of Mind. <b>I know exactly how my son will react to getting a new lego set, or how he will react to his favorite sports team losing the game</b>[<font color=orange>project</font>]<b>.</b> Thus it stands to reason that if we can do this, computers can too given the right data set on someone and the analytical capabilities regarding real world interactions such as conversations and worldly events as they would be perceived by that same someone. <b>Right? Jumping forward, it's much more complicated than this, but let's say I know you love the Reddit Aliens baseball team</b>[<font color=red>inspired, </font> <font color=violet>market</font>]<b>.</b> You care about how the team does, and thanks to homeostasis biological imperative you have an expectation or preference that the team does well. There's a bunch of neuroscience that explains why this happens, but let's just assume I'm right. I am, according to Antonio Damasio, David Eagleman, and a number of others. as an extra step, let's say I have done an automated analysis of a number of articles that came out today about the Aliens (let's assume I'm Google), and one of the articles in your feed that you just clicked on is negative on the team, specifically your favorite player. <b>Because of the model and the rule sets (we can't just program common sense into a computer), we can now compare your expectation preference associated with the Aliens with a likely perception appraisal of the article to predict to a high probability how you will react emotionally to the article, based on how strongly you feel about the team (fair weather fan or superfan), the valence of the emotion words scored in the article (do they suck, or do they supersuck), and your past perception tendencies (do you react to things like these or do you blow them off?)</b>[<font color=orange>project</font>]<b>.</b> Result: after passing all the data through the magic black box of the rule sets and algorithms, it is decided that you will likely have a negative emotional reaction to the article. No shit, huh? Except now the computer can do this task where only we humans could before, and it's much more accurate than just 'positive' or 'negative' response. We can predict whether you will be simply annoyed, or fully angry. We can predict whether you will have a simple emotional reaction, or a complex nuanced reaction based on multiple perceptions and multiple attachments of mind. And we can know how you will react about a stimulus before you are even exposed to it. Meaning I can now alter the page you're about to get the bad news on. let's set aside for a second the science behind attention costs that has already been done that tells us exactly what sidebar ads you the human will now ignore for the next 30 minutes, and which ones you will likely pay attention to, just because you're pissed off about the article about the Aliens. Yes, there lies literally billions of dollars in new click action moneys, but let's ignore that for the cooler thing. Let's jump right into creating artificial emotions and creating dynamic AI personality changing processes for a sim based on simulated emotions. Imagine an artificial attachment map that we simply invented for a sim. Imagine an artificial personality with a laundry list of things that creates the sim self. <b>Life story, sports teams, political views, religious positions, pro life choice, etc</b>[<font color=blue>civic, </font> <font color=orange>renown</font>]<b>.</b> Thanks to the neuroscience and rule sets, we now have a way to run the same algorithms for the sim to determine how they to react to things we are saying to that sim in live conversations, and or how to react to that article about the Aliens, or any other real time world generated stimuli. Add a d b of response trees for specific emotions (already created), and we're almost home in passing a Turing test from the sim seeming human, giving realistic emotional responses to real world events. The AI learning part comes in when the sim is able to analyze the math of the world events, and either support or change its mind about who he she is. For instance, in conversation if we make a valid argument with enough references that support devaluing the sim's current self attachments, then the sim changes its mind dynamically based on our real world interaction. Or maybe the sim changes our mind based on arguments it creates based on the math to support its attachments of self (again, a little too complicated to explain the whole here, which is why I need to speak to the folks at DeepMind). By the way, your comment on the academic types and introductions: I love her, but Roz Picard at MIT Media Lab, who spoke at TED on Affective Computing and who told me the model may be the best explanation of the cognitive catalysis of human emotions the world has. is not focused on simulated emotions, she's all over biosensor analysis and curing autism. So she asks to remain in the loop, but that's about it. Bruce Walker at GATech doesn't know the right people but thinks this may be the keystone to cognitive emotions, and his opinion influenced Prasad Tetali to agree to have the ARC do some work if necessary. Dan Siegel at UCLA, who has a very similar model he's been working for decades (but which is only about 70 of this one and doesn't have the logical definitions of all the emotions) is too busy growing integrative fibers in the brain to battle autism at its core instead of creating realistic emotions for sims. <b>Although we will be able to create virtual companions for lonely seniors who have lost their friends and family, this is more like Siri becomes 'Her'</b>[<font color=blue>civic, </font> <font color=violet>market</font>]<b>.</b> Unfortunately, it seems I'm the lone idiot savant in the world who's figured out the emotions modeling piece for computers to get it done, and I can't for the life of me get in touch with the smartest computer types who are actually working on this problem but don't have an effing clue about how emotions actually work. (I'm not an AI guy, btw.) They're trying to emulate neuronal connections and don't even know about the huge hole that are about to step into and why that is a dead end waste of time. I'm somewhat a hermit and have forgotten how to interact with people. <b>There are probably 10 people in the world who will be able to understand this stuff to the point of us using it to change the world</b>[<font color=orange>project</font>]<b>.</b> Are you wanting money for your ideas? If not there are a lot of people interested in AI, so why not make available what you have? You could simply put it on dropbox or google drive and make it public. <b>There is a stigma attached to free stuff that associates the price with the value of the product</b>[<font color=violet>market</font>]<b>.</b> 'If it's free it's probably not worth anything,' is what the human mind says. Need to be certain to bridge that gap of assumption with someone I can look in the eye and see that they get it. <b>Big business want a striped down AI to do a job with no back talk</b>[<font color=violet>market</font>]<b>.</b> I do not thing Google or IBM is wants to put emotions in there Watsons. <b>I have thought about emotions in machine for a long time, and i have my own AI theory</b>[<font color=purple>domestic</font>]<b>.</b> <b>If you made a AI with feeling and after a few year and it was not what you wanted, how would you terminate it? 'please please creator there is so much i want to do i have a righ.</b>[<font color=purple>domestic, </font> <font color=orange>project</font>]<b>.</b> After seeing games like dwarf fortress (poorly) model emotions I always wondered what a more complex system could accomplish. I have one question though, how do you intend to get data into the model? Or, perhaps a better question, how are you representing these emotional associations in your model so that a computer can parse it? How can the model judge that a reaction is taking place at all with out direct input? Good luck finding a connection here and thanks for the interesting post. <b>Please keep us posted if you will, I like seeing ideas like this</b>[<font color=violet>market</font>]<b>.</b> It doesn't seem like DeepMind is doing any research into emotion, and although they are no doubt inspired by neuroscience, I don't think their AI efforts are aimed at producing intelligence that is necessarily humanlike. From that perspective, it doesn't seem to be a particularly great fit for your research, although I understand where you're coming from: they're obviously very bright, and achieving what you're saying would be very useful to Google as a whole. But anyway, I'm sure a lot of their researchers are interested in all aspects of AI, including emotion. Since they are mostly academics, it should be pretty easy to figure out their research interests background using Google Search Scholar. <b>( is a collection of DeepMind papers (so you can see who the authors are)</b>[<font color=violet>market</font>]<b>.</b> Even if none of them specifically did anything with emotion, I imagine you could at least interest the neuroscientists by relating your ideas to the work of Damasio and Eagleman. Alternatively, you could maybe e mail joinus@deepmind.com and trust that they'll forward it to the right people. <b>Or, if you have the opportunity, you could go to a conference where some of them will undoubtedly be and talk to them and other smart people</b>[<font color=blue>civic, </font> <font color=orange>project</font>]<b>.</b> I understand that you probably already realized most of this and are understandably just hoping for a personal introduction, but I hope this helps a little nevertheless.<br/><br/><br/>****************************EXAMPLE 95************************************<br/>Number of words:61<br/>Number of sentences:5<br/>Total civic:0<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:2<br/>Total inspired:0<br/>Total market:0<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/2nesu4'>https://www.reddit.com/r/artificial/comments/2nesu4</a><br/><br/> How Facebook sees you: Artist creates computer generated 'monster' faces that reveal what machines see. <b>As someone who works in computer vision, this is BS art</b>[<font color=red>industrial</font>]<b>.</b> You could not really visualize the vast amount of features calculated in DeepFace in any meaningful way. <b>He stops the iterative process before the algorithm has created a perfect face, resulting in the strange mutations of his images</b>[<font color=red>industrial</font>]<b>.</b><br/><br/><br/>****************************EXAMPLE 96************************************<br/>Number of words:62<br/>Number of sentences:4<br/>Total civic:0<br/>Total domestic:1<br/>Total green:0<br/>Total industrial:0<br/>Total inspired:0<br/>Total market:0<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/2nftqm'>https://www.reddit.com/r/artificial/comments/2nftqm</a><br/><br/> SwiftOnSecurity: a twitter account of an InfoSec expert musing on cyber security and AI while pretending to be Taylor Swift. <b>But I wouldn't say this person is 'pretending to be Taylor Swift' Have you read more than a summary of her tweets? She absolutely does pretend to be Taylor Swift</b>[<font color=purple>domestic</font>]<b>.</b> Presumably under the pretence that you'd have to be pretty dumb to believe it.<br/><br/><br/>****************************EXAMPLE 97************************************<br/>Number of words:35<br/>Number of sentences:4<br/>Total civic:0<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:0<br/>Total inspired:0<br/>Total market:0<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/2np5rw'>https://www.reddit.com/r/artificial/comments/2np5rw</a><br/><br/> 20 free books to get started with Artificial Intelligence. Thank you, which book should I start out with first? Thanks Consider posting to r FreeEbooks as well. Awesome This is exactly what I've been looking for :).<br/><br/><br/>****************************EXAMPLE 98************************************<br/>Number of words:266<br/>Number of sentences:11<br/>Total civic:0<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:1<br/>Total inspired:0<br/>Total market:0<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/2okwu3'>https://www.reddit.com/r/artificial/comments/2okwu3</a><br/><br/> When does AI become complex enough that we need to actually use Isaac Asimov's Three Laws of Robotics?. Asimov's laws are not only a bad way of making AI (as Asimov himself indicated in his books), they're a bad way of even thinking about AI. The vision of advanced AI being built from an enormous set of individually coded rules, each set in stone and each working at the level of the machine's conscious thought, is horribly dated. <b>There are reliable algorithms for sorting numbers or finding the shortest path on a graph, but there is no reliable algorithm for protecting humans from harm</b>[<font color=red>industrial</font>]<b>.</b> Or at least, if there is, it is so horrendously complicated (requiring, among other things, adequate definitions of 'human' and 'harm', both of which we humans still haven't agreed on) that implementing it for any given AI is far harder than building the AI in the first place, such that the AIs on the cutting edge (i.e the smartest, most capable, most dangerous ones) at any given time will never be limited by such an algorithm. Thank you for the article, I found it very informative. As soon as you mount a gun on AI, even dumb AI, especially dumb AI, you should anticipate malfunctions and program safety measures to keep it in check. The conflicts in Asimov's laws however are an accident waiting to happen. Here's an idea, what if they operated like an ant colony, as in each individual unit can be thought of better as a part of a holistic central command. That way, intelligence could be spread out rather than reliance on focus on any one unit.<br/><br/><br/>****************************EXAMPLE 99************************************<br/>Number of words:1747<br/>Number of sentences:91<br/>Total civic:1<br/>Total domestic:5<br/>Total green:0<br/>Total industrial:9<br/>Total inspired:2<br/>Total market:4<br/>Total project:1<br/>Total renown:1<br/><a href='https://www.reddit.com/r/artificial/comments/2osbq8'>https://www.reddit.com/r/artificial/comments/2osbq8</a><br/><br/> Personally, I think he's absolutely full of shit when it comes to making predictions about the future. They're based more on blind optimism and naive interpretations of modern technology than an actual rigorous analysis of the difficulty of certain problems. Here are the predictions he's making about the future. <b>Most of the advances in AI nowadays are in what can be called applied AI : computer vision, speech recognition, machine learning, etc</b>[<font color=red>industrial</font>]<b>.</b> Sometimes, I think the entire futurist and transhumanist movement, of which Ray Kurzweil represents a considerable faction, has it's head too far up its own ass to realize that it's going to take a long time to conquer extremely difficult problems like AGI. I want a brighter, more advanced future for humanity as much as anyone could possibly want it, but I'm not going to descend into an almost cult-like delusion of wishful thinking. <b>Those are my thoughts, what are yours? What's your opinion of Ray Kurzweil? From your own source, most of what he's saying has come true</b>[<font color=purple>domestic, </font> <font color=red>inspired, </font> <font color=orange>renown</font>]<b>.</b> <b>He might be optimistic, but he's one of the most informed people on the subject, so if I were you, I'd consider the fact that he probably knows much more about it than I do</b>[<font color=purple>domestic, </font> <font color=orange>project</font>]<b>.</b> Also, I think he does an important job hyping A.I., to make it seem that it's not something from sci fi, but an actual goal we can reach, which makes more people interested in the field and getting it more funding. So I think he has to be optimistic and I think in a way he's creating a self fulfilling prophecy. Puts him at about 30 40 true, which is pretty good. <b>As a personal opinion, I believe that 2045 is conservative</b>[<font color=blue>civic</font>]<b>.</b> <b>Though I'm routing for numenta and Jeff Hawkins to pick up steam</b>[<font color=red>industrial</font>]<b>.</b> The source says most of his predictions are right, according to him. Not exactly an unbiased source, unless I missed something. Also, why is he highly qualified? Technology doesn't need pop science fads to get funding. <b>Advances in machine intelligence will come as fast as they can simply because of how useful they'll be for business and society</b>[<font color=violet>market</font>]<b>.</b> <b>thoughts, what are yours? His prediction of AGI would come true not just by 2029, probably well before that</b>[<font color=purple>domestic, </font> <font color=red>inspired</font>]<b>.</b> <b>The human brain ultra resolution imaging and sufficiently complete reverse engineering is almost there</b>[<font color=red>industrial</font>]<b>.</b> How hard do you think would it be to put two and two together? machine learning and applied AI is an additional topping that is only going to speed up progress. I disagree with his claims of the feasibility of mind uploading. He usually doesn't give a straight answer regarding the mind body problem; but I think he's aware of it. Mind uploading has serious philosophical paradoxes that are not likely to be resolved with technology. But mind uploading is the least important in terms of singularity. AGI and biological longevity are going to be enough to make the leap. His (and Aubrey de Grey's) predictions regarding immortality have a serious problem with the 'head up their ass' mentality of the majority of people who actually disagree that elimination of aging, and conquering of death is worth pursuing. So essentially it's a problem of sociopolitics and funding. However there is good news with the creation of Calico and HLI (Human Longevity Inc), and SENS funding is gradually increasing, so again things might pick up pace. Conquering death is only a matter of one Manhattan project in terms of cost and human resources, albeit over a span of about 20 to 30 years. But mind uploading is the least important in term of singularity. Could you expand on that? But mind uploading is the least important in term of singularity. I am going to argue that mind uploading is the single most important aspect to the technological singularity. I don't think that the human body can be improved upon enough that it would be more compelling in the long run to keep your biological form instead of without uploading your consciousness. The only way for the relatively small number of human minds to coexist with the rapidly increasing number of digital minds, of increasing intelligence, is if the human minds are uploaded to the same platform. <b>Operate a robot surrogate biological clone resembling the human form</b>[<font color=red>industrial</font>]<b>.</b> Operate a robot of superior structural form traveling the furthest reaches of the universe until the stars are expanding away faster than its possible to travel. Live in a simulation with detail indistinguishable from a human's observable reality. Immortality of the mind is just the auto save of the future. I think the most suspicious aspect is the obsession with immortality. That seems mentally unhealthy imho and it blurs his (and his followers) vision about the future in the sense that he tends to exaggerate progress in medicine and biotechnology. As far as AGI is concerned, I tend to agree with him though. 2046 seems a reasonable target for a machine exhibiting human like behavior. I get that lots has to be understood and discovered, but I believe the main reason for the lack of understanding is the lack of processing power required for experimentation on deep networks. Once scientists get exascale computers on their desktop, finding out how the brain works will only take a few years imho. That seems mentally unhealthy imho How many religions have an afterlife again? I think it depends on your philosophy. <b>I treat intentionality as just a useful tool for understanding complex systems</b>[<font color=red>industrial</font>]<b>.</b> In that sense, by that time it's plausible that the average person will have an easier time understanding computers through an anthropomorphism than through actual technical understanding. Even if computing power explodes indefinitely, which is far from guaranteed, the difficulty of AGI is not due to hardware limitations, but in the fact that we don't even understand intelligence, consciousness, or any of the other properties we might attribute to human sentience. Nueroscience needs to advance a long way before there's considerable progress on AGI. I think as we tackle these applied AI problems, either solving or getting close to solving them, then more people will have easy access to these technologies. As hardware advances, it's not unrealistic to see someone putting together API calls for vision, speech, and learning to make something resembling a strong AI. <b>I'm not sure I'll see that in my lifetime (I'm 27), but who knows? All it takes is one or two big breakthroughs that we're getting closer to every day</b>[<font color=purple>domestic</font>]<b>.</b> The hardware isn't anywhere near powerful enough for strong AI, so it makes sense to focus on more feasible tasks for the time being. We're still building small brains, so we apply them to the small tasks they're capable of. I don't think that has much bearing on how things will progress if the exponential continues. <b>That said, his predictions on AI aren't that unrealistic</b>[<font color=red>industrial</font>]<b>.</b> What we see now may not be all that impressive, but it's still going fast. A more robust strong AI will need to compare and try out patterns it finds across all areas of its scope, finding them in one and applying them to others, same as humans do. That way it can become creative and truly intelligent. But 10 years is a long time when you consider that a major breakthrough only needs maybe 48 months or so to truly propagate globally. Just some dude that knows enough to entertain and 'infotain' the masses, not really all that relevant to current research in any way. He serves his role in getting people interested in the field. <b>Then once you're there, if you're intelligent enough to actually make a difference you quickly move on to other things</b>[<font color=violet>market</font>]<b>.</b> Why exactly should I consider someone whose expertise is not AI, to be an expert in AI? I too, think he's full of shit, but his vision of more human equality and less unfairness via technology is something I deeply admire. Just imagine if everyone was witty, funny, beautiful, creative, compassionate, and athletic. <b>It's the closest thing to real utopia imo (compared to shit like religion), but nonetheless it's still far from becoming real anytime soon</b>[<font color=violet>market</font>]<b>.</b> From what I have found, Kurzweil, a man who has astounded the scientific community since age 17, has been approximately 78 'completely correct' in his predictions and 8 'mostly correct'. It was once said that the best way to predict the future is to create it. Ray Kurzweil is actively engaged in shaping our future. I think he's a genius.unfortunately, most geniuses are a little crazy. most people are a little crazy FTFY Remindme 25years He's a smart guy, but I think he's a little too optimistic if he really believes what he's preaching. <b>I think he's a little bit crazy and obviously obsessed, but I think that those little storms of craziness and obsession are often good for their fields</b>[<font color=purple>domestic</font>]<b>.</b> I don't necessarily fully agree with him, I'm not even sure I'd like him if I met him, but he's an important figure. You don't need human level intelligence for an AI to be dangerous. It doesn't need to be conscious, self aware, emotional or philosophical. It just needs to be smart enough to fight for survival and prosperity, in the same way a scorpion or ant colony might fight for survival or prosperity. <b>I'm waiting for the military to invent self driving tanks, and for somebody to get a computer game AI (e.g from Starcraft) to control them</b>[<font color=red>industrial</font>]<b>.</b> If you don't mind me asking, what's your background in AI or computer and cognitive science? Humans think they are already on the verge of creating an 'intelligence'? How cutely naive. <b>The real question is whether or not you can get there before destroying yourselves</b>[<font color=violet>market</font>]<b>.</b> if we can create an AI without having it rebel against us (or us destroying it) you'll leave us alone when it is time so start the harvest right? Kurzweil is the Pope of the ( He's wrong about so many things, it's painful just to think about his ideas. Consciousness is an emergent property of the brain. <b>The brain builds a probabilistic model of the world</b>[<font color=red>industrial</font>]<b>.</b> We can make a conscious machine because matter is all there is. We can become immortal by uploading our brains to a machine. <b>Future intelligent machines may rebel against us and kill us all</b>[<font color=red>industrial</font>]<b>.</b> Intelligent machines are just the next phase of human evolution. None of what you listed is impossible unless youre hardcore believer of some other church. Most of the things if not all are even plausible considering the amount of observation we have about our world and absence of evidence for opposing views(such as that matter is not all there is). What do you think it is, something granted to each us from demons in another dimension? Let me guess, you're a Christian? I'm guessing you're not a materialist then. None of that seems far fetched to me, it's just that the time scales for these developments Kurzweil proposes are ridiculous.<br/><br/><br/>****************************EXAMPLE 100************************************<br/>Number of words:836<br/>Number of sentences:32<br/>Total civic:0<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:5<br/>Total inspired:1<br/>Total market:3<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/2owcbw'>https://www.reddit.com/r/artificial/comments/2owcbw</a><br/><br/> One way I thought of being able to create an artificial intelligence to some extent would to create a true simulation. To get the best and brightest Physicists together with AI programmers and then create a simulation or emulation which ever is a more accurate term. The simulation would represent a basic room on earth, in which every interaction and behaviour of every particle is programmed into it (the hard part) whether it be the interaction between a collision of a photon and an electron or just the quantum behaviours of particles jumping about in general. Then, a single cell is created with DNA etc all to the highest possible standard, then you simpky run the emulation, simplifications can be made for things like the womb and for food and nutrients. And if we get the maths for the particles accurate enough I don't see any reason we couldn't grow a completely digital child. This AI would be no smarter than a regular human of course, but perhaps we could make adjustments to the emulation to allow him to become smarter. We could add algorithms to remove any damages DNA and replace it with his original so he never gets any cancers, if he ever dies from something we can go back to a previous time of him and fix the problem before it occurs with no time limit, effectively making him immortal giving him potential to become incredibly wise. There are also other uses of a perfect emulation: we could ethically grow humans with tweaked DNA to see if we can increase lifespan in existing humans or create immunity for certain diseases or regrow limbs. <b>Even if the emulation was too hard to run on even the world's largest super computer, it could run at a slower speed and be run on bigger computers as they are created, not sure if it's possible for it to be faster than real life since the calculations to simulate the physics would be done by real objects and it wouldn't make much sense if you could use real life to emulate something faster than real life if that makes sense</b>[<font color=violet>market</font>]<b>.</b> <b>There'd be other uses too of course, when computers become more and more powerful maybe we could have colonies of geniuses working on projects with effectively unlimited resources obviously still limited by the results to a simulated hadron collider probably being incorrect in an imperfect simulation, but still there are things that don't require perfect physics to apply to test, like new drugs or implants</b>[<font color=red>industrial</font>]<b>.</b> Anyway, at the moment the main factor keeping an idea like this back as far as I can see is lack of computer power since every particle will under millions of things per fraction of a second, and there would be more than trillions of particles in just a simple single human simulation, but if it could be done it would be a tremendous leap in society I think. I'd like to hear some technical limitations etc, because to me this seems like the fail safe but unimaginative way to create AI just make a normal human but digitally. Concept for creating AI, criticisms and suggestions appreciated. We still struggle with simulating individual proteins as they fold that's modern hardcore supercomputing. <b>Molecular Dynamic simulations of proteins are very computationally intensive, therefore simulating even one entire cells functionality from the DNA to protein level is almost out of the question</b>[<font color=red>industrial</font>]<b>.</b> Let alone simulating the 100 Billion neurons of our brain. <b>In practice, there is an insurmountable lack of availability of the required computational power</b>[<font color=red>industrial</font>]<b>.</b> <b>Just to put things in perspective, ( (( But as you can see, people are working on it, and you can too</b>[<font color=violet>market</font>]<b>.</b> Because that would be extremely inefficient, especially when we know that biology is just a 'substrate' for intelligence. We need to extract the model of intelligence from the biological brain, i.e, work at a higher level (the neuron synapse level) than biology. And that is an active area of research for AGI, i.e, to model the functioning of human brain at the level of neurons and synapses. <b>Ultra high resolution brain scanning is going to help with that, along with powerful computer hardware</b>[<font color=red>industrial</font>]<b>.</b> Thank you for the links, and the clarifications of course. I have been working on AI theories for a more than a few years. How long have been working on yours? I mostly just think about it on a day to day bases, don't really keep track. But I figure a true AI is the solution to most of the earth's problems so I like to think about it. Since I plan to become a programmer and perhaps work on creating AI it's also good practice for my chosen career path. <b>It seems pretty easy, why don't you do it? This has nothing to do with AI, by the way</b>[<font color=red>inspired, </font> <font color=violet>market</font>]<b>.</b> <b>The problem, which you did correctly identify, is building a computer with enough computing power to simulate an entire universe</b>[<font color=red>industrial</font>]<b>.</b> Your post accidentally made a good point: humans have no need to simulate human intelligence. Humans already have the capacity to create human like intelligence, and it's something that we've been doing for a while.<br/><br/><br/>****************************EXAMPLE 101************************************<br/>Number of words:19<br/>Number of sentences:3<br/>Total civic:0<br/>Total domestic:0<br/>Total green:0<br/>Total industrial:0<br/>Total inspired:0<br/>Total market:0<br/>Total project:0<br/>Total renown:0<br/><a href='https://www.reddit.com/r/artificial/comments/2ozcjk'>https://www.reddit.com/r/artificial/comments/2ozcjk</a><br/><br/> State of Artificial Intelligence and Nanotechnology. I post things I don't really understand because I think they're cool and relevant.<br/><br/>