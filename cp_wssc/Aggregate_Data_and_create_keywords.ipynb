{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T13:47:19.627648Z",
     "start_time": "2019-10-02T13:47:19.612420Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aideenf/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/aideenf/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/aideenf/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import ConditionalFreqDist \n",
    "from nltk.probability import FreqDist \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "set(stopwords.words('spanish'))\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# visualization imports\n",
    "from IPython.display import display\n",
    "import io\n",
    " \n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T13:47:23.385779Z",
     "start_time": "2019-10-02T13:47:23.323512Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f57412a7ca446bae4003c8a6716765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='FLAG__AGGREGATE_CONVENTIONS_SENTENCES_FROM_GITHUB')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf380a3d016408faf0fb94d532bb71c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='FLAG__GENERATE_SENTECE_TOKENIZED_DOCS')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80abe38ed9414f508e63cc8de99ef69b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Running on binder')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FLAG__AGGREGATE_CONVENTIONS_SENTENCES_FROM_GITHUB = True\n",
    "FLAG__GENERATE_SENTECE_TOKENIZED_DOCS=True\n",
    "FLAG__CREATE_KEYWORDS = True\n",
    "FLAG__ON_BINDER = False\n",
    "\n",
    "\n",
    "box_gstd = widgets.Checkbox(True, description='FLAG__GENERATE_SENTECE_TOKENIZED_DOCS')\n",
    "box_acsfg = widgets.Checkbox(True, description='FLAG__AGGREGATE_CONVENTIONS_SENTENCES_FROM_GITHUB')\n",
    "box_binder = widgets.Checkbox(False, description='Running on binder')\n",
    "\n",
    "def update_box_gstd(change):\n",
    "    global FLAG__GENERATE_SENTECE_TOKENIZED_DOCS\n",
    "    FLAG__GENERATE_SENTECE_TOKENIZED_DOCS= change['new']\n",
    "box_gstd.observe(update_box_gstd, 'value')\n",
    "\n",
    "def update_box_acsfg(change):\n",
    "    global FLAG__AGGREGATE_CONVENTIONS_SENTENCES_FROM_GITHUB\n",
    "    FLAG__AGGREGATE_CONVENTIONS_SENTENCES_FROM_GITHUB = change['new']\n",
    "\n",
    "box_acsfg.observe(update_box_acsfg, 'value')\n",
    "\n",
    "def update_box_binder(change):\n",
    "    global FLAG__ON_BINDER\n",
    "    FLAG__ON_BINDER = change['new']\n",
    "\n",
    "box_binder.observe(update_box_binder, 'value')\n",
    "\n",
    "#display(box_acsfg, box_gstd, filePath1, filePath2, filePath3)\n",
    "display(box_acsfg, box_gstd, box_binder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T13:47:26.782426Z",
     "start_time": "2019-10-02T13:47:26.774961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    }
   ],
   "source": [
    "if FLAG__ON_BINDER == True:\n",
    "    ROOT_DIR = \"/home/jovyan/cp_wssc\"\n",
    "else:\n",
    "    ROOT_DIR = \".\"\n",
    "    \n",
    "print (ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T13:47:29.865030Z",
     "start_time": "2019-10-02T13:47:29.857860Z"
    }
   },
   "outputs": [],
   "source": [
    "BASE_DIR_SCI_ARTICLES = ROOT_DIR + \"/Data/VALIDATED_DATA/Conventions/scientific_articles/\"\n",
    "BASE_DIR_GIT_HUB =  ROOT_DIR + \"/Data/VALIDATED_DATA/Conventions/github\"\n",
    "BASE_DIR_AGGREGATED =  ROOT_DIR + \"/Data/VALIDATED_DATA/Conventions/aggregated/\"\n",
    "KEYWORDS_DIR =  ROOT_DIR + \"/Data/VALIDATED_DATA/Conventions/keywords/\"\n",
    "TRAINING_DATA_DIR =  ROOT_DIR + \"/Data/VALIDATED_DATA/Conventions/training_data/\"\n",
    "\n",
    "\n",
    "if not os.path.exists(BASE_DIR_SCI_ARTICLES):\n",
    "    os.makedirs(BASE_DIR_SCI_ARTICLES)\n",
    "\n",
    "if not os.path.exists(BASE_DIR_GIT_HUB):\n",
    "    os.makedirs(BASE_DIR_GIT_HUB)\n",
    "\n",
    "if not os.path.exists(BASE_DIR_AGGREGATED):\n",
    "    os.makedirs(BASE_DIR_AGGREGATED)\n",
    "    \n",
    "if not os.path.exists(KEYWORDS_DIR):\n",
    "    os.makedirs(KEYWORDS_DIR)\n",
    "\n",
    "if not os.path.exists(TRAINING_DATA_DIR):\n",
    "    os.makedirs(TRAINING_DATA_DIR)\n",
    "    \n",
    "    \n",
    "NUM_KEYWORDS_PER_CONV = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T13:47:33.418644Z",
     "start_time": "2019-10-02T13:47:33.400659Z"
    }
   },
   "outputs": [],
   "source": [
    "def pre_process(text):\n",
    "    \n",
    "    # lowercase\n",
    "    text=text.lower()\n",
    "    text = text.replace(\"b'\", \"\")\n",
    "    \n",
    "    ## Removing strings such as \\\\xe5 \\\\xe6 \\\\xe7 that appear a lot in the descriptions\n",
    "    text = re.sub(r':?\\\\+x\\w{2}', ' ', text, flags=re.MULTILINE)\n",
    "     #text = text.replace('+',' ')\n",
    "    text = re.sub('-', ' ', text, flags=re.MULTILINE) # Added by Aideen\n",
    "    text = re.sub(' +', ' ', text, flags=re.MULTILINE) # Added by Aideen\n",
    "    text = text.replace('+',' ')\n",
    "    \n",
    "    return text\n",
    "\n",
    "# METHOD CALLED BUT NOT USED YET - MAYBE IMPLEMENT  LATER TO REMOVE SPECIAL CHARS.\n",
    "#######################################################################\n",
    "# using NLTK library take string, tokenize, lower case, remove stop words, lemmatize,\n",
    "# stem etc and return the processed string and word frequency distribution\n",
    "########################################################################\n",
    "def processString(example_sent):\n",
    "\n",
    "    ps = PorterStemmer()\n",
    "    wnl = WordNetLemmatizer()\n",
    "\n",
    "    # Combine the spanish and english stop words as some of the responses are mixed language\n",
    "    # stopwords considered as noise in the text. Text may contain stop words such as\n",
    "    # is, am, are, this, a, an, the, etc. In NLTK for removing stopwords, you need to create\n",
    "    # a list of stopwords and filter out your list of tokens from these words.There are default\n",
    "    # lists for some languages, here we use the default spanish and english stoplist(blacklist)\n",
    "    spanish_stop_words = set(stopwords.words('spanish'))\n",
    "    english_stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    stop_words = [\"a\"]\n",
    "  \n",
    "    stop_words = spanish_stop_words.union(english_stop_words)\n",
    "\n",
    "    # Tokenization is the first step in text analytics. The process of breaking down\n",
    "    # a text paragraph into smaller chunks such as words or sentence is called Tokenization.\n",
    "    # Token is a single entity that is building blocks for sentence or paragraph. Here we are breaking\n",
    "    # a sentence into individual words\n",
    "    word_tokens = word_tokenize(example_sent)\n",
    "\n",
    "    # convert to lower case\n",
    "    word_tokens = [w.lower() for w in word_tokens]\n",
    "    #print(\"--Tokenized and converted to lower case\")\n",
    "\n",
    "    # Only keep alpha characters, no special chars or words containing numerics\n",
    "    words = [word for word in word_tokens if word.isalpha()]\n",
    "    # --Words parsed to only retain alpha chars\"\n",
    "    numbers = [word for word in word_tokens if word.isnumeric()]\n",
    "    # --Words parsed to only retain numeric chars \", numbers)\n",
    "    words = numbers + words\n",
    "    #print(\"--Words parsed: \", words) \n",
    "    \n",
    "    # Remove the stop words (we are removing the english and spanish stop words)\n",
    "    filtered_string = [w for w in words if not w in stop_words]\n",
    "    filtered_string = []\n",
    "    for w in words:\n",
    "        if w not in stop_words:\n",
    "            filtered_string.append(ps.stem(wnl.lemmatize(w)))\n",
    "    #print(\"--Stopwords removed and remaining words Lemmatized and stemmed\")\n",
    "\n",
    "    fDist = FreqDist(filtered_string)\n",
    "    return fDist, filtered_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T13:47:36.628702Z",
     "start_time": "2019-10-02T13:47:36.601074Z"
    }
   },
   "outputs": [],
   "source": [
    "keyword_from_social_scientists_dict = {\n",
    "    'industrial': \n",
    "        ['efficiency', 'efficiant', 'performance', 'performances', 'productivity', 'productive', 'output', 'outputs', 'control', 'power', 'functionality', 'functional', 'organisation', 'organisations', 'professional', 'professionals', 'profession', 'professions', 'reliability', 'reliable', 'foreseeability', 'qualification', 'qualifications', 'expert', 'experts', 'expertise', 'progress' , 'standard', 'standards', 'standardized', 'norms', 'norm', 'growth', 'quantification', 'quantifiy', 'measurement', 'measure', 'measures', 'engineering', 'engineer', 'forecast', 'linearity', 'extrapolation', 'extrapolate', 'scientific', 'plans', 'plan', 'audit', 'audits', 'benchmark', 'benchmarks', 'benchmarking', 'test', 'tests'],\n",
    "    'project':\n",
    "        ['flexibility', 'project', 'projects', 'networking', 'networker', 'networkers', 'activity', 'intermediary', 'mediation', 'switch', 'switching', 'shift', 'shifting', 'mobility', 'dynamic', 'transition', 'transitions', 'temporality', 'fluidity', 'communication', 'communications', 'communicate', 'initiative', 'initiatives', 'bridging', 'adapt', 'adaption', 'adaptions'],\n",
    "    'market':\n",
    "        ['wealth', 'money', 'goods', 'rival', 'selfishness', 'winner', 'winners', 'price', 'customer', 'customers', 'customized', 'seller', 'sellers', 'sell', 'vendor', 'vendors', 'buyer', 'buyers', 'buy', 'purchaser', 'purchasers', 'purchase', 'opportunity', 'opportunities', 'competition', 'compete', 'sale', 'sales', 'business', 'interest', 'transaction', 'transactions', 'availability', 'owner', 'owners', 'bargain', 'contract', 'contracts', 'pay', 'deal', 'trade', 'scarcity', 'trading', 'entrepreneur', 'entrepreneurs', 'entrepreneurial', 'market', 'markets', 'marketing', 'marketplace'],\n",
    "    'inspired':\n",
    "        ['inspiration', 'illuminated', 'unexpected', 'feeling', 'fascination', 'fascinating', 'thrilling', 'genuineness', 'intuition', 'genius', 'brilliant', 'uniqueness', 'non-conformity', 'non-conform', 'passion', 'passionate', 'creative', 'creativity', 'imagination', 'visionary', 'extraordinary', 'emotional', 'conviction', 'holy', 'gifted', 'blessed'],\n",
    "    'civic':\n",
    "        ['collective', 'collectives', 'common good', 'community', 'communities', 'representative', 'representatives', 'general interest', 'unite', 'unity', 'union', 'unions', 'majority', 'civil right', 'civil rights', 'assembly', 'democracy', 'vote', 'votes', 'voting', 'election', 'elections', 'elect', 'equality', 'fair', 'fairness', 'law', 'laws', 'justice', 'unbiased', 'impartial'],\n",
    "    'domestic':\n",
    "        ['superior', 'superiority', 'continuity', 'continuation', 'dependence', 'dependency', 'familiarity', 'tradition', 'traditions', 'origins', 'origin', 'routine', 'routines', 'habit', 'habits', 'etiquette', 'common sense', 'respect', 'duty', 'duties', 'trust', 'craft', 'crafts', 'craftsman', 'crafted'],\n",
    "    'green':\n",
    "        ['nature', 'ecological', 'ecology', 'renewable', 'recyclable', 'recycle', 'sustainable', 'sustainability', 'preserve', 'preservation', 'holism', 'holistic', 'green', 'emission', 'organic', 'organical'],\n",
    "    # 'Vitality'\n",
    "    'renown':\n",
    "        ['acknowledgement', 'praise', 'public relation', 'public relations', 'public opinion', 'brands', 'brand', 'audience', 'follower', 'followers', 'supporter', 'supporters', 'fame', 'influence', 'influences', 'glory', 'attractive', 'appealing']\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T13:47:39.862132Z",
     "start_time": "2019-10-02T13:47:39.854054Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#obtain list of files in alphabetic order\n",
    "justif_files = sorted([f for f in os.listdir(BASE_DIR_SCI_ARTICLES) if (os.path.isfile(os.path.join(BASE_DIR_SCI_ARTICLES, f)) and not f.startswith( '.' ) and not \"random\" in f and not \"training_\" in f and not \"aggregated_\" in f and not \"splitted_\" in f)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T13:47:43.157719Z",
     "start_time": "2019-10-02T13:47:43.147268Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#obtain list of files in alphabetic order\n",
    "conv_files = sorted([f for f in os.listdir(BASE_DIR_GIT_HUB) if (os.path.isfile(os.path.join(BASE_DIR_GIT_HUB, f)) and not f.startswith( '.' ) and f.endswith( '.txt' ) and not \"random\" in f and not \"training_\" in f)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T13:47:46.526139Z",
     "start_time": "2019-10-02T13:47:46.506112Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Data/VALIDATED_DATA/Conventions/scientific_articles/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Civic.txt',\n",
       " 'Domestic.txt',\n",
       " 'Green.txt',\n",
       " 'Industrial.txt',\n",
       " 'Inspired.txt',\n",
       " 'Market.txt',\n",
       " 'Project.txt',\n",
       " 'Renown.txt']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "./Data/VALIDATED_DATA/Conventions/github\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Civic.txt',\n",
       " 'Domestic.txt',\n",
       " 'Green.txt',\n",
       " 'Industrial.txt',\n",
       " 'Inspired.txt',\n",
       " 'Market.txt',\n",
       " 'Project.txt',\n",
       " 'Renown.txt']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(BASE_DIR_SCI_ARTICLES)\n",
    "display(justif_files)\n",
    "print (\"\")\n",
    "print(BASE_DIR_GIT_HUB)\n",
    "display (conv_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agregate Scientific Article sentences with  conventions sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T13:47:53.116247Z",
     "start_time": "2019-10-02T13:47:49.774479Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing  Civic.txt\n",
      "number of sentences from github:  47\n",
      "number of sentences from scientific_articles  174\n",
      "Parsing  Domestic.txt\n",
      "number of sentences from github:  20\n",
      "number of sentences from scientific_articles  185\n",
      "Parsing  Green.txt\n",
      "number of sentences from github:  1\n",
      "number of sentences from scientific_articles  85\n",
      "Parsing  Industrial.txt\n",
      "number of sentences from github:  309\n",
      "number of sentences from scientific_articles  153\n",
      "Parsing  Inspired.txt\n",
      "number of sentences from github:  16\n",
      "number of sentences from scientific_articles  113\n",
      "Parsing  Market.txt\n",
      "number of sentences from github:  100\n",
      "number of sentences from scientific_articles  179\n",
      "Parsing  Project.txt\n",
      "number of sentences from github:  54\n",
      "number of sentences from scientific_articles  300\n",
      "Parsing  Renown.txt\n",
      "number of sentences from github:  8\n",
      "number of sentences from scientific_articles  145\n",
      "\n",
      "New aggregated files created in folder  ./Data/VALIDATED_DATA/Conventions/aggregated/ : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['aggregated_Civic.tsv',\n",
       " 'aggregated_Domestic.tsv',\n",
       " 'aggregated_Green.tsv',\n",
       " 'aggregated_Industrial.tsv',\n",
       " 'aggregated_Inspired.tsv',\n",
       " 'aggregated_Market.tsv',\n",
       " 'aggregated_Project.tsv',\n",
       " 'aggregated_Renown.tsv']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if FLAG__AGGREGATE_CONVENTIONS_SENTENCES_FROM_GITHUB:\n",
    "    docs = []\n",
    "    docsTest = []\n",
    "      #iterate through each file name in BASE_DIR_SCI_ARTICLES and BASE_DIR_GITHUB \n",
    "      #create a new file aggregated_{convention name} which \n",
    "      #contains aggregated content already tokenized into sentences using sent_tokenize\n",
    "    for i,x in enumerate(justif_files):\n",
    "        texts = []\n",
    "        source =  []\n",
    "        convs = []\n",
    "        \n",
    "        \n",
    "        print(\"Parsing \", x)\n",
    "        with open(os.path.join(BASE_DIR_GIT_HUB, x), 'rb') as f2:\n",
    "            data2 = str(f2.read()).replace(\"\\\\n\", \" \")\n",
    "            data2 = re.sub(' +', ' ', data2)\n",
    "            docs.append(pre_process(data2)) \n",
    "            #\n",
    "            a, b = processString(data2)\n",
    "            docsTest.append(b) \n",
    "            #\n",
    "            tokenised_into_sentences = sent_tokenize(data2)\n",
    "            print (\"number of sentences from github: \", len(tokenised_into_sentences))\n",
    "            for s in tokenised_into_sentences:\n",
    "                texts.append(pre_process(s))\n",
    "                source.append (\"github\")\n",
    "                convs.append(x.replace(\".txt\", \"\"))\n",
    "            f2.close() \n",
    "            \n",
    "        \n",
    "        with open(os.path.join(BASE_DIR_SCI_ARTICLES, x), 'rb') as f:\n",
    "             #data = str(f.read())\n",
    "            data = str(f.read()).replace(\"\\\\n\", \" \")\n",
    "            data = re.sub(' +', ' ', data)\n",
    "            #replace docs[i] with docs[i]+ the new data\n",
    "            docs.append(docs.pop(i)+pre_process(data)) \n",
    "            a, b = processString(data)\n",
    "            docsTest.append(docsTest.pop(i)+b) \n",
    "            tokenised_into_sentences = sent_tokenize(data)\n",
    "            print (\"number of sentences from scientific_articles \", len(tokenised_into_sentences))\n",
    "            for s in tokenised_into_sentences:\n",
    "                texts.append(pre_process(s))\n",
    "                source.append (\"sci_article\")\n",
    "                convs.append(x.replace(\".txt\", \"\"))\n",
    "            f.close() \n",
    "        \n",
    "\n",
    "        with open(os.path.join(\n",
    "                                BASE_DIR_AGGREGATED, \n",
    "                                'aggregated_{}'.format(x.replace(\".txt\", \"\"))+\".tsv\"),\n",
    "                                'w+') as f3: \n",
    "            f3.write(\"{}\\t{}\\t{}\\n\".format(\"sentence\", \"provenance\", \"convention\"))\n",
    "            for text, source, conv in zip (texts, source, convs):\n",
    "                #f3.write(t.replace(\"\\\\n\", \"\")+ '\\n')\n",
    "                sentence = text.replace(\"\\\\n\", \"\").replace('\\t', ' ')  # + '\\n'\n",
    "                f3.write(\"{}\\t{}\\t{}\\n\".format(sentence, source, conv.lower()))\n",
    "            f3.close()\n",
    "            \n",
    "    #read newly created aggregated files into list sorted alphabetically      \n",
    "    aggregated_files = sorted([f for f in os.listdir(BASE_DIR_AGGREGATED) if (os.path.isfile(os.path.join(BASE_DIR_AGGREGATED, f)) and not f.startswith( '.' ) and not \"random\" in f and not \"training_\" in f and \"aggregated_\" in f)])\n",
    "    print(\"\")\n",
    "    print(\"New aggregated files created in folder \", BASE_DIR_AGGREGATED, \": \" )\n",
    "    display(aggregated_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T13:47:58.498786Z",
     "start_time": "2019-10-02T13:47:58.427779Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>provenance</th>\n",
       "      <th>convention</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in view of the issues with the swagger codegen...</td>\n",
       "      <td>github</td>\n",
       "      <td>civic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is an easter egg concerned with the com...</td>\n",
       "      <td>github</td>\n",
       "      <td>civic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>we would like to take this opportunity to than...</td>\n",
       "      <td>github</td>\n",
       "      <td>civic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and of course, we couldn\\'t do this without [o...</td>\n",
       "      <td>github</td>\n",
       "      <td>civic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>remember that this is a community project, pe...</td>\n",
       "      <td>github</td>\n",
       "      <td>civic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence provenance convention\n",
       "0  in view of the issues with the swagger codegen...     github      civic\n",
       "1    this is an easter egg concerned with the com...     github      civic\n",
       "2  we would like to take this opportunity to than...     github      civic\n",
       "3  and of course, we couldn\\'t do this without [o...     github      civic\n",
       "4   remember that this is a community project, pe...     github      civic"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>provenance</th>\n",
       "      <th>convention</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1878</th>\n",
       "      <td>retreat outside dreams leads to a fall.</td>\n",
       "      <td>sci_article</td>\n",
       "      <td>inspired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879</th>\n",
       "      <td>beings in the state of unworthiness are define...</td>\n",
       "      <td>sci_article</td>\n",
       "      <td>inspired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1880</th>\n",
       "      <td>they are also qualified by properties that exp...</td>\n",
       "      <td>sci_article</td>\n",
       "      <td>inspired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1881</th>\n",
       "      <td>knowledge acquired through education the routi...</td>\n",
       "      <td>sci_article</td>\n",
       "      <td>inspired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1882</th>\n",
       "      <td>the polity comes undone when the temptation to...</td>\n",
       "      <td>sci_article</td>\n",
       "      <td>inspired</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence   provenance  \\\n",
       "1878            retreat outside dreams leads to a fall.  sci_article   \n",
       "1879  beings in the state of unworthiness are define...  sci_article   \n",
       "1880  they are also qualified by properties that exp...  sci_article   \n",
       "1881  knowledge acquired through education the routi...  sci_article   \n",
       "1882  the polity comes undone when the temptation to...  sci_article   \n",
       "\n",
       "     convention  \n",
       "1878   inspired  \n",
       "1879   inspired  \n",
       "1880   inspired  \n",
       "1881   inspired  \n",
       "1882   inspired  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "convention_sentences_df = pd.concat([pd.read_csv(f, sep='\\t') for f in glob.glob(BASE_DIR_AGGREGATED+'*.tsv')], ignore_index = True)\n",
    "\n",
    "display(convention_sentences_df.head(5))\n",
    "display(convention_sentences_df.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T13:48:02.520446Z",
     "start_time": "2019-10-02T13:48:02.493515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of words in a document is: 28.66171003717472.\n",
      "The minimum number of words in a document is: 1.\n",
      "The maximum number of words in a document is: 269.\n"
     ]
    }
   ],
   "source": [
    "document_lengths = np.array(list(map(len, convention_sentences_df.sentence.str.split(' '))))\n",
    "\n",
    "print(\"The average number of words in a document is: {}.\".format(np.mean(document_lengths)))\n",
    "print(\"The minimum number of words in a document is: {}.\".format(min(document_lengths)))\n",
    "print(\"The maximum number of words in a document is: {}.\".format(max(document_lengths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USE TF_IDF to create keywords per convention\n",
    "TF-IDF stands for “Term Frequency — Inverse Data Frequency”. \n",
    "\n",
    "Term Frequency (tf): gives us the frequency of the word in each document in the corpus. It is the ratio of number of times the word appears in a document compared to the total number of words in that document. It increases as the number of occurrences of that word within the document increases. Each document has its own tf.\n",
    "\n",
    "Inverse Data Frequency (idf): used to calculate the weight of rare words across all documents in the corpus. The words that occur rarely in the corpus have a high IDF score. \n",
    "\n",
    "For equations see:\n",
    "https://www.freecodecamp.org/news/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T13:48:06.480123Z",
     "start_time": "2019-10-02T13:48:06.395192Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['view',\n",
       " 'issues',\n",
       " 'swagger',\n",
       " 'codegen',\n",
       " 'beta',\n",
       " 'release',\n",
       " 'disagreement',\n",
       " 'direction',\n",
       " '40',\n",
       " 'top']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### \n",
    "#To test this we need to first reconvert the words into one string. \n",
    "#docs = docsTest\n",
    "########\n",
    "cv=CountVectorizer(max_df=0.85,max_features=10000)\n",
    "word_count_vector=cv.fit_transform(docs)\n",
    "list(cv.vocabulary_.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T13:48:10.155102Z",
     "start_time": "2019-10-02T13:48:10.134428Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T13:48:13.716451Z",
     "start_time": "2019-10-02T13:48:13.707823Z"
    }
   },
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    " \n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    " \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    " \n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T13:48:17.624625Z",
     "start_time": "2019-10-02T13:48:17.496804Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>convention</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>community</td>\n",
       "      <td>civic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>collective</td>\n",
       "      <td>civic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>civic</td>\n",
       "      <td>civic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>solidarity</td>\n",
       "      <td>civic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>interests</td>\n",
       "      <td>civic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      keyword convention\n",
       "0   community      civic\n",
       "1  collective      civic\n",
       "2       civic      civic\n",
       "3  solidarity      civic\n",
       "4   interests      civic"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# you only needs to do this once, this is a mapping of index to \n",
    "feature_names=cv.get_feature_names()\n",
    "\n",
    "keywords_for_df = []\n",
    "convention_for_df = []\n",
    "for i,f in enumerate(aggregated_files):\n",
    "    # get the document that we want to extract keywords from\n",
    "    doc = docs[i]\n",
    "    #generate tf-idf for the given document\n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
    "\n",
    "    #sort the tf-idf vectors by descending order of scores\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "    #extract only the top n; n here is 30\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items,NUM_KEYWORDS_PER_CONV)\n",
    "    \n",
    "\n",
    "    ss_keys = keyword_from_social_scientists_dict[f.replace('aggregated_', '').replace('.tsv','').lower()]\n",
    "    # now add the result to dataframe\n",
    "    for k in keywords:\n",
    "        convention_for_df.append(f.replace('aggregated_', '').replace('.tsv','').lower())\n",
    "        keywords_for_df.append(k)\n",
    "    for key in ss_keys:\n",
    "        convention_for_df.append(f.replace('aggregated_', '').replace('.tsv','').lower())\n",
    "        keywords_for_df.append(key)\n",
    "\n",
    "zippedList =  list(zip(keywords_for_df, convention_for_df))\n",
    "keywords_df = pd.DataFrame(zippedList, columns = ['keyword' , 'convention'])\n",
    "keywords_df = keywords_df.drop_duplicates()\n",
    "display (keywords_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store keywords as CSV file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T13:48:20.350538Z",
     "start_time": "2019-10-02T13:48:20.337251Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'storing keywords.csv at path:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'./Data/VALIDATED_DATA/Conventions/keywords/keywords.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = KEYWORDS_DIR+'keywords.csv'\n",
    "display (\"storing keywords.csv at path:\", path)\n",
    "keywords_df.to_csv(path,index = None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training data DF and Save as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T13:48:23.283708Z",
     "start_time": "2019-10-02T13:48:23.211519Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying one hot encoding to convention\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>provenance</th>\n",
       "      <th>convention_civic</th>\n",
       "      <th>convention_domestic</th>\n",
       "      <th>convention_green</th>\n",
       "      <th>convention_industrial</th>\n",
       "      <th>convention_inspired</th>\n",
       "      <th>convention_market</th>\n",
       "      <th>convention_project</th>\n",
       "      <th>convention_renown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in view of the issues with the swagger codegen...</td>\n",
       "      <td>github</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is an easter egg concerned with the com...</td>\n",
       "      <td>github</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>we would like to take this opportunity to than...</td>\n",
       "      <td>github</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and of course, we couldn\\'t do this without [o...</td>\n",
       "      <td>github</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>remember that this is a community project, pe...</td>\n",
       "      <td>github</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence provenance  \\\n",
       "0  in view of the issues with the swagger codegen...     github   \n",
       "1    this is an easter egg concerned with the com...     github   \n",
       "2  we would like to take this opportunity to than...     github   \n",
       "3  and of course, we couldn\\'t do this without [o...     github   \n",
       "4   remember that this is a community project, pe...     github   \n",
       "\n",
       "   convention_civic  convention_domestic  convention_green  \\\n",
       "0                 1                    0                 0   \n",
       "1                 1                    0                 0   \n",
       "2                 1                    0                 0   \n",
       "3                 1                    0                 0   \n",
       "4                 1                    0                 0   \n",
       "\n",
       "   convention_industrial  convention_inspired  convention_market  \\\n",
       "0                      0                    0                  0   \n",
       "1                      0                    0                  0   \n",
       "2                      0                    0                  0   \n",
       "3                      0                    0                  0   \n",
       "4                      0                    0                  0   \n",
       "\n",
       "   convention_project  convention_renown  \n",
       "0                   0                  0  \n",
       "1                   0                  0  \n",
       "2                   0                  0  \n",
       "3                   0                  0  \n",
       "4                   0                  0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Apply one hot encoding to column \"convention\"\n",
    "print(\"Applying one hot encoding to convention\")\n",
    "convention_sentences_df[\"convention\"] = pd.Categorical(convention_sentences_df[\"convention\"])\n",
    "dfDummies = pd.get_dummies(convention_sentences_df[\"convention\"], prefix=\"convention\")\n",
    "convention_sentences_df = pd.concat([convention_sentences_df, dfDummies], axis=1)\n",
    "convention_sentences_df = convention_sentences_df.drop([\"convention\"], axis=1)\n",
    "display (convention_sentences_df.head(5))\n",
    "convention_sentences_df.to_csv(TRAINING_DATA_DIR+'training_data.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP HERE (REVIEW REST OF CODE TO CREATE A panda DF per model and dump each to a csv file rather than several read/write to files )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the training data per model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code generates a training file for each characteristic by \n",
    "## concetanting sentences for convention X annotated with label one \n",
    "## and sentences from all the other convention that are annotated with label 0.\n",
    "\n",
    "\n",
    "#Generate the data frames for each model\n",
    "#df_by_convention = df[df.convention == 'Green']\n",
    "#display (df_by_convention.sample(n = 3))\n",
    "\n",
    "\n",
    "##It generates a training_{convention_name}.csv file for each convention\n",
    "def generate_training_files(data_dir, data_files):\n",
    "    for file_name in data_files:\n",
    "\n",
    "        f = open(os.path.join(data_dir,file_name), \"r\")\n",
    "        original_lines = f.readlines()\n",
    "        \n",
    "        original_lines2=[]\n",
    "        for l in original_lines:\n",
    "                for l2 in l.split(\".\"):\n",
    "                    original_lines2.append(l2)\n",
    "        f.close()\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Step 1: Generate four random numbers between 0 and 1\n",
    "        Step 2: Add these four numbers\n",
    "        Step 3: Divide each of the four numbers by the sum,\n",
    "        Step 4: Multiply by 100, and round to the nearest integer.\n",
    "        \"\"\"\n",
    "        num_original_lines = len(original_lines2)*2\n",
    "\n",
    "        num_random_lines = np.random.uniform(0,1,len(data_files))\n",
    "        num_random_lines = (num_random_lines / sum(num_random_lines))*num_original_lines\n",
    "\n",
    "        tmp_all_files = [f for f in data_files if f != file_name]\n",
    "\n",
    "        random_lines = []    \n",
    "        for idx, file_name2 in enumerate(tmp_all_files):\n",
    "            f2 = open(os.path.join(data_dir,file_name2), \"r\")\n",
    "            tmp_lines = f2.readlines()\n",
    "            \n",
    "            tmp_lines2 = []\n",
    "            for l in tmp_lines:\n",
    "                for l2 in l.split(\".\"):\n",
    "                    tmp_lines2.append(l2)\n",
    "            f2.close()\n",
    "\n",
    "            for i in range(int(num_random_lines[idx])):\n",
    "                random_lines.append(random.choice(tmp_lines2))\n",
    "\n",
    "        with open(rreplace(os.path.join(data_dir,file_name), \"/\", \"/training_\",1), \"w\") as f3:\n",
    "            f3.write(\"{}\\t{}\\n\".format(\"category\", \"text\"))\n",
    "\n",
    "            for idx, l in enumerate(original_lines2):\n",
    "                l = l.replace('\\n', ' ').replace('\\t', ' ')\n",
    "                f3.write(\"{}\\t\\\"{}\\\"\\n\".format(1, l))\n",
    "\n",
    "\n",
    "            for idx, l2 in enumerate(random_lines):\n",
    "                l2 = l2.replace('\\n', '').replace('\\t', ' ')\n",
    "                f3.write(\"{}\\t\\\"{}\\\"\\n\".format(0, l2))\n",
    "            f3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
