{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T15:39:37.644569Z",
     "start_time": "2019-10-01T15:39:37.622164Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "\n",
    "## Deep Learning imports for the classifiers\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, Concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "##Â ML required imports (for clustering)\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA, LatentDirichletAllocation\n",
    "from sklearn.preprocessing import scale, StandardScaler\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer,TfidfTransformer\n",
    "\n",
    "\n",
    "# visualization imports\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import base64\n",
    "import io\n",
    "%matplotlib inline\n",
    "sns.set() \n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T17:01:24.833278Z",
     "start_time": "2019-10-01T17:01:24.765287Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e4eed2cfa74a43b473731700970365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='FLAG__AGGREGATE_CONVENTIONS_SENTENCES_FROM_GITHUB')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "695cec9dad974ab5869ed35ece49249b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='FLAG__GENERATE_SENTECE_TOKENIZED_DOCS')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c7b78c52c544834b02a06b467cdb813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Running on binder')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FLAG__AGGREGATE_CONVENTIONS_SENTENCES_FROM_GITHUB = True\n",
    "FLAG__GENERATE_SENTECE_TOKENIZED_DOCS=True\n",
    "FLAG__CREATE_KEYWORDS = True\n",
    "FLAG__ON_BINDER = False\n",
    "\n",
    "\n",
    "box_gstd = widgets.Checkbox(True, description='FLAG__GENERATE_SENTECE_TOKENIZED_DOCS')\n",
    "box_acsfg = widgets.Checkbox(True, description='FLAG__AGGREGATE_CONVENTIONS_SENTENCES_FROM_GITHUB')\n",
    "box_binder = widgets.Checkbox(False, description='Running on binder')\n",
    "\n",
    "def update_box_gstd(change):\n",
    "    global FLAG__GENERATE_SENTECE_TOKENIZED_DOCS\n",
    "    FLAG__GENERATE_SENTECE_TOKENIZED_DOCS= change['new']\n",
    "box_gstd.observe(update_box_gstd, 'value')\n",
    "\n",
    "def update_box_acsfg(change):\n",
    "    global FLAG__AGGREGATE_CONVENTIONS_SENTENCES_FROM_GITHUB\n",
    "    FLAG__AGGREGATE_CONVENTIONS_SENTENCES_FROM_GITHUB = change['new']\n",
    "\n",
    "box_acsfg.observe(update_box_acsfg, 'value')\n",
    "\n",
    "def update_box_binder(change):\n",
    "    global FLAG__ON_BINDER\n",
    "    FLAG__ON_BINDER = change['new']\n",
    "\n",
    "box_binder.observe(update_box_binder, 'value')\n",
    "\n",
    "#display(box_acsfg, box_gstd, filePath1, filePath2, filePath3)\n",
    "display(box_acsfg, box_gstd, box_binder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T17:01:58.065745Z",
     "start_time": "2019-10-01T17:01:58.057068Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    }
   ],
   "source": [
    "if FLAG__ON_BINDER == True:\n",
    "    ROOT_DIR = \"/home/jovyan/cp_wssc\"\n",
    "else:\n",
    "    ROOT_DIR = \".\"\n",
    "    \n",
    "print (ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T17:02:25.969241Z",
     "start_time": "2019-10-01T17:02:25.959104Z"
    }
   },
   "outputs": [],
   "source": [
    "BASE_DIR_SCI_ARTICLES = ROOT_DIR + \"/Data/VALIDATED_DATA/Conventions/scientific_articles/\"\n",
    "BASE_DIR_GIT_HUB =  ROOT_DIR + \"/Data/VALIDATED_DATA/Conventions/github\"\n",
    "BASE_DIR_AGGREGATED =  ROOT_DIR + \"/Data/VALIDATED_DATA/Conventions/aggregated/\"\n",
    "KEYWORDS_DIR =  ROOT_DIR + \"/Data/VALIDATED_DATA/Conventions/keywords/\"\n",
    "\n",
    "\n",
    "if not os.path.exists(BASE_DIR_SCI_ARTICLES):\n",
    "    os.makedirs(BASE_DIR_SCI_ARTICLES)\n",
    "\n",
    "if not os.path.exists(BASE_DIR_GIT_HUB):\n",
    "    os.makedirs(BASE_DIR_GIT_HUB)\n",
    "\n",
    "if not os.path.exists(BASE_DIR_AGGREGATED):\n",
    "    os.makedirs(BASE_DIR_AGGREGATED)\n",
    "    \n",
    "if not os.path.exists(KEYWORDS_DIR):\n",
    "    os.makedirs(KEYWORDS_DIR)\n",
    "    \n",
    "    \n",
    "NUM_KEYWORDS_PER_CONV = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T15:39:42.882564Z",
     "start_time": "2019-10-01T15:39:42.875184Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#obtain list of files in alphabetic order\n",
    "justif_files = sorted([f for f in os.listdir(BASE_DIR_SCI_ARTICLES) if (os.path.isfile(os.path.join(BASE_DIR_SCI_ARTICLES, f)) and not f.startswith( '.' ) and not \"random\" in f and not \"training_\" in f and not \"aggregated_\" in f and not \"splitted_\" in f)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T15:39:46.278273Z",
     "start_time": "2019-10-01T15:39:46.269582Z"
    }
   },
   "outputs": [],
   "source": [
    "def pre_process(text):\n",
    "    \n",
    "    # lowercase\n",
    "    text=text.lower()\n",
    "    text = text.replace(\"b'\", \"\")\n",
    "    \n",
    "    ## Removing strings such as \\\\xe5 \\\\xe6 \\\\xe7 that appear a lot in the descriptions\n",
    "    text = re.sub(r':?\\\\+x\\w{2}', ' ', text, flags=re.MULTILINE)\n",
    "     #text = text.replace('+',' ')\n",
    "    text = re.sub('-', ' ', text, flags=re.MULTILINE) # Added by Aideen\n",
    "    text = re.sub(' +', ' ', text, flags=re.MULTILINE) # Added by Aideen\n",
    "    text = text.replace('+',' ')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T15:39:47.676593Z",
     "start_time": "2019-10-01T15:39:47.670710Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#obtain list of files in alphabetic order\n",
    "conv_files = sorted([f for f in os.listdir(BASE_DIR_GIT_HUB) if (os.path.isfile(os.path.join(BASE_DIR_GIT_HUB, f)) and not f.startswith( '.' ) and f.endswith( '.txt' ) and not \"random\" in f and not \"training_\" in f)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T15:39:49.059124Z",
     "start_time": "2019-10-01T15:39:49.047468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./DATA/VALIDATED_DATA/Conventions/scientific_articles/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Civic.txt',\n",
       " 'Domestic.txt',\n",
       " 'Green.txt',\n",
       " 'Industrial.txt',\n",
       " 'Inspired.txt',\n",
       " 'Market.txt',\n",
       " 'Project.txt',\n",
       " 'Renown.txt']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "./DATA/VALIDATED_DATA/Conventions/github\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Civic.txt',\n",
       " 'Domestic.txt',\n",
       " 'Green.txt',\n",
       " 'Industrial.txt',\n",
       " 'Inspired.txt',\n",
       " 'Market.txt',\n",
       " 'Project.txt',\n",
       " 'Renown.txt']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(BASE_DIR_SCI_ARTICLES)\n",
    "display(justif_files)\n",
    "print (\"\")\n",
    "print(BASE_DIR_GIT_HUB)\n",
    "display (conv_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agregate Scientific Article sentences with  conventions sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T15:39:58.801124Z",
     "start_time": "2019-10-01T15:39:58.378406Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing  Civic.txt\n",
      "number of sentences from github:  47\n",
      "number of sentences from scientific_articles  174\n",
      "Parsing  Domestic.txt\n",
      "number of sentences from github:  20\n",
      "number of sentences from scientific_articles  185\n",
      "Parsing  Green.txt\n",
      "number of sentences from github:  1\n",
      "number of sentences from scientific_articles  85\n",
      "Parsing  Industrial.txt\n",
      "number of sentences from github:  309\n",
      "number of sentences from scientific_articles  153\n",
      "Parsing  Inspired.txt\n",
      "number of sentences from github:  16\n",
      "number of sentences from scientific_articles  113\n",
      "Parsing  Market.txt\n",
      "number of sentences from github:  100\n",
      "number of sentences from scientific_articles  179\n",
      "Parsing  Project.txt\n",
      "number of sentences from github:  54\n",
      "number of sentences from scientific_articles  300\n",
      "Parsing  Renown.txt\n",
      "number of sentences from github:  8\n",
      "number of sentences from scientific_articles  145\n",
      "\n",
      "New aggregated files created in folder  ./DATA/VALIDATED_DATA/Conventions/aggregated/ : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['aggregated_Civic.tsv',\n",
       " 'aggregated_Domestic.tsv',\n",
       " 'aggregated_Green.tsv',\n",
       " 'aggregated_Industrial.tsv',\n",
       " 'aggregated_Inspired.tsv',\n",
       " 'aggregated_Market.tsv',\n",
       " 'aggregated_Project.tsv',\n",
       " 'aggregated_Renown.tsv']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if FLAG__AGGREGATE_CONVENTIONS_SENTENCES_FROM_GITHUB:\n",
    "    docs = []\n",
    "      #iterate through each file name in BASE_DIR_SCI_ARTICLES and BASE_DIR_GITHUB \n",
    "      #create a new file aggregated_{convention name} which \n",
    "      #contains aggregated content already tokenized into sentences using sent_tokenize\n",
    "    for i,x in enumerate(justif_files):\n",
    "        texts = []\n",
    "        source =  []\n",
    "        convs = []\n",
    "        \n",
    "        \n",
    "        print(\"Parsing \", x)\n",
    "        with open(os.path.join(BASE_DIR_GIT_HUB, x), 'rb') as f2:\n",
    "            data2 = str(f2.read()).replace(\"\\\\n\", \" \")\n",
    "            data2 = re.sub(' +', ' ', data2)\n",
    "            docs.append(pre_process(data2)) \n",
    "            tokenised_into_sentences = sent_tokenize(data2)\n",
    "            print (\"number of sentences from github: \", len(tokenised_into_sentences))\n",
    "            for s in tokenised_into_sentences:\n",
    "                texts.append(pre_process(s))\n",
    "                source.append (\"github\")\n",
    "                convs.append(x.replace(\".txt\", \"\"))\n",
    "            f2.close() \n",
    "            \n",
    "        \n",
    "        with open(os.path.join(BASE_DIR_SCI_ARTICLES, x), 'rb') as f:\n",
    "             #data = str(f.read())\n",
    "            data = str(f.read()).replace(\"\\\\n\", \" \")\n",
    "            data = re.sub(' +', ' ', data)\n",
    "            #replace docs[i] with docs[i]+ the new data\n",
    "            docs.append(docs.pop(i)+pre_process(data)) \n",
    "            tokenised_into_sentences = sent_tokenize(data)\n",
    "            print (\"number of sentences from scientific_articles \", len(tokenised_into_sentences))\n",
    "            for s in tokenised_into_sentences:\n",
    "                texts.append(pre_process(s))\n",
    "                source.append (\"justification\")\n",
    "                convs.append(x.replace(\".txt\", \"\"))\n",
    "            f.close() \n",
    "        \n",
    "\n",
    "        with open(os.path.join(\n",
    "                                BASE_DIR_AGGREGATED, \n",
    "                                'aggregated_{}'.format(x.replace(\".txt\", \"\"))+\".tsv\"),\n",
    "                                'w+') as f3: \n",
    "            f3.write(\"{}\\t{}\\t{}\\n\".format(\"sentence\", \"provenance\", \"convention\"))\n",
    "            for text, source, conv in zip (texts, source, convs):\n",
    "                #f3.write(t.replace(\"\\\\n\", \"\")+ '\\n')\n",
    "                sentence = text.replace(\"\\\\n\", \"\").replace('\\t', ' ')  # + '\\n'\n",
    "                f3.write(\"{}\\t{}\\t{}\\n\".format(sentence, source, conv.lower()))\n",
    "            f3.close()\n",
    "            \n",
    "    #read newly created aggregated files into list sorted alphabetically      \n",
    "    aggregated_files = sorted([f for f in os.listdir(BASE_DIR_AGGREGATED) if (os.path.isfile(os.path.join(BASE_DIR_AGGREGATED, f)) and not f.startswith( '.' ) and not \"random\" in f and not \"training_\" in f and \"aggregated_\" in f)])\n",
    "    print(\"\")\n",
    "    print(\"New aggregated files created in folder \", BASE_DIR_AGGREGATED, \": \" )\n",
    "    display(aggregated_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T15:40:04.297063Z",
     "start_time": "2019-10-01T15:40:04.196696Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>provenance</th>\n",
       "      <th>convention</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in view of the issues with the swagger codegen...</td>\n",
       "      <td>github</td>\n",
       "      <td>civic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is an easter egg concerned with the com...</td>\n",
       "      <td>github</td>\n",
       "      <td>civic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>we would like to take this opportunity to than...</td>\n",
       "      <td>github</td>\n",
       "      <td>civic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and of course, we couldn\\'t do this without [o...</td>\n",
       "      <td>github</td>\n",
       "      <td>civic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>remember that this is a community project, pe...</td>\n",
       "      <td>github</td>\n",
       "      <td>civic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence provenance convention\n",
       "0  in view of the issues with the swagger codegen...     github      civic\n",
       "1    this is an easter egg concerned with the com...     github      civic\n",
       "2  we would like to take this opportunity to than...     github      civic\n",
       "3  and of course, we couldn\\'t do this without [o...     github      civic\n",
       "4   remember that this is a community project, pe...     github      civic"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>provenance</th>\n",
       "      <th>convention</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1878</th>\n",
       "      <td>retreat outside dreams leads to a fall.</td>\n",
       "      <td>justification</td>\n",
       "      <td>inspired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879</th>\n",
       "      <td>beings in the state of unworthiness are define...</td>\n",
       "      <td>justification</td>\n",
       "      <td>inspired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1880</th>\n",
       "      <td>they are also qualified by properties that exp...</td>\n",
       "      <td>justification</td>\n",
       "      <td>inspired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1881</th>\n",
       "      <td>knowledge acquired through education the routi...</td>\n",
       "      <td>justification</td>\n",
       "      <td>inspired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1882</th>\n",
       "      <td>the polity comes undone when the temptation to...</td>\n",
       "      <td>justification</td>\n",
       "      <td>inspired</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence     provenance  \\\n",
       "1878            retreat outside dreams leads to a fall.  justification   \n",
       "1879  beings in the state of unworthiness are define...  justification   \n",
       "1880  they are also qualified by properties that exp...  justification   \n",
       "1881  knowledge acquired through education the routi...  justification   \n",
       "1882  the polity comes undone when the temptation to...  justification   \n",
       "\n",
       "     convention  \n",
       "1878   inspired  \n",
       "1879   inspired  \n",
       "1880   inspired  \n",
       "1881   inspired  \n",
       "1882   inspired  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "convention_sentences_df = pd.concat([pd.read_csv(f, sep='\\t') for f in glob.glob(BASE_DIR_AGGREGATED+'*.tsv')], ignore_index = True)\n",
    "\n",
    "display(convention_sentences_df.head(5))\n",
    "display(convention_sentences_df.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T15:40:05.221096Z",
     "start_time": "2019-10-01T15:40:05.194332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of words in a document is: 28.66171003717472.\n",
      "The minimum number of words in a document is: 1.\n",
      "The maximum number of words in a document is: 269.\n"
     ]
    }
   ],
   "source": [
    "document_lengths = np.array(list(map(len, convention_sentences_df.sentence.str.split(' '))))\n",
    "\n",
    "print(\"The average number of words in a document is: {}.\".format(np.mean(document_lengths)))\n",
    "print(\"The minimum number of words in a document is: {}.\".format(min(document_lengths)))\n",
    "print(\"The maximum number of words in a document is: {}.\".format(max(document_lengths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USE TF_IDF to create keywords per convention\n",
    "TF-IDF stands for “Term Frequency — Inverse Data Frequency”. \n",
    "\n",
    "Term Frequency (tf): gives us the frequency of the word in each document in the corpus. It is the ratio of number of times the word appears in a document compared to the total number of words in that document. It increases as the number of occurrences of that word within the document increases. Each document has its own tf.\n",
    "\n",
    "Inverse Data Frequency (idf): used to calculate the weight of rare words across all documents in the corpus. The words that occur rarely in the corpus have a high IDF score. \n",
    "\n",
    "For equations see:\n",
    "https://www.freecodecamp.org/news/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T15:40:09.386099Z",
     "start_time": "2019-10-01T15:40:09.300441Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['view',\n",
       " 'issues',\n",
       " 'swagger',\n",
       " 'codegen',\n",
       " 'beta',\n",
       " 'release',\n",
       " 'disagreement',\n",
       " 'direction',\n",
       " '40',\n",
       " 'top']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv=CountVectorizer(max_df=0.85,max_features=10000)\n",
    "word_count_vector=cv.fit_transform(docs)\n",
    "list(cv.vocabulary_.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T15:40:10.897746Z",
     "start_time": "2019-10-01T15:40:10.886928Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T15:40:13.131111Z",
     "start_time": "2019-10-01T15:40:13.121389Z"
    }
   },
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    " \n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    " \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    " \n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T15:40:14.483587Z",
     "start_time": "2019-10-01T15:40:14.331156Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>convention</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>community</td>\n",
       "      <td>civic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>collective</td>\n",
       "      <td>civic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>civic</td>\n",
       "      <td>civic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>solidarity</td>\n",
       "      <td>civic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>interests</td>\n",
       "      <td>civic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>union</td>\n",
       "      <td>civic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>workers</td>\n",
       "      <td>civic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>representative</td>\n",
       "      <td>civic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>chapter</td>\n",
       "      <td>civic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>equality</td>\n",
       "      <td>civic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>delegates</td>\n",
       "      <td>civic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>civil</td>\n",
       "      <td>civic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>legal</td>\n",
       "      <td>civic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>law</td>\n",
       "      <td>civic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>citizens</td>\n",
       "      <td>civic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>representatives</td>\n",
       "      <td>civic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>rights</td>\n",
       "      <td>civic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>delegate</td>\n",
       "      <td>civic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>collectives</td>\n",
       "      <td>civic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>laws</td>\n",
       "      <td>civic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            keyword convention\n",
       "0         community      civic\n",
       "1        collective      civic\n",
       "2             civic      civic\n",
       "3        solidarity      civic\n",
       "4         interests      civic\n",
       "5             union      civic\n",
       "6           workers      civic\n",
       "7    representative      civic\n",
       "8           chapter      civic\n",
       "9          equality      civic\n",
       "10        delegates      civic\n",
       "11            civil      civic\n",
       "12            legal      civic\n",
       "13              law      civic\n",
       "14         citizens      civic\n",
       "15  representatives      civic\n",
       "16           rights      civic\n",
       "17         delegate      civic\n",
       "18      collectives      civic\n",
       "19             laws      civic"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# you only needs to do this once, this is a mapping of index to \n",
    "feature_names=cv.get_feature_names()\n",
    "\n",
    "keywords_for_df = []\n",
    "convention_for_df = []\n",
    "for i,f in enumerate(aggregated_files):\n",
    "    # get the document that we want to extract keywords from\n",
    "    doc = docs[i]\n",
    "    #generate tf-idf for the given document\n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
    "\n",
    "    #sort the tf-idf vectors by descending order of scores\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "    #extract only the top n; n here is 30\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items,NUM_KEYWORDS_PER_CONV)\n",
    "\n",
    "    # now add the result to dataframe\n",
    "    for k in keywords:\n",
    "        convention_for_df.append(f.replace('aggregated_', '').replace('.tsv','').lower())\n",
    "        keywords_for_df.append(k)\n",
    "\n",
    "zippedList =  list(zip(keywords_for_df, convention_for_df))\n",
    "keywords_df = pd.DataFrame(zippedList, columns = ['keyword' , 'convention'])\n",
    "display (keywords_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store keywords as CSV file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T15:40:19.847091Z",
     "start_time": "2019-10-01T15:40:19.828995Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'storing keywords.csv at path:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'./DATA/VALIDATED_DATA/Conventions/keywords/keywords.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = KEYWORDS_DIR+'keywords.csv'\n",
    "display (\"storing keywords.csv at path:\", path)\n",
    "keywords_df.to_csv(path,index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords=extract_topn_from_vector(feature_names,sorted_items,15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP HERE (REVIEW REST OF CODE TO CREATE ONE DATA SOURCE )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models on justification text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T16:20:22.993235Z",
     "start_time": "2019-09-30T16:20:22.988907Z"
    }
   },
   "outputs": [],
   "source": [
    "## Deep Learning models config\n",
    "## Classificaiton NETWORKs Configuration parameters\n",
    "MAX_SEQUENCE_LENGTH = 32\n",
    "MAX_NB_WORDS = 10000\n",
    "EMBEDDING_DIM = 100 ## 100, 200 or 300\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T11:35:36.787522Z",
     "start_time": "2019-09-30T11:35:18.716907Z"
    }
   },
   "outputs": [],
   "source": [
    "## Reading GLOVE (precalculated word embeddings)\n",
    "\n",
    "GLOVE_DIR = \"/Users/aideenf/Documents/GitHub/Economy_of_Conventions/glove.6B/\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.{}d.txt'.format(EMBEDDING_DIM)))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rreplace(s, old, new, occurrence):\n",
    "    li = s.rsplit(old, occurrence)\n",
    "    return new.join(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T08:06:52.204103Z",
     "start_time": "2019-10-01T08:06:52.154283Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Apply one hot encoding to column \"convention\"\n",
    "\n",
    "display (df.head(5))\n",
    "print(\"Applying one hot encoding to convention\")\n",
    "df[\"convention\"] = pd.Categorical(df[\"convention\"])\n",
    "dfDummies = pd.get_dummies(df[\"convention\"], prefix=\"convention\")\n",
    "display (dfDummies.head(5))\n",
    "df_final = pd.concat([df, dfDummies], axis=1)\n",
    "#df_final = df.drop(value, axis=1)\n",
    "display (df_final.head(5))\n",
    "\n",
    "\n",
    "#Generate the data frames for each model\n",
    "\n",
    "#df_by_convention = df[df.convention == 'Green']\n",
    "#display (df_by_convention.sample(n = 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code generates a training file for each characteristic by \n",
    "## concetanting sentences for convention X annotated with label one \n",
    "## and sentences from all the other convention that are annotated with label 0.\n",
    "\n",
    "\n",
    "\n",
    "##It generates a training_{convention_name}.csv file for each convention\n",
    "def generate_training_files(data_dir, data_files):\n",
    "    for file_name in data_files:\n",
    "\n",
    "        f = open(os.path.join(data_dir,file_name), \"r\")\n",
    "        original_lines = f.readlines()\n",
    "        \n",
    "        original_lines2=[]\n",
    "        for l in original_lines:\n",
    "                for l2 in l.split(\".\"):\n",
    "                    original_lines2.append(l2)\n",
    "        f.close()\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Step 1: Generate four random numbers between 0 and 1\n",
    "        Step 2: Add these four numbers\n",
    "        Step 3: Divide each of the four numbers by the sum,\n",
    "        Step 4: Multiply by 100, and round to the nearest integer.\n",
    "        \"\"\"\n",
    "        num_original_lines = len(original_lines2)*2\n",
    "\n",
    "        num_random_lines = np.random.uniform(0,1,len(data_files))\n",
    "        num_random_lines = (num_random_lines / sum(num_random_lines))*num_original_lines\n",
    "\n",
    "        tmp_all_files = [f for f in data_files if f != file_name]\n",
    "\n",
    "        random_lines = []    \n",
    "        for idx, file_name2 in enumerate(tmp_all_files):\n",
    "            f2 = open(os.path.join(data_dir,file_name2), \"r\")\n",
    "            tmp_lines = f2.readlines()\n",
    "            \n",
    "            tmp_lines2 = []\n",
    "            for l in tmp_lines:\n",
    "                for l2 in l.split(\".\"):\n",
    "                    tmp_lines2.append(l2)\n",
    "            f2.close()\n",
    "\n",
    "            for i in range(int(num_random_lines[idx])):\n",
    "                random_lines.append(random.choice(tmp_lines2))\n",
    "\n",
    "        with open(rreplace(os.path.join(data_dir,file_name), \"/\", \"/training_\",1), \"w\") as f3:\n",
    "            f3.write(\"{}\\t{}\\n\".format(\"category\", \"text\"))\n",
    "\n",
    "            for idx, l in enumerate(original_lines2):\n",
    "                l = l.replace('\\n', ' ').replace('\\t', ' ')\n",
    "                f3.write(\"{}\\t\\\"{}\\\"\\n\".format(1, l))\n",
    "\n",
    "\n",
    "            for idx, l2 in enumerate(random_lines):\n",
    "                l2 = l2.replace('\\n', '').replace('\\t', ' ')\n",
    "                f3.write(\"{}\\t\\\"{}\\\"\\n\".format(0, l2))\n",
    "            f3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_training_files(BASE_DIR, justif_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG__AGGREGATE_CONVENTIONS_SENTENCES_FROM_GITHUB:\n",
    "    justif_training_files = [f for f in os.listdir(BASE_DIR) if (os.path.isfile(os.path.join(BASE_DIR, f)) and not f.startswith( '.' ) and not \"random\" in f and \"training_aggregated\" in f and not \"splitted_\" in f)]\n",
    "    print(justif_training_files)\n",
    "else:\n",
    "    justif_training_files = [f for f in os.listdir(BASE_DIR) if (os.path.isfile(os.path.join(BASE_DIR, f)) and not f.startswith( '.' ) and not \"random\" in f and not \"training_aggregated\" in f and \"training_\" in f and not \"splitted_\" in f)]\n",
    "    print(justif_training_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(DATA_DIR, data_files, max_words=MAX_NB_WORDS):\n",
    "    texts = []\n",
    "    for d in data_files:\n",
    "        data_train = pd.read_csv(os.path.join(DATA_DIR,d), sep='\\t')\n",
    "    \n",
    "        for idx in range(data_train.text.shape[0]):\n",
    "            text = data_train.text[idx]\n",
    "            texts.append(str(text))\n",
    "    \n",
    "    _tokenizer = Tokenizer(num_words=max_words, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "    _tokenizer.fit_on_texts(texts)\n",
    "    \n",
    "    return _tokenizer \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conventions_tokenizer = create_tokenizer(BASE_DIR, justif_training_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_and_train_model(DATA_DIR, data_file, tokenizer=None, num_epochs=NUM_EPOCHS):\n",
    "    ## USING licensing text from github\n",
    "    data_train = pd.read_csv(os.path.join(DATA_DIR,data_file), sep='\\t')\n",
    "    print(data_train.shape)\n",
    "\n",
    "    data_train = data_train\n",
    "\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    for idx in range(data_train.text.shape[0]):\n",
    "        text = data_train.text[idx]#BeautifulSoup(data_train.text[idx])\n",
    "        texts.append(str(text))#clean_str(text))\n",
    "\n",
    "        labels.append(data_train.category[idx])\n",
    "        \n",
    "    if tokenizer is None:\n",
    "        tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "        tokenizer.fit_on_texts(texts)\n",
    "        sequences = tokenizer.texts_to_sequences(texts)\n",
    "    else:\n",
    "        print(\" -- Tokenizer has not been retrained\")\n",
    "\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    #embeddings_index = {}\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    labels = to_categorical(np.asarray(labels))\n",
    "    print('Shape of data tensor:', data.shape)\n",
    "    print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "    indices = np.arange(data.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    data = data[indices]\n",
    "    labels = labels[indices]\n",
    "    nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "    x_train = data[:-nb_validation_samples]\n",
    "    y_train = labels[:-nb_validation_samples]\n",
    "    x_val = data[-nb_validation_samples:]\n",
    "    y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "    \n",
    "    print('\\nNumber of elements from each class in traing and validation set ')\n",
    "    print(y_train.sum(axis=0))\n",
    "    print(y_val.sum(axis=0))\n",
    "    \n",
    "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)\n",
    "\n",
    "    \"\"\"\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    l_cov1= Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "    l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "    l_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)\n",
    "    l_pool2 = MaxPooling1D(5)(l_cov2)\n",
    "    #l_cov3 = Conv1D(128, 5, activation='relu')(l_pool2)\n",
    "    #l_pool3 = MaxPooling1D(35)(l_cov3)  # global max pooling\n",
    "    l_flat = Flatten()(l_pool2)\n",
    "    l_dense = Dense(128, activation='relu')(l_flat)\n",
    "    preds = Dense(2, activation='softmax')(l_dense)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    print(\"model fitting - simplified convolutional neural network\")\n",
    "    model.summary()\n",
    "    train_history = model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "              epochs=num_epochs, batch_size=128)\n",
    "    \"\"\"\n",
    "    # applying a more complex convolutional approach\n",
    "    convs = []\n",
    "    filter_sizes = [3,4,5]\n",
    "\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    for fsz in filter_sizes:\n",
    "        l_conv = Conv1D(nb_filter=128,filter_length=fsz,activation='relu')(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(5)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Concatenate(axis=1)(convs)#Merge(mode='concat', concat_axis=1)(convs)\n",
    "    l_cov1= Conv1D(128, 5, activation='relu')(l_merge)\n",
    "    l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "    #l_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)\n",
    "    #l_pool2 = MaxPooling1D(30)(l_cov2)\n",
    "    #l_flat = Flatten()(l_pool2)\n",
    "    l_flat = Flatten()(l_pool1)\n",
    "    l_dense = Dense(128, activation='relu')(l_flat)\n",
    "    preds = Dense(2, activation='softmax')(l_dense)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    print(\"model fitting - more complex convolutional neural network\")\n",
    "    model.summary()\n",
    "    train_history = model.fit(x_train, y_train, validation_data=(x_val, y_val),epochs=num_epochs, batch_size=50)\n",
    "    \n",
    "    return model,tokenizer,x_val,y_val, train_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_conventions_models = {}\n",
    "_conventions_tokenizers = {}\n",
    "_conventions_data_val_x = {}\n",
    "_conventions_data_val_y = {}\n",
    "_conventions_train_histories = {}\n",
    "\n",
    "for f in justif_training_files:\n",
    "\n",
    "\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(\"            {}                  \".format(f))\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "\n",
    "    _model, _tokenizer, _x_val, _y_val, _train_h = read_file_and_train_model(BASE_DIR, f, tokenizer=conventions_tokenizer)\n",
    "\n",
    "\n",
    "    _conventions_models[f] = _model\n",
    "    _conventions_tokenizers[f] = _tokenizer\n",
    "    _conventions_data_val_x[f] = _x_val\n",
    "    _conventions_data_val_y[f] = _y_val\n",
    "    _conventions_train_histories[f] = _train_h\n",
    "\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analying models precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_matches(sequences, model):\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    preds = model.predict(data)\n",
    "\n",
    "    return preds\n",
    "\n",
    "def count_positive_preds(preds):\n",
    "    count = 0\n",
    "    for j in range(len(preds)):\n",
    "    \n",
    "        count += int(preds[j][1]>preds[j][0])\n",
    "    return count\n",
    "\n",
    "def count_positive_preds_with_threshold(preds, threshold=0.5):\n",
    "    count = 0\n",
    "    for j in range(len(preds)):\n",
    "    \n",
    "        count += int(preds[j][1]>=threshold)\n",
    "    return count\n",
    "\n",
    "def get_positive_preds_with_threshold(preds, threshold=0.5):\n",
    "\n",
    "    preds_results = []\n",
    "    for j in range(len(preds)):\n",
    "        int_pred = int(preds[j][1]>=threshold)\n",
    "        \n",
    "        preds_results.append(int_pred)\n",
    "    return preds_results\n",
    "\n",
    "def calculate_matches(repositories_descriptions, _models, _tokenizers, model_preds_func=count_positive_preds, threshold=None):\n",
    "    _repos_matches = []\n",
    "\n",
    "    ##Â Counting number of matches per model for each repo\n",
    "    for r_description in repositories_descriptions:\n",
    "        r_description = r_description.split(\"\\n\")\n",
    "\n",
    "        this_repo_matches = []\n",
    "        for model_key in _models.keys():\n",
    "            matches=0\n",
    "\n",
    "            tokenized_r_description = _tokenizers[model_key].texts_to_sequences(r_description)\n",
    "\n",
    "            preds = get_model_matches(tokenized_r_description, _models[model_key])\n",
    "\n",
    "            if threshold is None:\n",
    "                num_matches = model_preds_func(preds)\n",
    "            else:\n",
    "                num_matches = model_preds_func(preds, threshold)\n",
    "\n",
    "            this_repo_matches.append(num_matches)\n",
    "\n",
    "        _repos_matches.append(this_repo_matches)\n",
    "        \n",
    "    return _repos_matches\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matches_histogram(_matches, _models, title=None, ):\n",
    "\n",
    "    freqs = np.array(_matches).sum(axis=0)\n",
    "    elems = [k.replace(\"training_\", \"\").replace(\".txt\", \"\") for k in list(_models.keys())]\n",
    "\n",
    "    unique_elems = list(set(elems))\n",
    "\n",
    "    f, ax = plt.subplots()\n",
    "\n",
    "\n",
    "    plt.barh(unique_elems, freqs)\n",
    "    if title is None:\n",
    "        plt.title(\"Model matches per repository\")\n",
    "    else:\n",
    "        plt.title(\"{} model matches per repository\".format(title))\n",
    "    plt.ylabel(\"Classifiers\")\n",
    "    plt.xlabel(\"Frequency\")\n",
    "    \n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0,1,100)\n",
    "\n",
    "_conventions_matches_precissions = {}\n",
    "\n",
    "\n",
    "for k in _conventions_models.keys():\n",
    "    \n",
    "    tmp_precissions = []\n",
    "    \n",
    "    my_model = _conventions_models[k]\n",
    "    my_data_x = _conventions_data_val_x[k]\n",
    "    my_data_y = [int(x[0]<x[1]) for x in _conventions_data_val_y[k]]\n",
    "\n",
    "    preds = my_model.predict(my_data_x)\n",
    "    \n",
    "    for t in thresholds:\n",
    "        y_pred = get_positive_preds_with_threshold(preds,t)\n",
    "        \n",
    "        tmp_precissions.append(metrics.precision_score(my_data_y, y_pred))\n",
    "        \n",
    "\n",
    "    _conventions_matches_precissions[k] = tmp_precissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,7))\n",
    "\n",
    "equalized_conv_precissions_threshold=0.725\n",
    "for k in _conventions_matches_precissions:\n",
    "    x = thresholds\n",
    "    y = _conventions_matches_precissions[k]\n",
    "    \n",
    "    plt.plot(x,y, marker='', linewidth=2, linestyle='dashed', label=\"toto\")\n",
    "    \n",
    "plt.legend([\"Precission for \"+x.replace(\"training_\", \"\").replace(\".txt\", \"\")+\" model\"\n",
    "            for x in list(_conventions_matches_precissions.keys())], fontsize=15)\n",
    "\n",
    "plt.title(\"Comparing precission scores for all classifiers\", fontdict={'fontsize':20}, pad=25)\n",
    "plt.axvline(x=equalized_conv_precissions_threshold, color='black', linestyle=\"-.\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "for k in _conventions_matches_precissions:\n",
    "    print(\"--{}--\".format(k))\n",
    "    print(_conventions_matches_precissions[k][int(equalized_conv_precissions_threshold*100)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models evaluation on Github Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_sentences = {}\n",
    "\n",
    "for i,x in enumerate(justif_files):\n",
    "    texts = []\n",
    "    \n",
    "    data_train = pd.read_csv(os.path.join(BASE_DIR_GIT_HUB,\"training_\"+x), sep='\\t')\n",
    "    conv = x.replace(\".txt\", \"\")\n",
    "    github_sentences[conv] = data_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_sentences['Industrial'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0,1,100)\n",
    "\n",
    "_conventions_matches_github_precissions = {}\n",
    "\n",
    "\n",
    "for k in _conventions_models.keys():\n",
    "    \n",
    "    tmp_precissions = []\n",
    "    \n",
    "    \n",
    "    my_model = _conventions_models[k]\n",
    "    \n",
    "    conv = k.replace(\"training_\", \"\").replace(\".txt\", \"\")\n",
    "    data_train = github_sentences[conv]\n",
    "    \n",
    "    \n",
    "    sequences = conventions_tokenizer.texts_to_sequences(data_train[\"text\"].values)\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    \n",
    "    my_data_x = data\n",
    "    my_data_y = data_train[\"category\"].values\n",
    "    \n",
    "\n",
    "    preds = my_model.predict(my_data_x)\n",
    "    \n",
    "    for t in thresholds:\n",
    "        y_pred = get_positive_preds_with_threshold(preds,t)\n",
    "        \n",
    "        tmp_precissions.append(metrics.precision_score(my_data_y, y_pred))\n",
    "        \n",
    "\n",
    "    _conventions_matches_github_precissions[conv] = tmp_precissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,7))\n",
    "\n",
    "equalized_conv_precissions_github_threshold=0.738\n",
    "for k in _conventions_matches_github_precissions:\n",
    "    x = thresholds\n",
    "    y = _conventions_matches_github_precissions[k]\n",
    "    \n",
    "    plt.plot(x,y, marker='', linewidth=2, linestyle='dashed', label=\"toto\")\n",
    "    \n",
    "plt.legend([\"Precission for \"+x.replace(\"training_\", \"\").replace(\".txt\", \"\")+\" model\"\n",
    "            for x in list(_conventions_matches_precissions.keys())], fontsize=15)\n",
    "\n",
    "plt.title(\"Comparing precission scores for all classifiers\", fontdict={'fontsize':20}, pad=25)\n",
    "plt.axvline(x=equalized_conv_precissions_github_threshold, color='black', linestyle=\"-.\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for k in _conventions_matches_github_precissions:\n",
    "    print(\"--{}--\".format(k))\n",
    "    print(_conventions_matches_github_precissions[k][int(equalized_conv_precissions_github_threshold*100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
